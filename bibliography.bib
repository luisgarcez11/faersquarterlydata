@article{Andersen2012,
abstract = {Background: The evidence base for the diagnosis and management of amyotrophic lateral sclerosis (ALS) is weak. Objectives: To provide evidence-based or expert recommendations for the diagnosis and management of ALS based on a literature search and the consensus of an expert panel. Methods: All available medical reference systems were searched, and original papers, meta-analyses, review papers, book chapters and guidelines recommendations were reviewed. The final literature search was performed in February 2011. Recommendations were reached by consensus. Recommendations: Patients with symptoms suggestive of ALS should be assessed as soon as possible by an experienced neurologist. Early diagnosis should be pursued, and investigations, including neurophysiology, performed with a high priority. The patient should be informed of the diagnosis by a consultant with a good knowledge of the patient and the disease. Following diagnosis, the patient and relatives/carers should receive regular support from a multidisciplinary care team. Medication with riluzole should be initiated as early as possible. Control of symptoms such as sialorrhoea, thick mucus, emotional lability, cramps, spasticity and pain should be attempted. Percutaneous endoscopic gastrostomy feeding improves nutrition and quality of life, and gastrostomy tubes should be placed before respiratory insufficiency develops. Non-invasive positive-pressure ventilation also improves survival and quality of life. Maintaining the patient's ability to communicate is essential. During the entire course of the disease, every effort should be made to maintain patient autonomy. Advance directives for palliative end-of-life care should be discussed early with the patient and carers, respecting the patient's social and cultural background. {\textcopyright} 2011 The Author(s). European Journal of Neurology {\textcopyright} 2011 EFNS.},
author = {Andersen, Peter M. and Abrahams, Sharon and Borasio, Gian D. and de Carvalho, Mamede and Chio, Adriano and {Van Damme}, Philip and Hardiman, Orla and Kollewe, Katja and Morrison, Karen E. and Petri, Susanne and Pradat, Pierre Francois and Silani, Vincenzo and Tomik, Barbara and Wasner, Maria and Weber, Markus},
doi = {10.1111/J.1468-1331.2011.03501.X},
file = {::},
issn = {1468-1331},
journal = {European Journal of Neurology},
keywords = {ALS,Evidence,based medicine,breaking the diagnosis,bronchial secretions,caregiver,cognitive dysfunction,drooling,genetic counselling,nutrition,palliative care,terminal care,ventilation},
month = {mar},
number = {3},
pages = {360--375},
pmid = {21914052},
publisher = {John Wiley & Sons, Ltd},
title = {{EFNS guidelines on the Clinical Management of Amyotrophic Lateral Sclerosis (MALS) – revised report of an EFNS task force}},
url = {https://onlinelibrary.wiley.com/doi/full/10.1111/j.1468-1331.2011.03501.x https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-1331.2011.03501.x https://onlinelibrary.wiley.com/doi/10.1111/j.1468-1331.2011.03501.x},
volume = {19},
year = {2012}
}
@article{DeCarvalho2019,
abstract = {The main reason for short survival in amyotrophic lateral sclerosis (ALS) is involvement of respiratory muscles. Severe compromise of diaphragmatic function due to marked loss of motor units causes poor inspiratory strength leading to symptomatic respiratory fatigue, and hypercapnia and hypoxemia, often firstly detected while sleeping supine. Weakness of expiratory muscles leads to cough weakness and poor bronchial clearance, increasing the risk of respiratory infection. Respiratory tests should therefore encompass inspiratory and expiratory function, and include measurements of blood gases during sleep. Non-volitional tests, such as phrenic nerve stimulation, are particularly convenient for investigating respiratory function in patients unable to perform standard respiratory function tests due to poor cooperation or facial weakness. However, SNIP is a sensitive test when patients with bulbar involvement are able to perform the necessary maneuvers. It is likely that central respiratory regulation is disturbed in some ALS patients, but its evaluation is more complex and not regularly implemented. Practical tests should incorporate tolerability, sensitivity, easy application for regular monitoring, and prognostic value. Impending respiratory failure can cause increased circulating inflammatory markers, but molecular assessment of respiratory distress requires further study. In future, home-monitoring of patients with accessible devices should be developed.},
author = {{De Carvalho}, Mamede and Swash, Michael and Pinto, Susana},
doi = {10.3389/FNEUR.2019.00143/BIBTEX},
file = {::},
issn = {16642295},
journal = {Frontiers in Neurology},
keywords = {Amyotrophic lateral sclerosis,Diaphragm physiology,Progression,Respiratory function tests,Survival},
month = {feb},
number = {FEB},
pages = {443356},
publisher = {Frontiers Media S.A.},
title = {{Diaphragmatic neurophysiology and respiratory markers in ALS}},
volume = {10},
year = {2019}
}
@article{Pinto1995,
abstract = {Noninvasive ventilatory assistance, in ALS patients, with the bilevel intermittent positive air pressure (Bipap) was studied, in a prospective and controlled trial, by the authors. Twenty ALS bulbar patients, fulfilling El Escorial criteria for probable or definite disease, were selected. For the follow-up all patients were submitted to evaluation with the Norris scale, modified Barthel score and an analog scale of life satisfaction, every 3 months. All patients were also submitted to respiratory functional testing (RFT). Ten of these patients were treated with palliative management (group I), the remaining ten patients received Bipap support (group II). Clinical evolution curves and clinical parameters were not statistically different in both groups, except for the percentage of actual predicted value of vital capacity (p < 0.03), showing a more advanced disease in group II patients. Analog scale of life satisfaction showed improvement in the group II, even after the beginning of respiratory insufficiency, though without significance probably due to the small sample size (p < 0.1). Since 6 patients in group II are still alive survival rates were compared with log rank test considering cumulative survivals with Kaplan-Meier estimates. Total survival and survival from diurnal abnormalities in gas exchange (survival 1) were significantly longer for group II (p < 0.006 and p < 0.0004, respectively). In spite of the small number of patients, preliminary results strongly support the importance of BIPAP in ALS patients, though further studies must go on in order to optimize the best time for introducing Bipap. {\textcopyright} 1995 Elsevier Science B.V. All rights reserved.},
author = {Pinto, A. C. and Evangelista, T. and Carvalho, M. and Alves, M. A. and {Sales Lu{\'{i}}s}, M. L.},
doi = {10.1016/0022-510X(95)00052-4},
issn = {0022510X},
journal = {Journal of the Neurological Sciences},
keywords = {Amyotrophic lateral sclerosis,Bipap,Noninvasive ventilatory support,Outcome measures,Quality of life,Respiratory insufficiency},
number = {SUPPL.},
pages = {19--26},
pmid = {7595610},
publisher = {J Neurol Sci},
title = {{Respiratory assistance with a non-invasive ventilator (Bipap) in MND/ALS patients: Survival rates in a controlled trial}},
url = {https://pubmed.ncbi.nlm.nih.gov/7595610/},
volume = {129},
year = {1995}
}
@article{Neumann2019,
author = {Neumann, Franz-Josef and Sousa-Uva, Miguel and Ahlsson, Anders and Alfonso, Fernando and Banning, Adrian P and Benedetto, Umberto and Byrne, Robert A and Collet, Jean-Philippe and Falk, Volkmar and Head, Stuart J and J{\"{u}}ni, Peter and Kastrati, Adnan and Koller, Akos and Kristensen, Steen D and Niebauer, Josef and Richter, Dimitrios J and Seferovi{\'{c}}, Petar M and Sibbing, Dirk and Stefanini, Giulio G and Windecker, Stephan and Yadav, Rashmi and Zembala, Michael O and Wijns, William and Glineur, David and Aboyans, Victor and Achenbach, Stephan and Agewall, Stefan and Andreotti, Felicita and Barbato, Emanuele and Baumbach, Andreas and Brophy, James and Bueno, H{\'{e}}ctor and Calvert, Patrick A and Capodanno, Davide and Davierwala, Piroze M and Delgado, Victoria and Dudek, Dariusz and Freemantle, Nick and Funck-Brentano, Christian and Gaemperli, Oliver and Gielen, Stephan and Gilard, Martine and Gorenek, Bulent and Haasenritter, Joerg and Haude, Michael and Ibanez, Borja and Iung, Bernard and Jeppsson, Anders and Katritsis, Demosthenes and Knuuti, Juhani and Kolh, Philippe and Leite-Moreira, Adelino and Lund, Lars H and Maisano, Francesco and Mehilli, Julinda and Metzler, Bernhard and Montalescot, Gilles and Pagano, Domenico and Petronio, Anna Sonia and Piepoli, Massimo Francesco and Popescu, Bogdan A and S{\'{a}}daba, Rafael and Shlyakhto, Evgeny and Silber, Sigmund and Simpson, Iain A and Sparv, David and Tavilla, Giuseppe and Thiele, Holger and Tousek, Petr and {Van Belle}, Eric and Vranckx, Pascal and Witkowski, Adam and Zamorano, Jose Luis and Roffi, Marco and Windecker, Stephan and Aboyans, Victor and Agewall, Stefan and Barbato, Emanuele and Bueno, H{\'{e}}ctor and Coca, Antonio and Collet, Jean-Philippe and Coman, Ioan Mircea and Dean, Veronica and Delgado, Victoria and Fitzsimons, Donna and Gaemperli, Oliver and Hindricks, Gerhard and Iung, Bernard and J{\"{u}}ni, Peter and Katus, Hugo A and Knuuti, Juhani and Lancellotti, Patrizio and Leclercq, Christophe and McDonagh, Theresa A and Piepoli, Massimo Francesco and Ponikowski, Piotr and Richter, Dimitrios J and Roffi, Marco and Shlyakhto, Evgeny and Sousa-Uva, Miguel and Simpson, Iain A and Zamorano, Jose Luis and Pagano, Domenico and Freemantle, Nick and Sousa-Uva, Miguel and Chettibi, Mohamed and Sisakian, Hamayak and Metzler, Bernhard and İbrahimov, Firdovsi and Stelmashok, Valeriy I and Postadzhiyan, Arman and Skoric, Bosko and Eftychiou, Christos and Kala, Petr and Terkelsen, Christian Juhl and Magdy, Ahmed and Eha, Jaan and Niemel{\"{a}}, Matti and Kedev, Sasko and Motreff, Pascal and Aladashvili, Alexander and Mehilli, Julinda and Kanakakis, Ioannis-Georgios and Becker, David and Gudnason, Thorarinn and Peace, Aaron and Romeo, Francesco and Bajraktari, Gani and Kerimkulova, Alina and Rudzītis, Ainārs and Ghazzal, Ziad and Kibarskis, Aleksandras and Pereira, Bruno and Xuereb, Robert G and Hofma, Sjoerd H and Steigen, Terje K and Witkowski, Adam and de Oliveira, Eduardo Infante and Mot, Stefan and Duplyakov, Dmitry and Zavatta, Marco and Beleslin, Branko and Kovar, Frantisek and Bunc, Matja{\v{z}} and Ojeda, Soledad and Witt, Nils and Jeger, Raban and Addad, Faouzi and Akdemir, Ramazan and Parkhomenko, Alexander and Henderson, Robert},
doi = {10.1093/eurheartj/ehy394},
issn = {0195-668X},
journal = {European Heart Journal},
month = {jan},
number = {2},
pages = {87--165},
title = {{2018 ESC/EACTS Guidelines on myocardial revascularization}},
volume = {40},
year = {2019}
}
@article{Hochman2023,
abstract = {Background: The ISCHEMIA trial (International Study of Comparative Health Effectiveness With Medical and Invasive Approaches) compared an initial invasive versus an initial conservative management strategy for patients with chronic coronary disease and moderate or severe ischemia, with no major difference in most outcomes during a median of 3.2 years. Extended follow-up for mortality is ongoing. Methods: ISCHEMIA participants were randomized to an initial invasive strategy added to guideline-directed medical therapy or a conservative strategy. Patients with moderate or severe ischemia, ejection fraction ≥35%, and no recent acute coronary syndromes were included. Those with an unacceptable level of angina were excluded. Extended follow-up for vital status is being conducted by sites or through central death index search. Data obtained through December 2021 are included in this interim report. We analyzed all-cause, cardiovascular, and noncardiovascular mortality by randomized strategy, using nonparametric cumulative incidence estimators, Cox regression models, and Bayesian methods. Undetermined deaths were classified as cardiovascular as prespecified in the trial protocol. Results: Baseline characteristics for 5179 original ISCHEMIA trial participants included median age 65 years, 23% women, 16% Hispanic, 4% Black, 42% with diabetes, and median ejection fraction 0.60. A total of 557 deaths accrued during a median follow-up of 5.7 years, with 268 of these added in the extended follow-up phase. This included a total of 343 cardiovascular deaths, 192 noncardiovascular deaths, and 22 unclassified deaths. All-cause mortality was not different between randomized treatment groups (7-year rate, 12.7% in invasive strategy, 13.4% in conservative strategy; adjusted hazard ratio, 1.00 [95% CI, 0.85-1.18]). There was a lower 7-year rate cardiovascular mortality (6.4% versus 8.6%; adjusted hazard ratio, 0.78 [95% CI, 0.63-0.96]) with an initial invasive strategy but a higher 7-year rate of noncardiovascular mortality (5.6% versus 4.4%; adjusted hazard ratio, 1.44 [95% CI, 1.08-1.91]) compared with the conservative strategy. No heterogeneity of treatment effect was evident in prespecified subgroups, including multivessel coronary disease. Conclusions: There was no difference in all-cause mortality with an initial invasive strategy compared with an initial conservative strategy, but there was lower risk of cardiovascular mortality and higher risk of noncardiovascular mo{\ldots}},
author = {Hochman, Judith S. and Anthopolos, Rebecca and Reynolds, Harmony R. and Bangalore, Sripal and Xu, Yifan and O'Brien, Sean M. and Mavromichalis, Stavroula and Chang, Michelle and Contreras, Aira and Rosenberg, Yves and Kirby, Ruth and Bhargava, Balram and Senior, Roxy and Banfield, Ann and Goodman, Shaun G. and Lopes, Renato D. and Praco{\'{n}}, Rados{\l}aw and L{\'{o}}pez-Send{\'{o}}n, Jos{\'{e}} and Maggioni, Aldo Pietro and Newman, Jonathan D. and Berger, Jeffrey S. and Sidhu, Mandeep S. and White, Harvey D. and Troxel, Andrea B. and Harrington, Robert A. and Boden, William E. and Stone, Gregg W. and Mark, Daniel B. and Spertus, John A. and Maron, David J. and Hochman, Judith S. and Maron, David J. and Reynolds, Harmony R. and Bangalore, Sripal and Mavromichalis, Stavroula and Chang, Michelle and Contreras, Aira and Esquenazi-Karonika, Shari and Gilsenan, Margaret and Gwiszcz, Ewelina and Mathews, Patenne and Mohamed, Samaa and Naumova, Anna and Roberts, Arline and Vanloo, Kerrie and Anthopolos, Rebecca and Xu, Yifan and Troxel, Andrea B. and Lu, Ying and Huang, Zhen and Broderick, Samuel and Guzm{\'{a}}n, Luis and Selvanayagam, Joseph and Lopes, Renato D. and Goodman, Shaun G. and Steg, Gabriel and Juliard, Jean Michel and Doerr, Rolf and Keltai, Matyas and Bhargava, Balram and Thomas, Boban and Sharir, Tali and Nikolsky, Eugenia and Maggioni, Aldo P. and Kohsaka, Shun and Escobedo, Jorge and Praco{\'{n}}, Rados{\l}aw and Bockeria, Olga and L{\'{o}}pez-Send{\'{o}}n, Jos{\'{e}} and Held, Claes and Senior, Roxy and Banfield, Ann and Shaw, Leslee J. and Phillips, Lawrence and Berman, Daniel and Kwong, Raymond Y. and Picard, Michael H. and Chaitman, Bernard R. and Ali, Ziad and Min, James and Mancini, G. B.John and Leipsic, Jonathon and Guzman, Luis and Hillis, Graham and Thambar, Suku and Joseph, Majo and Selvnayagam, Joseph and Beltrame, John and Lang, Irene and Schuchlenz, Herwig and Huber, Kurt and Goetschalckx, Kaatje and Hueb, Whady and Caramori, Paulo Ricardo and {De Quadros}, Alexandre and Smanio, Paola and Mesquita, Claudio and Lopas, Renato D. and Vitola, Jo{\~{a}}o and Marin-Neto, Jos{\'{e}} and {Da Silva}, Expedito Ribeiro and Tumelero, Rog{\'{e}}rio and Andrade, Marianna and Alves, Alvaro Rabelo and Dall'Orto, Frederico and Polanczyk, Carisi and Figueiredo, Estev{\~{a}}o and Howarth, Andrew and Gosselin, Gilbert and Cheema, Asim and Bainey, Kevin and Phaneuf, Denis and Diaz, Ariel and Garg, Pallav and Mehta, Shamir and Wong, Graham and Lam, Andy and Cha, James and Galiwango, Paul and Uxa, Amar and Chow, Benjamin and Hameed, Adnan and Udell, Jacob and Chema, Asim and Hamid, Magdy and Hauguel-Moreau, Marie and Furber, Alain and Goube, Pascal and Steg, Philippe Gabriel and Barone-Rochette, Gilles and Thuaire, Christophe and Slama, Michel and Doer, Rolf and Nickenig, Georg and Bekeredjian, Raffi and Schulze, P. Christian and Merkely, Bela and Fontos, Geza and V{\'{e}}rtes, Andr{\'{a}}s and Varga, Albert and Bhargva, Balram and Kumar, Ajit and Nair, Rajesh G. and Grant, Purvez and Manjunath, Cholenahally and Moorthy, Nagaraja and Satheesh, Santhosh and Nath, Ranjit Kumar and Wander, Gurpreet and Christopher, Johann and Dwivedi, Sudhanshu and Oomman, Abraham and Mathur, Atul and Gadkari, Milind and Naik, Sudhir and Punnoose, Eapen and Kachru, Ranjan and Christophar, Johann and Kaul, Upendra and Sharer, Tali and Kerner, Arthur and Tarantini, Giuseppe and Perna, Gian Piero and Racca, Emanuela and Mortara, Andrea and Monti, Lorenzo and Briguori, Carlo and Leone, Gianpiero and Amati, Roberto and Salvatori, Mauro and {Di Chiara}, Antonio and Calabro, Paolo and Galvani, Marcello and Provasoli, Stefano and Fukuda, Keiichi and Koshaka, Shun and Nakano, Shintaro and Laucevicius, Aleksandras and Kedev, Sasko and Khairuddin, Ahmad and Escobdo, Jorge and Riezebos, Robert and Timmer, Jorik and Heald, Spencer and Stewart, Ralph and Ramos, Walter Mogrovejo and Demkow, Marcin and Mazurek, Tomasz and Drozdz, Jarozlaw and Szwed, Hanna and Witkowski, Adam and Ferreira, Nuno and Pinto, Fausto and Ramos, Ruben and Popescu, Bogdan and Pop, Calin and Bockeria, Leo and Bockerya, Olga and Demchenko, Elena and Romanov, Alexander and Bershtein, Leonid and Jizeeri, Ahmed and Stankovic, Goran and Apostolovic, Svetlana and Adjic, Nada Cemerlic and Zdravkovic, Marija and Beleslin, Branko and Dekleva, Milica and Davidovic, Goran and Chua, Terrance and Foo, David and Poh, Kian Keong and Ntsekhe, Mpiko and Sionis, Alessandro and Marin, Francisco and Mir{\'{o}}, Vicente and L{\'{o}}pez-Sendon, Jos{\'{e}} and Blancas, Montserrat Gracida and Gonz{\'{a}}lez-Juanatey, Jos{\'{e}} and Fern{\'{a}}ndez-Avil{\'{e}}s, Francisco and Peteiro, Jes{\'{u}}s and Luena, Jose Enrique Castillo and Held, Cleas and Aspberg, Johannes and Rossi, Mariagrazia and Kuanprasert, Srun and Yamwong, Sukit and Johnston, Nicola and Donnelly, Patrick and Moriarty, Andrew and Roxy, R. and Elghamaz, Ahmed and Gurunathan, Sothinathan and Karogiannis, Nikolaos and Shah, Benoy N. and Trimlett, Richard H.J. and Rubens, Michael B. and Nicol, Edward D. and Mittal, Tarun K. and Hampson, Reinette and Gamma, Reto and {De Belder}, Mark and Nageh, Thuraia and Lindsay, Steven and Mavromatis, Kreton and Miller, Todd and Banerjee, Subhash and Reynolds, Harmony and Nour, Khaled and Stone, Peter},
doi = {10.1161/CIRCULATIONAHA.122.062714},
issn = {15244539},
journal = {Circulation},
number = {1},
pages = {8--19},
pmid = {36335918},
title = {{Survival after Invasive or Conservative Management of Stable Coronary Disease}},
volume = {147},
year = {2023}
}
@article{Persson2023,
abstract = {Aims: An observational nationwide all-comers prospective register study to analyse outcomes after coronary artery bypass grafting (CABG) or percutaneous coronary intervention (PCI) in unprotected left main coronary artery (LMCA) disease. Methods and results: All patients undergoing coronary angiography in Sweden are registered in the Swedish Web-system for Enhancement and Development of Evidence-based care in Heart disease Evaluated According to Recommended Therapies registry. Between 01/01/2005 and 12/31/2015, 11 137 patients with LMCA disease underwent CABG (n = 9364) or PCI (n = 1773). Patients with previous CABG, ST-elevation myocardial infarction (MI) or cardiac shock were excluded. Death, MI, stroke, and new revascularization during follow-up until 12/31/2015 were identified using national registries. Cox regression with inverse probability weighting (IPW) and an instrumental variable (IV), administrative region, were used. Patients undergoing PCI were older, had higher prevalence of comorbidity but lower prevalence of three-vessel disease. PCI patients had higher mortality than CABG patients after adjustments for known cofounders with IPW analysis (hazard ratio [HR] 2.0 [95% confidence interval (CI) 1.5-2.7]) and known/unknown confounders with IV analysis (HR 1.5 [95% CI 1.1-2.0]). PCI was associated with higher incidence of major adverse cardiovascular and cerebrovascular events (MACCE; death, MI, stroke, or new revascularization) than CABG, with IV analysis (HR 2.8 [95% CI 1.8-4.5]). There was a quantitative interaction for diabetic status regarding mortality (P = 0.014) translating into 3.6 years (95% CI 3.3-4.0) longer median survival time favouring CABG in patients with diabetes. Conclusion: In this non-randomized study, CABG in patients with LMCA disease was associated with lower mortality and fewer MACCE compared to PCI after multivariable adjustment for known and unknown confounders.},
author = {Persson, Jonas and Yan, Jacinth and Anger{\aa}s, Oskar and Venetsanos, Dimitrios and Jeppsson, Anders and Sj{\"{o}}gren, Iwar and Linder, Rikard and Erlinge, David and Ivert, Torbj{\"{o}}rn and Omerovic, Elmir},
doi = {10.1093/eurheartj/ehad369},
issn = {15229645},
journal = {European Heart Journal},
number = {30},
pages = {2833--2842},
pmid = {37288564},
publisher = {Oxford University Press},
title = {{PCI or CABG for left main coronary artery disease: The SWEDEHEART registry}},
volume = {44},
year = {2023}
}
@misc{RCoreTeam2022,
address = {Vienna, Austria},
author = {{R Core Team}},
title = {{R: A Language and Environment for Statistical Computing}},
url = {https://www.r-project.org/},
year = {2022}
}
@article{Shafi2020,
abstract = {Background: Young patients with coronary artery disease are undergoing percutaneous coronary intervention (PCI) primarily, with a view to deferring coronary artery bypass grafting (CABG). We investigated the validity of this approach, by comparing outcomes in patients ≤50 years undergoing CABG or PCI. Methods: One hundred consecutive patients undergoing PCI and 100 undergoing CABG in 2004 were retrospectively studied to allow for 5 and 12 years follow-up. The two groups were compared for the primary endpoints of major adverse cardiac or cerebrovascular event (MACCE). Results: Diabetes, peripheral vascular disease, and left ventricular ejection fraction <50% were higher in the CABG group. At 5 years, rates of myocardial infarction (MI) (9% vs 1%, P =.02), repeat revascularization (31% vs 7%, P <.01), and MACCE (34 vs 12, P <.01) were greater in the PCI vs the CABG group. Similarly, at 12 years, rates of MI (27.4% vs 19.4%, P =.19), repeat revascularization (41.1% vs 20.4%, P <.01), and MACCE (51 vs 40, P =.07) were greater in the PCI group. There were no differences in major outcomes in patients with 1 or 2VD, at 5 or 12 years. Rates of MI, revascularization, and MACCE were higher in patients with 3VD undergoing PCI (n = 21; MI, 47.6%; revascularization, 66.7%; and MACCE, 19 events) vs CABG (n = 78; MI, 19.2%; revascularization, 20.5%; and MACCE, 31 events); P <.01, for all end points. Conclusions: MACCE was lower in young patients undergoing CABG vs PCI at both 5 and 12 years follow-up, primarily as a consequence of patients with 3VD undergoing PCI having more MI and repeat revascularization. CABG should remain the preferred method of revascularization in young patients with 3VD.},
author = {Shafi, Ahmed M.A. and Dhanji, Al Rehan A.A. and Habib, Ahmed M. and Kennon, Simon R.O. and Awad, Wael I.},
doi = {10.1111/jocs.14370},
issn = {15408191},
journal = {Journal of Cardiac Surgery},
number = {2},
pages = {320--327},
pmid = {31803987},
title = {{Coronary artery bypass vs percutaneous coronary intervention in under 50s}},
volume = {35},
year = {2020}
}
@article{Hlatky2009,
author = {Hlatky, Mark A and Boothroyd, Derek B and Bravata, Dena M and Boersma, Eric and Booth, Jean and Brooks, Maria M and Carri{\'{e}}, Didier and Clayton, Tim C and Danchin, Nicolas and Flather, Marcus and Hamm, Christian W and Hueb, Whady A and K{\"{a}}hler, Jan and Kelsey, Sheryl F and King, Spencer B and Kosinski, Andrzej S and Lopes, Neuza and McDonald, Kathryn M and Rodriguez, Alfredo and Serruys, Patrick and Sigwart, Ulrich and Stables, Rodney H and Owens, Douglas K and Pocock, Stuart J},
doi = {10.1016/S0140-6736(09)60552-3},
issn = {01406736},
journal = {The Lancet},
month = {apr},
number = {9670},
pages = {1190--1197},
title = {{Coronary artery bypass surgery compared with percutaneous coronary interventions for multivessel disease: a collaborative analysis of individual patient data from ten randomised trials}},
volume = {373},
year = {2009}
}
@article{Tarr2005,
author = {Tarr, Ferenc I. and Sasv{\'{a}}ri, M{\'{a}}ria and Tarr, M{\'{a}}rton and R{\'{a}}cz, Roz{\'{a}}lia},
doi = {10.1016/j.athoracsur.2005.05.005},
issn = {00034975},
journal = {The Annals of Thoracic Surgery},
month = {nov},
number = {5},
pages = {1728--1731},
title = {{Evidence of Nitric Oxide Produced by the Internal Mammary Artery Graft in Venous Drainage of the Recipient Coronary Artery}},
volume = {80},
year = {2005}
}
@article{Sidhu2022,
author = {Sidhu, Mandeep S. and Alexander, Karen P. and Huang, Zhen and O'Brien, Sean M. and Chaitman, Bernard R. and Stone, Gregg W. and Newman, Jonathan D. and Boden, William E. and Maggioni, Aldo P. and Steg, Philippe Gabriel and Ferguson, Thomas B. and Demkow, Marcin and Peteiro, Jesus and Wander, Gurpreet S. and Phaneuf, Denis C. and {De Belder}, Mark A. and Doerr, Rolf and Alexanderson-Rosas, Erick and Polanczyk, Carisi A. and Henriksen, Peter A. and Conway, Dwayne S.G. and Miro, Vicente and Sharir, Tali and Lopes, Renato D. and Min, James K. and Berman, Daniel S. and Rockhold, Frank W. and Balter, Stephen and Borrego, David and Rosenberg, Yves D. and Bangalore, Sripal and Reynolds, Harmony R. and Hochman, Judith S. and Maron, David J.},
doi = {10.1016/j.ahj.2022.01.017},
issn = {00028703},
journal = {American Heart Journal},
month = {jun},
pages = {72--83},
title = {{Causes of cardiovascular and noncardiovascular death in the ISCHEMIA trial}},
volume = {248},
year = {2022}
}
@article{Zheng2020,
author = {Zheng, Ruyi and Liu, Yan and Hao, Zirui and Liao, Huocheng and Xiao, Chun},
doi = {10.12659/MSM.922957},
issn = {1643-3750},
journal = {Medical Science Monitor},
month = {may},
title = {{Clinical Characteristics and Prognosis of Young Patients with Coronary Heart Disease}},
volume = {26},
year = {2020}
}
@article{Robich2022,
abstract = {Objectives: The goal of this analysis was to examine the comparative effectiveness of coronary artery bypass grafting versus percutaneous coronary intervention among patients aged less than 60 years. Methods: We performed a multicenter, retrospective analysis of all cardiac revascularization procedures from 2005 to 2015 among 7 medical centers. Inclusion criteria were age less than 60 years and 70% stenosis or greater in 1 or more major coronary artery distribution. Exclusion criteria were left main 50% or greater, ST-elevation myocardial infarction, emergency status, and prior revascularization procedure. After applying inclusion and exclusion criteria, the final study cohort included 1945 patients who underwent cardiac surgery and 2938 patients who underwent percutaneous coronary intervention. The primary end point was all-cause mortality stratified by revascularization strategy. Secondary end points included stroke, repeat revascularization, and 30-day mortality. We used inverse probability weighting to balance differences among the groups. Results: After adjustment, there was no significant difference in 30-day mortality (surgery: 0.8%; percutaneous coronary intervention: 0.7%, P = .86) for patients with multivessel disease. Patients undergoing surgery had a higher risk of stroke (1.3% [n = 25] vs 0.07% [n = 2], P < .001). Overall, surgery was associated with superior 10-year survival compared with percutaneous coronary intervention (hazard ratio, 0.71; 95% confidence interval, 0.57-0.88; P = .002). Repeat procedures occurred in 13.4% (n = 270) of the surgery group and 36.4% (n = 1068) of the percutaneous coronary intervention group, with both groups mostly undergoing percutaneous coronary intervention as their second operation. Accounting for death as a competing risk, at 10 years, surgery resulted in a lower cumulative incidence of repeat revascularization compared with percutaneous coronary intervention (subdistribution hazard ratio, 0.34; 95% confidence interval, 0.28-0.40; P < .001). Conclusions: Among patients aged less than 60 years with 2-vessel disease that includes the left anterior descending or 3-vessel coronary artery disease, surgery was associated with greater long-term survival and decreased risk of repeat revascularization.},
author = {Robich, Michael P. and Leavitt, Bruce J. and Ryan, Thomas J. and Westbrook, Benjamin M. and Malenka, David J. and Gelb, Daniel J. and Ross, Cathy S. and Wiseman, Alan and Magnus, Patrick and Huang, Yi Ling and DiScipio, Anthony W. and Iribarne, Alexander},
doi = {10.1016/j.jtcvs.2020.03.164},
issn = {1097685X},
journal = {Journal of Thoracic and Cardiovascular Surgery},
number = {2},
pages = {645--656.e2},
pmid = {32684394},
publisher = {The American Association For Thoracic Surgery},
title = {{Comparative effectiveness of revascularization strategies for early coronary artery disease: A multicenter analysis}},
volume = {163},
year = {2022}
}
@article{Thuijs2019,
abstract = {Background: The Synergy between PCI with Taxus and Cardiac Surgery (SYNTAX) trial was a non-inferiority trial that compared percutaneous coronary intervention (PCI) using first-generation paclitaxel-eluting stents with coronary artery bypass grafting (CABG) in patients with de-novo three-vessel and left main coronary artery disease, and reported results up to 5 years. We now report 10-year all-cause death results. Methods: The SYNTAX Extended Survival (SYNTAXES) study is an investigator-driven extension of follow-up of a multicentre, randomised controlled trial done in 85 hospitals across 18 North American and European countries. Patients with de-novo three-vessel and left main coronary artery disease were randomly assigned (1:1) to the PCI group or CABG group. Patients with a history of PCI or CABG, acute myocardial infarction, or an indication for concomitant cardiac surgery were excluded. The primary endpoint of the SYNTAXES study was 10-year all-cause death, which was assessed according to the intention-to-treat principle. Prespecified subgroup analyses were performed according to the presence or absence of left main coronary artery disease and diabetes, and according to coronary complexity defined by core laboratory SYNTAX score tertiles. This study is registered with ClinicalTrials.gov, NCT03417050. Findings: From March, 2005, to April, 2007, 1800 patients were randomly assigned to the PCI (n=903) or CABG (n=897) group. Vital status information at 10 years was complete for 841 (93%) patients in the PCI group and 848 (95%) patients in the CABG group. At 10 years, 244 (27%) patients had died after PCI and 211 (24%) after CABG (hazard ratio 1{\textperiodcentered}17 [95% CI 0{\textperiodcentered}97–1{\textperiodcentered}41], p=0{\textperiodcentered}092). Among patients with three-vessel disease, 151 (28%) of 546 had died after PCI versus 113 (21%) of 549 after CABG (hazard ratio 1{\textperiodcentered}41 [95% CI 1{\textperiodcentered}10–1{\textperiodcentered}80]), and among patients with left main coronary artery disease, 93 (26%) of 357 had died after PCI versus 98 (28%) of 348 after CABG (0{\textperiodcentered}90 [0{\textperiodcentered}68–1{\textperiodcentered}20], pinteraction=0{\textperiodcentered}019). There was no treatment-by-subgroup interaction with diabetes (pinteraction=0{\textperiodcentered}66) and no linear trend across SYNTAX score tertiles (ptrend=0{\textperiodcentered}30). Interpretation: At 10 years, no significant difference existed in all-cause death between PCI using first-generation paclitaxel-eluting stents and CABG. However, CABG provided a significant survival benefit in patients with three-vessel disease, but not in patients with left main coronary artery disease. Funding: German Fo{\ldots}},
author = {Thuijs, Daniel J.F.M. and Kappetein, A. Pieter and Serruys, Patrick W. and Mohr, Friedrich Wilhelm and Morice, Marie Claude and Mack, Michael J. and Holmes, David R. and Curzen, Nick and Davierwala, Piroze and Noack, Thilo and Milojevic, Milan and Dawkins, Keith D. and da Costa, Bruno R. and J{\"{u}}ni, Peter and Head, Stuart J. and Casselman, Filip and de Bruyne, Bernard and {H{\o}j Christiansen}, Evald and Ruiz-Nodar, Juan M. and Vermeersch, Paul and Schultz, Werner and Sabat{\'{e}}, Manel and Guagliumi, Giulio and Grubitzsch, Herko and Stangl, Karl and Darremont, Olivier and Bentala, M. and den Heijer, Peter and Preda, Istvan and Stoler, Robert and Szerafin, Tam{\'{a}}s and Buckner, John K. and Guber, Myles S. and Verberkmoes, Niels and Akca, Ferdi and Feldman, Ted and Beyersdorf, Friedhelm and Drieghe, Benny and Oldroyd, Keith and Berg, Geoff and Jeppsson, Anders and Barber, Kimberly and Wolschleger, Kevin and Heiser, John and van der Harst, Pim and Mariani, Massimo A. and Reichenspurner, Hermann and Stark, Christoffer and Laine, Mika and Ho, Paul C. and Chen, John C. and Zelman, Richard and Horwitz, Phillip A. and Bochenek, Andrzej and Krauze, Agata and Grothusen, Christina and Dudek, Dariusz and Heyrich, George and Kolh, Philippe and LeGrand, Victor and Coelho, Pedro and Ensminger, Stephan and Nasseri, Boris and Ingemansson, Richard and Olivecrona, Goran and Escaned, Javier and Guera, Reddy and Berti, Sergio and Chieffo, Alaide and Burke, Nicholas and Mooney, Michael and Spolaor, Alvise and Hagl, Christian and N{\"{a}}bauer, Michael and Suttorp, Maarten Jan and Stine, Ronald A. and McGarry, Thomas and Lucas, Scott and Endresen, Knut and Taussig, Andrew and Accola, Kevin and Canosi, Umberto and Horvath, Ivan and Cannon, Louis and Talbott, John D. and Akins, Chris W. and Kramer, Robert and Aschermann, Michael and Killinger, William and Narbute, Inga and Burzotta, Francesco and Bogers, Ad and Zijlstra, Felix and Eltchaninoff, Helene and Berland, Jacques and Stefanini, Giulio and {Cruz Gonzalez}, Ignacio and Hoppe, Uta and Kiesz, Stefan and Gora, Bartlomiej and Ahlsson, Anders and Corbascio, Matthias and Bilfinger, Thomas and Carrie, Didier and Tch{\'{e}}tch{\'{e}}, Didier and Hauptman, Karl Eugen and Stahle, Elisabeth and James, Stefan and Sandner, Sigrid and Laufer, G{\"{u}}nther and Lang, Irene and Witkowski, Adam and Thourani, Vinod and Suryapranata, Harry and Redwood, Simon and Knight, Charles and MacCarthy, Philip and de Belder, Adam and Banning, Adrian and Gershlick, Anthony},
doi = {10.1016/S0140-6736(19)31997-X},
issn = {1474547X},
journal = {The Lancet},
number = {10206},
pages = {1325--1334},
pmid = {31488373},
title = {{Percutaneous coronary intervention versus coronary artery bypass grafting in patients with three-vessel or left main coronary artery disease: 10-year follow-up of the multicentre randomised controlled SYNTAX trial}},
volume = {394},
year = {2019}
}
@misc{Polley2023,
author = {Polley, Eric and LeDell, Erin and Kennedy, Chris and van der Laan, Mark},
title = {{SuperLearner: Super Learner Prediction}},
year = {2023}
}
@article{Sabatine2021,
author = {Sabatine, Marc S and Bergmark, Brian A and Murphy, Sabina A and O'Gara, Patrick T and Smith, Peter K and Serruys, Patrick W and Kappetein, A Pieter and Park, Seung-Jung and Park, Duk-Woo and Christiansen, Evald H and Holm, Niels R and Nielsen, Per H and Stone, Gregg W and Sabik, Joseph F and Braunwald, Eugene},
doi = {10.1016/S0140-6736(21)02334-5},
issn = {01406736},
journal = {The Lancet},
month = {dec},
number = {10318},
pages = {2247--2257},
title = {{Percutaneous coronary intervention with drug-eluting stents versus coronary artery bypass grafting in left main coronary artery disease: an individual patient data meta-analysis}},
volume = {398},
year = {2021}
}
@article{Rastan2009,
abstract = {Background— The objective was to evaluate the impact of complete revascularization (CR) versus reasonable incomplete surgical revascularization (IR) in others than left anterior descending artery territory on early and late survival in patients with multivessel coronary artery disease (CAD).},
author = {Rastan, Ardawan Julian and Walther, Thomas and Falk, Volkmar and Kempfert, Joerg and Merk, Denis and Lehmann, Sven and Holzhey, David and Mohr, Friedrich Wilhelm},
doi = {10.1161/CIRCULATIONAHA.108.842005},
issn = {0009-7322},
journal = {Circulation},
month = {sep},
number = {11_suppl_1},
title = {{Does Reasonable Incomplete Surgical Revascularization Affect Early or Long-Term Survival in Patients With Multivessel Coronary Artery Disease Receiving Left Internal Mammary Artery Bypass to Left Anterior Descending Artery?}},
volume = {120},
year = {2009}
}
@misc{Hastie2023,
author = {Hastie, Trevor},
title = {{gam: Generalized Additive Models}},
url = {https://cran.r-project.org/package=gam},
year = {2023}
}
@article{Head2018,
abstract = {Background: Numerous randomised trials have compared coronary artery bypass grafting (CABG) with percutaneous coronary intervention (PCI) for patients with coronary artery disease. However, no studies have been powered to detect a difference in mortality between the revascularisation strategies. Methods: We did a systematic review up to July 19, 2017, to identify randomised clinical trials comparing CABG with PCI using stents. Eligible studies included patients with multivessel or left main coronary artery disease who did not present with acute myocardial infarction, did PCI with stents (bare-metal or drug-eluting), and had more than 1 year of follow-up for all-cause mortality. In a collaborative, pooled analysis of individual patient data from the identified trials, we estimated all-cause mortality up to 5 years using Kaplan-Meier analyses and compared PCI with CABG using a random-effects Cox proportional-hazards model stratified by trial. Consistency of treatment effect was explored in subgroup analyses, with subgroups defined according to baseline clinical and anatomical characteristics. Findings: We included 11 randomised trials involving 11 518 patients selected by heart teams who were assigned to PCI (n=5753) or to CABG (n=5765). 976 patients died over a mean follow-up of 3{\textperiodcentered}8 years (SD 1{\textperiodcentered}4). Mean Synergy between PCI with Taxus and Cardiac Surgery (SYNTAX) score was 26{\textperiodcentered}0 (SD 9{\textperiodcentered}5), with 1798 (22{\textperiodcentered}1%) of 8138 patients having a SYNTAX score of 33 or higher. 5 year all-cause mortality was 11{\textperiodcentered}2% after PCI and 9{\textperiodcentered}2% after CABG (hazard ratio [HR] 1{\textperiodcentered}20, 95% CI 1{\textperiodcentered}06–1{\textperiodcentered}37; p=0{\textperiodcentered}0038). 5 year all-cause mortality was significantly different between the interventions in patients with multivessel disease (11{\textperiodcentered}5% after PCI vs 8{\textperiodcentered}9% after CABG; HR 1{\textperiodcentered}28, 95% CI 1{\textperiodcentered}09–1{\textperiodcentered}49; p=0{\textperiodcentered}0019), including in those with diabetes (15{\textperiodcentered}5% vs 10{\textperiodcentered}0%; 1{\textperiodcentered}48, 1{\textperiodcentered}19–1{\textperiodcentered}84; p=0{\textperiodcentered}0004), but not in those without diabetes (8{\textperiodcentered}7% vs 8{\textperiodcentered}0%; 1{\textperiodcentered}08, 0{\textperiodcentered}86–1{\textperiodcentered}36; p=0{\textperiodcentered}49). SYNTAX score had a significant effect on the difference between the interventions in multivessel disease. 5 year all-cause mortality was similar between the interventions in patients with left main disease (10{\textperiodcentered}7% after PCI vs 10{\textperiodcentered}5% after CABG; 1{\textperiodcentered}07, 0{\textperiodcentered}87–1{\textperiodcentered}33; p=0{\textperiodcentered}52), regardless of diabetes status and SYNTAX score. Interpretation: CABG had a mortality benefit over PCI in patients with multivessel disease, particularly those with diabetes and higher coronary complexity. No benefit for CABG over PCI was seen in patients with left main disease{\ldots}},
author = {Head, Stuart J. and Milojevic, Milan and Daemen, Joost and Ahn, Jung Min and Boersma, Eric and Christiansen, Evald H. and Domanski, Michael J. and Farkouh, Michael E. and Flather, Marcus and Fuster, Valentin and Hlatky, Mark A. and Holm, Niels R. and Hueb, Whady A. and Kamalesh, Masoor and Kim, Young Hak and M{\"{a}}kikallio, Timo and Mohr, Friedrich W. and Papageorgiou, Grigorios and Park, Seung Jung and Rodriguez, Alfredo E. and Sabik, Joseph F. and Stables, Rodney H. and Stone, Gregg W. and Serruys, Patrick W. and Kappetein, Arie Pieter},
doi = {10.1016/S0140-6736(18)30423-9},
issn = {1474547X},
journal = {The Lancet},
number = {10124},
pages = {939--948},
pmid = {29478841},
title = {{Mortality after coronary artery bypass grafting versus percutaneous coronary intervention with stenting for coronary artery disease: a pooled analysis of individual patient data}},
volume = {391},
year = {2018}
}
@article{Biancari2014,
abstract = {Data on the outcome of young patients after coronary artery bypass grafting (CABG) and percutaneous coronary intervention (PCI) are scarce. Data on 2,209 consecutive patients aged ≤50 years who underwent CABG or PCI were retrospectively collected from 15 European institutions. PCI and CABG had similar 30-day mortality rates (0.8% vs 1.4%, p = 0.27), late survival (at 5 years, 97.8% vs 94.9%, p = 0.082), and freedom from stroke (at 5 years, 98.0% and 98.0%, p = 0.731). PCI was associated with significantly lower freedom from major adverse cardiac and cerebrovascular events (at 5 years, 73.9% vs 85.0%, p 0.0001), repeat revascularization (at 5 years, 77.6% vs 92.5%, p 0.0001), and myocardial infarction (at 5 years, 89.9% vs 96.6%, p 0.0001) compared with CABG. These findings were confirmed in propensity score-adjusted and matched analyses. Freedom from major adverse cardiac and cerebrovascular events after PCI was particularly low in diabetics (at 5 years, 58.0% vs 75.9%, p 0.0001) and in patients with multivessel disease (at 5 years, 63.6% vs 85.1%, p 0.0001). PCI in patients with ST elevation myocardial infarction was associated with significantly better 5-year survival (97.5% vs 88.8%, p = 0.001), which was driven by its lower 30-day mortality rate (1.5% vs 6.0%, p = 0.017). In conclusion, patients aged ;50 years have an excellent immediate outcome after either PCI or CABG with similar long-term survival when used according to the current clinical practice. PCI was associated with significantly lower freedom from myocardial infarction and repeat revascularization. {\textcopyright} 2014 Elsevier Inc. All rights reserved.},
author = {Biancari, Fausto and Gudbjartsson, Tomas and Heikkinen, Jouni and Anttila, Vesa and M{\"{a}}kikallio, Timo and Jeppsson, Anders and Thimour-Bergstr{\"{o}}m, Linda and Mignosa, Carmelo and Rubino, Antonino S. and Kuttila, Kari and Gunn, Jarmo and Wistbacka, Jan Ola and Teittinen, Kari and Korpilahti, Kari and Onorati, Francesco and Faggian, Giuseppe and Vinco, Giulia and Vassanelli, Corrado and Ribichini, Flavio and Juvonen, Tatu and Axelsson, Tomas A. and Sigurdsson, Axel F. and Karjalainen, Pasi P. and Mennander, Ari and Kajander, Olli and Eskola, Markku and Ilveskoski, Erkki and D'Oria, Veronica and {De Feo}, Marisa and Kiviniemi, Tuomas and Airaksinen, K. E.Juhani},
doi = {10.1016/j.amjcard.2014.04.025},
issn = {18791913},
journal = {American Journal of Cardiology},
number = {2},
pages = {198--205},
pmid = {24878127},
publisher = {Elsevier Ltd},
title = {{Comparison of 30-day and 5-year outcomes of percutaneous coronary intervention versus coronary artery bypass grafting in patients aged ≤50 years (the Coronary aRtery diseAse in younG adultS Study)}},
volume = {114},
year = {2014}
}
@article{Spadaccio2016,
author = {Spadaccio, Cristiano and Nappi, Francesco and Nenna, Antonio and Beattie, Gwyn and Chello, Massimo and Sutherland, Fraser W.H.},
doi = {10.1016/j.ijcard.2016.09.055},
issn = {01675273},
journal = {International Journal of Cardiology},
month = {dec},
pages = {295--298},
title = {{Is it time to change how we think about incomplete coronary revascularization?}},
volume = {224},
year = {2016}
}
@article{Investigators2007,
abstract = {Objectives: We sought to compare 10-year clinical outcomes in the BARI (Bypass Angioplasty Revascularization Investigation) trial patients who were randomly assigned to percutaneous transluminal coronary balloon angioplasty (PTCA) versus coronary artery bypass grafting (CABG). Background: Angioplasty and bypass surgery have been compared in numerous studies, but long-term clinical outcomes are limited. Methods: Symptomatic patients with multivessel coronary artery disease (n = 1,829) were randomly assigned to initial treatment with PTCA or CABG and followed up for an average of 10.4 years. Analyses were conducted on an intention-to-treat basis. Results: The 10-year survival was 71.0% for PTCA and 73.5% for CABG (p = 0.18). At 10 years, the PTCA group had substantially higher subsequent revascularization rates than the CABG group (76.8% vs. 20.3%, p < 0.001), but angina rates for the 2 groups were similar. In the subgroup of patients with no treated diabetes, survival rates were nearly identical by randomization (PTCA 77.0% vs. CABG 77.3%, p = 0.59). In the subgroup with treated diabetes, the CABG assigned group had higher survival than the PTCA assigned group (PTCA 45.5% vs. CABG 57.8%, p = 0.025). Conclusions: There was no significant long-term disadvantage regarding mortality or myocardial infarction associated with an initial strategy of PTCA compared with CABG. Among patients with treated diabetes, CABG conferred long-term survival benefit, whereas the 2 initial strategies were equivalent regarding survival for patients without diabetes. {\textcopyright} 2007 American College of Cardiology Foundation.},
author = {Investigators, The BARI},
doi = {10.1016/j.jacc.2006.11.048},
issn = {07351097},
journal = {Journal of the American College of Cardiology},
number = {15},
pages = {1600--1606},
pmid = {17433949},
title = {{The Final 10-Year Follow-Up Results From the BARI Randomized Trial}},
volume = {49},
year = {2007}
}
@article{Buszman2016,
abstract = {Objectives This study has reported 10-year clinical follow-up of patients enrolled in the prospective, randomized LE MANS (Left Main Stenting) trial. Background The very long-term outcome after left main stenting in comparison with surgical revascularization remains unknown. Methods In this prospective, multicenter trial, we randomly assigned 105 patients with unprotected left main coronary artery stenosis with low and medium complexity of coexisting coronary artery disease according to SYNTAX (Synergy Between Percutaneous Coronary Intervention With Taxus and Cardiac Surgery) score to percutaneous coronary intervention (PCI) with stenting (n = 52) or coronary artery bypass grafting (CABG) (n = 53). Drug-eluting stents were implanted in 35%, whereas arterial grafts to the left anterior descending artery were utilized in 81%. Currently, the mean long-term follow-up was collected at 9.8 ± 1.0 years. Follow up for all-cause mortality is complete, whereas the incidence of major adverse cardiovascular and cerebral events (MACCE) was reported from 90% of patients. Ambulatory follow-up was completed in 46 (43.9%) patients. Results At 10 years, there was a trend toward higher ejection fraction in stenting when compared with surgery (54.9 ± 8.3% vs. 49.8 ± 10.3%; p = 0.07). The mortality (21.6% vs. 30.2%; p = 0.41) and MACCE (51.1% vs. 64.4%; p = 0.28) were statistically not different between groups; however, numerically the difference was in favor of stenting. Similarly, there was no difference in the occurrence of myocardial infarction (8.7 vs. 10.4%; p = 0.62), stroke (4.3 vs. 6.3%; p = 0.68), and repeated revascularization rates (26.1% vs. 31.3%; p = 0.64). The probability of very long-term survival up to 14 years was comparable between PCI and CABG (74.2% vs. 67.5%; p = 0.34; hazard ratio: 1.45, 95% confidence interval: 0.67 to 3.13); however, there was a trend toward higher MACCE-free survival in the PCI group (34.7% vs. 22.1%; p = 0.06; hazard ratio: 1.71, 95% confidence interval: 0.97 to 2.99). Conclusions In patients with unprotected left main coronary artery stenosis with low and medium complexity of coexisting coronary artery disease, stenting offers numerically, but statistically nonsignificant, favorable long-term outcome up to 10 years in terms of safety and efficacy outcome measures, therefore, constitutes an alternative therapy for CABG.},
author = {Buszman, Pawel E. and Buszman, Piotr P. and Banasiewicz-Szkr{\'{o}}bka, Iwona and Milewski, Krzysztof P. and Zurakowski, Aleksander and Orlik, Bart{\l}omiej and Konkolewska, Magda and Trela, B{\l}azej and Janas, Adam and Martin, Jack L. and Kiesz, R. Stefan and Bochenek, Andrzej},
doi = {10.1016/j.jcin.2015.10.044},
issn = {18767605},
journal = {JACC: Cardiovascular Interventions},
number = {4},
pages = {318--327},
pmid = {26892080},
title = {{Left main stenting in comparison with surgical revascularization: 10-year outcomes of the (Left Main Coronary Artery Stenting) le MANS trial}},
volume = {9},
year = {2016}
}
@article{Rubin2012,
author = {Rubin, Jessica B. and Borden, William B.},
doi = {10.1007/s11883-012-0226-3},
issn = {1523-3804},
journal = {Current Atherosclerosis Reports},
month = {apr},
number = {2},
pages = {140--149},
pmid = {22249950},
title = {{Coronary Heart Disease in Young Adults}},
volume = {14},
year = {2012}
}
@article{Gaudino2020,
abstract = {Importance: Mortality is a common outcome in trials comparing percutaneous coronary intervention (PCI) with coronary artery bypass grafting (CABG). Controversy exists regarding whether all-cause mortality or cardiac mortality is preferred as a study end point, because noncardiac mortality should be unrelated to the treatment. Objective: To evaluate the difference in all-cause and cause-specific mortality in randomized clinical trials (RCTs) comparing PCI with CABG for the treatment of patients with coronary artery disease. Data Sources: MEDLINE (1946 to the present), Embase (1974 to the present), and the Cochrane Library (1992 to the present) databases were searched on November 24, 2019. Reference lists of included articles were also searched, and additional studies were included if appropriate. Study Selection: Articles were considered for inclusion if they were in English, were RCTs comparing PCI with drug-eluting or bare-metal stents and CABG for the treatment of coronary artery disease, and reported mortality and/or cause-specific mortality. Trials of PCI involving angioplasty without stenting were excluded. For each included trial, the publication with the longest follow-up duration for each outcome was selected. Data Extraction and Synthesis: For data extraction, all studies were reviewed by 2 independent investigators, and disagreements were resolved by a third investigator in accordance with the Preferred Reporting Items for Systematic Reviews and Meta-analyses guideline. Data were pooled using fixed-and random-effects models. Main Outcomes and Measures: The primary outcomes were all-cause and cause-specific (cardiac vs noncardiac) mortality. Subgroup analyses were performed for PCI trials using drug-eluting vs bare-metal stents and for trials involving patients with left main disease. Results: Twenty-three unique trials were included involving 13620 unique patients (6829 undergoing PCI and 6791 undergoing CABG; men, 39.9%-99.0% of study populations; mean age range, 60.0-71.0 years). The weighted mean (SD) follow-up was 5.3 (3.6) years. Compared with CABG, PCI was associated with a higher rate of all-cause (incidence rate ratio, 1.17; 95% CI, 1.05-1.29) and cardiac (incidence rate ratio, 1.24; 95% CI, 1.05-1.45) mortality but also noncardiac mortality (incidence rate ratio, 1.19; 95% CI, 1.00-1.41). Conclusions and Relevance: Percutaneous coronary intervention was associated with higher all-cause, cardiac, and noncardiac mortality compared with {\ldots}},
author = {Gaudino, Mario and Hameed, Irbaz and Farkouh, Michael E. and Rahouma, Mohamed and Naik, Ajita and Robinson, N. Bryce and Ruan, Yongle and Demetres, Michelle and Biondi-Zoccai, Giuseppe and Angiolillo, Dominick J. and Bagiella, Emilia and Charlson, Mary E. and Benedetto, Umberto and Ruel, Marc and Taggart, David P. and Girardi, Leonard N. and Bhatt, Deepak L. and Fremes, Stephen E.},
doi = {10.1001/jamainternmed.2020.4748},
issn = {21686114},
journal = {JAMA Internal Medicine},
number = {12},
pages = {1638--1646},
pmid = {33044497},
title = {{Overall and Cause-Specific Mortality in Randomized Clinical Trials Comparing Percutaneous Interventions with Coronary Bypass Surgery: A Meta-analysis}},
volume = {180},
year = {2020}
}
@misc{gam,
title = {{CRAN - Package gam}},
url = {https://cran.r-project.org/web/packages/gam/index.html},
urldate = {2023-10-20}
}
@article{jul,
month = {jul},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{Super Learner Prediction [R package SuperLearner version 2.0-28.1]}},
url = {https://cran.r-project.org/package=SuperLearner},
year = {2023}
}
@article{Friedman2001,
abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent “boosting” paradigm is developed for additive expansions based on any fitting criterion.Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such “TreeBoost” models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
author = {Friedman, Jerome H.},
doi = {10.1214/AOS/1013203451},
file = {::},
issn = {0090-5364},
journal = {https://doi.org/10.1214/aos/1013203451},
keywords = {62-02,62-07,62-08,62G08,62H30,68T10,Function estimation,boosting,decision trees,robust nonparametric regression},
month = {oct},
number = {5},
pages = {1189--1232},
publisher = {Institute of Mathematical Statistics},
title = {{Greedy function approximation: A gradient boosting machine.}},
url = {https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boosting-machine/10.1214/aos/1013203451.full https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-appro},
volume = {29},
year = {2001}
}
@article{KumarJaiswal2018,
abstract = {Over the past decades, a multitude of experimental drugs have been shown to delay disease progression in preclinical animal models of amyotrophic lateral sclerosis (ALS) but failed to show efficacy in human clinical trials or are still waiting for approval under Phase I-III trials. Riluzole, a glutamatergic neurotrans-mission inhibitor, is the only drug approved by the USA Food and Drug Administration for ALS treatment with modest benefits on survival. Recently, an antioxidant drug, edaravone, developed by Mitsubishi Tanabe Pharma was found to be effective in halting ALS progression during early stages. The newly approved drug edaravone is a force multiplier for ALS treatment. This short report provides an overview of the two drugs that have been approved for ALS treatment and highlights an update on the timeline of drug development, how clinical trials were done, the outcome of these trials, primary endpoint, mechanism of actions, dosing information, administration, side effects, and storage procedures. Moreover, we also discussed the pressing issues and challenges of ALS clinical trials and drug developments as well as future outlook.},
author = {{Kumar Jaiswal}, Manoj and Jaiswal, Kumar},
doi = {10.1002/med.21528},
file = {::},
title = {{Riluzole and edaravone: A tale of two amyotrophic lateral sclerosis drugs}},
url = {https://onlinelibrary.wiley.com/doi/10.1002/med.21528},
year = {2018}
}
@article{Gordon2010,
abstract = {The aim of the study is to determine the shape of the progression curve in ALS, assess the impact of clinical variables on the rate of progression, and evaluate the association between functional decline and survival. Data were prospectively collected and entered into a clinical database from all patients seen in 2002-2008 at the Centre SLA, H{\^{o}}pital de la Salp{\^{e}}tri{\`{e}} re, Paris. Variables analyzed were demographic and baseline information, the ALS functional rating scale (ALSFRS-R), strength testing (MMT), and survival. Generalized additive mixed models characterized changes in ALSFRS-R and MMT scores over time. Linear mixed effects assessed the impact of demographic and clinical measures on rate of progression and Cox models examined their effect on survival. Of 2,452 patients with ALS identified, 1,884 had adequate data for analysis. The ALSFRS-R and MMT declined in a curvilinear way; a quadratic fit described the trends but a linear fit did not. The total ALSFRS-R score was negatively associated with age-of-onset (p < 0.001), and positively associated with baseline ALSFRS-R (p < 0.001) as well as more severe bulbar features (p < 0.001). Higher rate of decline in ALSFRS-R and MMT, older age-at-onset and bulbar-onset predicted shorter survival. Deterioration in ALS is non-linear. The early and late phases of the illness show the most rapid rates of decline. Older age and bulbar signs are associated with a steeper decline, and along with more rapid initial rate of decline, but not current functional status, also predict survival. {\textcopyright} Springer-Verlag 2010.},
author = {Gordon, Paul H. and Cheng, Bin and Salachas, Francois and Pradat, Pierre Francois and Bruneteau, Gaelle and Corcia, Philippe and Lacomblez, Lucette and Meininger, Vincent},
doi = {10.1007/S00415-010-5609-1},
issn = {1432-1459},
journal = {Journal of neurology},
keywords = {Adult,Aged,Amyotrophic Lateral Sclerosis / epidemiology,Amyotrophic Lateral Sclerosis / mortality,Amyotrophic Lateral Sclerosis / physiopathology*,Bin Cheng,Disease Progression*,Female,Humans,MEDLINE,Male,Middle Aged,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Nonlinear Dynamics*,Paul H Gordon,Predictive Value of Tests,Proportional Hazards Models,PubMed Abstract,Retrospective Studies,Survival Analysis,Time Factors,Vincent Meininger,doi:10.1007/s00415-010-5609-1,pmid:20532545},
month = {oct},
number = {10},
pages = {1713--1717},
pmid = {20532545},
publisher = {J Neurol},
title = {{Progression in ALS is not linear but is curvilinear}},
url = {https://pubmed.ncbi.nlm.nih.gov/20532545/},
volume = {257},
year = {2010}
}
@misc{U.S.FoodandDrugAsministrationFDA2023,
author = {{U.S. Food and Drug Asministration (FDA)}},
title = {{FDA Adverse Event Reporting System (FAERS): Latest Quarterly Data Files | FDA}},
url = {https://www.fda.gov/drugs/questions-and-answers-fdas-adverse-event-reporting-system-faers/fda-adverse-event-reporting-system-faers-latest-quarterly-data-files},
urldate = {2023-06-17},
year = {2023}
}
@article{Flach2012,
abstract = {As one of the most comprehensive machine learning texts around, this book does justice to the field's incredible richness, but without losing sight of the unifying principles. Peter Flach's clear, example-based approach begins by discussing how a spam filter works, which gives an immediate introduction to machine learning in action, with a minimum of technical fuss. Flach provides case studies of increasing complexity and variety with well-chosen examples and illustrations throughout. He covers a wide range of logical, geometric and statistical models and state-of-the-art topics such as matrix factorisation and ROC analysis. Particular attention is paid to the central role played by features. The use of established terminology is balanced with the introduction of new and useful concepts, and summaries of relevant background material are provided with pointers for revision if necessary. These features ensure Machine Learning will set a new standard as an introductory textbook.},
author = {Flach, Peter},
doi = {10.1017/CBO9780511973000},
isbn = {9781107096394},
month = {sep},
publisher = {Cambridge University Press},
title = {{Machine Learning: The Art and Science of Algorithms that Make Sense of Data}},
url = {https://www.cambridge.org/core/books/machine-learning/621D3E616DF879E494B094CC93ED36A4},
year = {2012}
}
@misc{Flach,
title = {{Machine Learning: The Art and Science of Algorithms that Make Sense of Data by Peter Flach}},
url = {http://people.cs.bris.ac.uk/$\sim$flach/mlbook//},
urldate = {2023-06-06}
}
@article{Hahsler2023,
abstract = {Mining frequent itemsets and association rules is a popular and well researched approach for discovering interesting relationships between variables in large databases. The R package arules presented in this paper provides a basic infrastructure for creating and manipulating input data sets and for analyzing the resulting itemsets and rules. The package also includes interfaces to two fast mining algorithms, the popular C implementations of Apriori and Eclat by Christian Borgelt. These algorithms can be used to mine frequent itemsets, maximal frequent itemsets, closed frequent itemsets and association rules.},
author = {Hahsler, Michael and Gr{\"{u}}n, Bettina and Hornik, Kurt},
doi = {10.18637/JSS.V014.I15},
file = {::},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {Association rules,Data mining,Frequent itemsets,R},
month = {mar},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{Mining Association Rules and Frequent Itemsets [R package arules version 1.7-6]}},
url = {https://cran.r-project.org/package=arules},
volume = {14},
year = {2023}
}
@article{Yates1934,
abstract = {applicability for this approach.},
author = {Yates, F.},
doi = {10.2307/2983604},
issn = {14666162},
journal = {Supplement to the Journal of the Royal Statistical Society},
number = {2},
pages = {217},
publisher = {JSTOR},
title = {{Contingency Tables Involving Small Numbers and the $\chi$ 2 Test}},
volume = {1},
year = {1934}
}
@article{Rothman2004,
abstract = {Purpose. The proportional reporting ratio (PRR) is the proportion of spontaneous reports for a given drug that are linked to a specific adverse outcome, divided by the corresponding proportion for all or several other drugs. The PRR is similar to the proportional mortality ratio (PMR), an old epidemiologic measure calculated from death registries and constructed in similar fashion to the PRR. The PMR has important deficiencies, however, which the PRR shares. Miettinen and Wang demonstrated that the PMR could be improved by reformulating it as an odds ratio and applying the principles of a case-control study to the measure. In this paper, we review the problem with the PRR and show how the corresponding odds ratio represents an improvement over the PRR. Methods. The method used is discussion and illustration by way of a hypothetical example. Results. The PRR does not estimate relative risk. If, however, a spontaneous report database is viewed as source data for a case-control study, the reporting odds ratio (ROR) can be used to estimate relative risk. Treating the data as source data for a case-control study allows for further reduction of bias by the judicious choice of controls. Conclusions. Calculating the ROR in spontaneous report databases offers advantages over the PRR. It allows for estimation of the relative risk, and focuses attention on which people or reports should be included or excluded from the control series, permitting more deliberate elimination of biases. It also highlights the inherent weaknesses in spontaneous report data, which become more evident in light of the usual principles of control selection in case-control studies. Copyright {\textcopyright} 2004 John Wiley & Sons, Ltd.},
author = {Rothman, Kenneth J. and Lanes, Stephan and Sacks, Susan T.},
doi = {10.1002/PDS.1001},
file = {::},
issn = {10538569},
journal = {Pharmacoepidemiology and Drug Safety},
keywords = {Adverse events,Bias,Drug safety,Mortality odds ratio,Proportional mortality ratio,Proportional reporting ratio,Reporting odds ratio,Spontaneous reports},
month = {aug},
number = {8},
pages = {519--523},
pmid = {15317031},
title = {{The reporting odds ratio and its advantages over the proportional reporting ratio}},
url = {https://www.researchgate.net/publication/8394590_The_reporting_odds_ratio_and_its_advantages_over_proportional_reporting_ratio},
volume = {13},
year = {2004}
}
@article{Andreaggi2020,
abstract = {To perform an in-depth analysis of opioid-related ADRs reported by consumers, manufacturers and healthcare professionals. Delving into the depth and breadth of reported opioid-related adverse drug reactions (ADRs) provides an opportunity to strategize better clinical management and alleviate safety concerns. Retrospective pharmacovigilance disproportionality analysis for opioid-related ADRs in the FDA Adverse Event Reporting System (FAERS) database was performed. Detailed analysis of patient (sex, age) and report (year of report; reporter: healthcare worker vs consumer) characteristics were conducted using reports from 2004 quarter 1 to 2018 quarter 4. Reporting odds ratios and confidence intervals (RORs,CI) were calculated. Of the 1 916 674 ADR reports, 300 985 indicated opioids as the primary medication. There was a surge in opioid-related ADRs reported in 2018 with six times more reports compared to 2004 and twice the number of reports compared to 2017. The largest ROR among the 20 common ADRs was depression and suicide-self-injury (ROR 3.12, 95% CI 3.01-3.22) for reports in age group ≥65 compared to age group 18 to 64, and lack of efficacy (ROR 6.80, 95% CI 6.61-7.00) for males compared to females. ADRs with the largest RORs for consumers included lack of efficacy/effect (ROR 3.37, 95% CI 3.28-3.46), administration site reactions (ROR 3.21, 95% CI 3.11-3.32), depression and suicide self-injury (ROR 2.26, 95% CI 2.14-2.38) compared to healthcare professionals. Important aspects of opioid ADR voluntary reporting included suicidal ideation in elderly patients and lack of efficacy, especially in male patients. This examination provides insight to better manage safety concerns of opioids.},
author = {Andreaggi, Christian A. and Novak, Emily A. and Mirabile, Mitchell E. and Sampathkumar, Shivani and Gray, Matthew P. and He, Meiqi and Kane-Gill, Sandra L.},
doi = {10.1002/PDS.5105},
issn = {1099-1557},
journal = {Pharmacoepidemiology and Drug Safety},
keywords = {adverse drug event,adverse drug reaction reporting systems,analgesics,drug induced abnormalities,error,medication,opioid,pharmacoepidemiology,pharmacovigilance},
month = {dec},
number = {12},
pages = {1627--1635},
pmid = {32851782},
publisher = {John Wiley & Sons, Ltd},
title = {{Safety concerns reported by consumers, manufacturers and healthcare professionals: A detailed evaluation of opioid-related adverse drug reactions in the FDA database over 15 years}},
url = {https://onlinelibrary.wiley.com/doi/full/10.1002/pds.5105 https://onlinelibrary.wiley.com/doi/abs/10.1002/pds.5105 https://onlinelibrary.wiley.com/doi/10.1002/pds.5105},
volume = {29},
year = {2020}
}
@article{Evans2001,
abstract = {Background. The process of generating 'signals' of possible unrecognized hazards from spontaneous adverse drug reaction reporting data has been likened to looking for a needle in a haystack. However, statistical approaches to the data have been underutilised. Methods. Using the UK Yellow Card database, we have developed and evaluated a statistical aid to signal generation called a Proportional Reporting Ratio (PRR). The proportion of all reactions to a drug which are for a particular medical condition of interest is compared to the same proportion for all drugs in the database, in a 2 × 2 table. We investigated a group of newly-marketed drugs using as minimum criteria for a signal, 3 or more cases, PRR at least 2, chi-squared of at least 4. Findings. The database was used to examine retrospectively 15 drugs newly-marketed in the UK, with the highest levels of ADR reporting. The method identified 481 signals meeting the minimum criteria during the period 1996-8. Further evaluation of these showed that 70% were known adverse reactions, 13% were events which were likely to be related to the underlying disease and 17% were signals requiring further evaluation. Implications. Proportional reporting ratios are a valuable aid to signal generation from spontaneous reporting data which are easy to calculate and interpret, and various refinements are possible. {\textcopyright} Crown copyright 2001. Reproduced with the permission of Her Majesty's Stationery Office. Published by John Wiley & Sons, Ltd.},
author = {Evans, S. J.W. and Waller, P. C. and Davis, S.},
doi = {10.1002/PDS.677},
file = {::},
issn = {1053-8569},
journal = {Pharmacoepidemiology and drug safety},
keywords = {Adverse Drug Reaction Reporting Systems / statisti,Data Interpretation,Databases,Factual,MEDLINE,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,P C Waller,PubMed Abstract,S Davis,S J Evans,Software,Statistical,doi:10.1002/pds.677,pmid:11828828},
number = {6},
pages = {483--486},
pmid = {11828828},
publisher = {Pharmacoepidemiol Drug Saf},
title = {{Use of proportional reporting ratios (PRRs) for signal generation from spontaneous adverse drug reaction reports}},
url = {https://pubmed.ncbi.nlm.nih.gov/11828828/},
volume = {10},
year = {2001}
}
@article{Champely2020,
author = {Champely, Stephane},
month = {mar},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{Basic Functions for Power Analysis [R package pwr version 1.3-0]}},
url = {https://cran.r-project.org/package=pwr},
year = {2020}
}
@misc{pwr,
title = {{pwr package - RDocumentation}},
url = {https://www.rdocumentation.org/packages/pwr/versions/1.3-0},
urldate = {2023-04-24}
}
@article{anon,
month = {apr},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{Create 'Table 1' to Describe Baseline Characteristics with or without Propensity Score Weights [R package tableone version 0.13.2]}},
url = {https://cran.r-project.org/package=tableone},
year = {2022}
}
@article{Wickham2022,
author = {Wickham, Hadley},
month = {dec},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{Simple, Consistent Wrappers for Common String Operations [R package stringr version 1.5.0]}},
url = {https://cran.r-project.org/package=stringr},
year = {2022}
}
@article{Garrido2014,
abstract = {Objectives To model the steps involved in preparing for and carrying out propensity score analyses by providing step-by-step guidance and Stata code applied to an empirical dataset. Study Design Guidance, Stata code, and empirical examples are given to illustrate (1) the process of choosing variables to include in the propensity score; (2) balance of propensity score across treatment and comparison groups; (3) balance of covariates across treatment and comparison groups within blocks of the propensity score; (4) choice of matching and weighting strategies; (5) balance of covariates after matching or weighting the sample; and (6) interpretation of treatment effect estimates. Empirical Application We use data from the Palliative Care for Cancer Patients (PC4C) study, a multisite observational study of the effect of inpatient palliative care on patient health outcomes and health services use, to illustrate the development and use of a propensity score. Conclusions Propensity scores are one useful tool for accounting for observed differences between treated and comparison groups. Careful testing of propensity scores is required before using them to estimate treatment effects.},
author = {Garrido, Melissa M. and Kelley, Amy S. and Paris, Julia and Roza, Katherine and Meier, Diane E. and Morrison, R. Sean and Aldridge, Melissa D.},
doi = {10.1111/1475-6773.12182},
file = {::},
issn = {14756773},
journal = {Health Services Research},
keywords = {Observational data/quasi-experiments,administrative data uses,patient outcomes/function},
month = {oct},
number = {5},
pages = {1701},
pmid = {24779867},
publisher = {Health Research & Educational Trust},
title = {{Methods for Constructing and Assessing Propensity Scores}},
url = {/pmc/articles/PMC4213057/ /pmc/articles/PMC4213057/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4213057/},
volume = {49},
year = {2014}
}
@article{Friedman2010,
abstract = {We develop fast algorithms for estimation of generalized linear models with convex penalties. The models include linear regression, two-class logistic regression, and multinomial regression problems while the penalties include l1 (the lasso), l2 (ridge regression) and mixtures of the two (the elastic net). The algorithms use cyclical coordinate descent, computed along a regularization path. The methods can handle large problems and can also deal efficiently with sparse features. In comparative timings we find that the new algorithms are considerably faster than competing methods.},
author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
doi = {10.18637/JSS.V033.I01},
file = {::},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {Coordinate-descent,Elastic net,L1 penalty,Lasso,Logistic regression,Regularization path},
number = {1},
pages = {1--22},
pmid = {20808728},
publisher = {University of California at Los Angeles},
title = {{Regularization paths for generalized linear models via coordinate descent}},
volume = {33},
year = {2010}
}
@article{oct,
month = {oct},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{Recursive Partitioning and Regression Trees [R package rpart version 4.1.19]}},
url = {https://cran.r-project.org/package=rpart},
year = {2022}
}
@misc{raprt,
title = {{CRAN - Package rpart}},
url = {https://cran.r-project.org/web/packages/rpart/index.html},
urldate = {2023-01-20}
}
@article{Sparapani2021,
abstract = {In this article, we introduce the BART R package which is an acronym for Bayesian additive regression trees. BART is a Bayesian nonparametric, machine learning, ensemble predictive modeling method for continuous, binary, categorical and time-to-event out-comes. Furthermore, BART is a tree-based, black-box method which fits the outcome to an arbitrary random function, f, of the covariates. The BART technique is relatively computationally efficient as compared to its competitors, but large sample sizes can be demanding. Therefore, the BART package includes efficient state-of-the-art implemen-tations for continuous, binary, categorical and time-to-event outcomes that can take advantage of modern off-the-shelf hardware and software multi-threading technology. The BART package is written in C++ for both programmer and execution efficiency. The BART package takes advantage of multi-threading via forking as provided by the parallel package and OpenMP when available and supported by the platform. The ensemble of binary trees produced by a BART fit can be stored and re-used later via the R predict function. In addition to being an R package, the installed BART routines can be called directly from C++. The BART package provides the tools for your BART toolbox.},
author = {Sparapani, Rodney and Spanbauer, Charles and McCulloch, Robert},
doi = {10.18637/JSS.V097.I01},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {Binary trees,Black-box,Categorical,Competing risks,Continuous,Ensemble predictive model,Forking,Multi-threading,Multinomial,OpenMP,Recurrent events,Survival analysis},
month = {jan},
number = {1},
pages = {1--66},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{Bayesian Additive Regression Trees [R package BART version 2.9]}},
url = {https://cran.r-project.org/package=BART},
volume = {97},
year = {2021}
}
@article{Breiman2001,
abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, * * *, 148-156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
author = {Breiman, Leo},
doi = {10.1023/A:1010933404324},
issn = {08856125},
journal = {Machine Learning},
keywords = {Classification,Ensemble,Regression},
month = {oct},
number = {1},
pages = {5--32},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{Random forests}},
volume = {45},
year = {2001}
}
@article{Ripley2022,
author = {Ripley, Brian},
month = {sep},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{Feed-Forward Neural Networks and Multinomial Log-Linear Models [R package nnet version 7.3-18]}},
url = {https://cran.r-project.org/package=nnet},
year = {2022}
}
@article{Friedman2010a,
abstract = {We develop fast algorithms for estimation of generalized linear models with convex penalties. The models include linear regression, two-class logistic regression, and multinomial regression problems while the penalties include l1 (the lasso), l2 (ridge regression) and mixtures of the two (the elastic net). The algorithms use cyclical coordinate descent, computed along a regularization path. The methods can handle large problems and can also deal efficiently with sparse features. In comparative timings we find that the new algorithms are considerably faster than competing methods.},
author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
doi = {10.18637/JSS.V033.I01},
file = {::},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {Coordinate-descent,Elastic net,L1 penalty,Lasso,Logistic regression,Regularization path},
number = {1},
pages = {1--22},
pmid = {20808728},
publisher = {University of California at Los Angeles},
title = {{Regularization paths for generalized linear models via coordinate descent}},
volume = {33},
year = {2010}
}
@article{Wood2017,
abstract = {The first edition of this book has established itself as one of the leading references on generalized additive models (GAMs), and the only book on the topic to be introductory in nature with a wealth of practical examples and software implementation. It is self-contained, providing the necessary background in linear models, linear mixed models, and generalized linear models (GLMs), before presenting a balanced treatment of the theory and applications of GAMs and related models. The author bases his approach on a framework of penalized regression splines, and while firmly focused on the practical aspects of GAMs, discussions include fairly full explanations of the theory underlying the methods. Use of R software helps explain the theory and illustrates the practical application of the methodology. Each chapter contains an extensive set of exercises, with solutions in an appendix or in the book's R data package gamair, to enable use as a course text or for self-study.},
author = {Wood, Simon N.},
doi = {10.1201/9781315370279},
isbn = {9781498728348},
journal = {Generalized Additive Models: An Introduction with R, Second Edition},
month = {jan},
pages = {1--476},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{Generalized additive models: An introduction with R, second edition}},
year = {2017}
}
@article{Diamond2013,
abstract = {This paper presents genetic matching, a method of multivariate matching that uses an evolutionary search algorithm to determine the weight each covariate is given. Both propensity score matching and matching based on Mahalanobis distance are limiting cases of this method. The algorithm makes transparent certain issues that all matching methods must confront. We present simulation studies that showthat the algorithm improves covariate balance and that it may reduce bias if the selection on observables assumption holds. We then present a reanalysis of a number of data sets in the LaLonde (1986) controversy. {\textcopyright} 2013 by the President and Fellows of Harvard College and the Massachusetts Institute of Technology.},
author = {Diamond, Alexis and Sekhon, Jasjeet S.},
doi = {10.1162/REST_A_00318},
issn = {0034-6535},
journal = {The Review of Economics and Statistics},
month = {jul},
number = {3},
pages = {932--945},
publisher = {MIT Press},
title = {{Genetic Matching for Estimating Causal Effects: A General Multivariate Matching Method for Achieving Balance in Observational Studies}},
url = {https://direct.mit.edu/rest/article/95/3/932/58101/Genetic-Matching-for-Estimating-Causal-Effects-A},
volume = {95},
year = {2013}
}
@article{Stuart2008,
abstract = {Matching methods such as nearest neighbor propensity score matching are increasingly popular techniques for controlling confounding in nonexperimental studies. However, simple k:1 matching methods, which select k well-matched comparison individuals for each treated individual, are sometimes criticized for being overly restrictive and discarding data (the unmatched comparison individuals). The authors illustrate the use of a more flexible method called full matching. Full matching makes use of all individuals in the data by forming a series of matched sets in which each set has either 1 treated individual and multiple comparison individuals or 1 comparison individual and multiple treated individuals. Full matching has been shown to be particularly effective at reducing bias due to observed confounding variables. The authors illustrate this approach using data from the Woodlawn Study, examining the relationship between adolescent marijuana use and adult outcomes. {\textcopyright} 2008 American Psychological Association.},
author = {Stuart, Elizabeth A. and Green, Kerry M.},
doi = {10.1037/0012-1649.44.2.395},
issn = {00121649},
journal = {Developmental Psychology},
keywords = {long-term consequences,longitudinal studies,observational study,propensity score,substance use},
month = {mar},
number = {2},
pages = {395--406},
pmid = {18331131},
title = {{Using Full Matching to Estimate Causal Effects in Nonexperimental Studies: Examining the Relationship Between Adolescent Marijuana Use and Adult Outcomes}},
url = {/doiLanding?doi=10.1037%2F0012-1649.44.2.395},
volume = {44},
year = {2008}
}
@article{Hansen2012,
abstract = {Among matching techniques for observational studies, full matching is in principle the best, in the sense that its alignment of comparable treated and control subjects is as good as that of any alt...},
author = {Hansen, Ben B.},
doi = {10.1198/016214504000000647},
issn = {01621459},
journal = {https://doi.org/10.1198/016214504000000647},
keywords = {Graph algorithm,Matching with multiple controls,Network flow,Optimal matching,Propensity score,Quasiexperiment},
month = {sep},
number = {467},
pages = {609--618},
publisher = {Taylor & Francis},
title = {{Full Matching in an Observational Study of Coaching for the SAT}},
url = {https://www.tandfonline.com/doi/abs/10.1198/016214504000000647},
volume = {99},
year = {2012}
}
@article{Austin2014,
abstract = {Propensity-score matching is increasingly being used to reduce the confounding that can occur in observational studies examining the effects of treatments or interventions on outcomes. We used Monte Carlo simulations to examine the following algorithms for forming matched pairs of treated and untreated subjects: optimal matching, greedy nearest neighbor matching without replacement, and greedy nearest neighbor matching without replacement within specified caliper widths. For each of the latter two algorithms, we examined four different sub-algorithms defined by the order in which treated subjects were selected for matching to an untreated subject: lowest to highest propensity score, highest to lowest propensity score, best match first, and random order. We also examined matching with replacement. We found that (i) nearest neighbor matching induced the same balance in baseline covariates as did optimal matching; (ii) when at least some of the covariates were continuous, caliper matching tended to induce balance on baseline covariates that was at least as good as the other algorithms; (iii) caliper matching tended to result in estimates of treatment effect with less bias compared with optimal and nearest neighbor matching; (iv) optimal and nearest neighbor matching resulted in estimates of treatment effect with negligibly less variability than did caliper matching; (v) caliper matching had amongst the best performance when assessed using mean squared error; (vi) the order in which treated subjects were selected for matching had at most a modest effect on estimation; and (vii) matching with replacement did not have superior performance compared with caliper matching without replacement. {\textcopyright} 2013 The Authors. Statistics in Medicine published by John Wiley & Sons, Ltd.},
author = {Austin, Peter C.},
doi = {10.1002/SIM.6004},
file = {::},
issn = {1097-0258},
journal = {Statistics in Medicine},
keywords = {Monte Carlo simulations,computer algorithms,matching,optimal matching,propensity,propensity score,score matching},
month = {mar},
number = {6},
pages = {1057--1069},
pmid = {24123228},
publisher = {John Wiley & Sons, Ltd},
title = {{A comparison of 12 algorithms for matching on the propensity score}},
url = {https://onlinelibrary.wiley.com/doi/full/10.1002/sim.6004 https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.6004 https://onlinelibrary.wiley.com/doi/10.1002/sim.6004},
volume = {33},
year = {2014}
}
@article{Austin2010,
abstract = {Propensity-score matching is increasingly being used to estimate the effects of treatments using observational data. In many-to-one (M:1) matching on the propensity score, M untreated subjects are matched to each treated subject using the propensity score. The authors used Monte Carlo simulations to examine the effect of the choice of M on the statistical performance of matched estimators. They considered matching 1-5 untreated subjects to each treated subject using both nearest-neighbor matching and caliper matching in 96 different scenarios. Increasing the number of untreated subjects matched to each treated subject tended to increase the bias in the estimated treatment effect; conversely, increasing the number of untreated subjects matched to each treated subject decreased the sampling variability of the estimated treatment effect. Using nearest-neighbor matching, the mean squared error of the estimated treatment effect was minimized in 67.7% of the scenarios when 1:1 matching was used. Using nearest-neighbor matching or caliper matching, the mean squared error was minimized in approximately 84% of the scenarios when, at most, 2 untreated subjects were matched to each treated subject. The authors recommend that, in most settings, researchers match either 1 or 2 untreated subjects to each treated subject when using propensity-score matching. {\textcopyright} The Author 2010.},
author = {Austin, Peter C.},
doi = {10.1093/AJE/KWQ224},
file = {::},
issn = {0002-9262},
journal = {American Journal of Epidemiology},
keywords = {bias (epidemiology),matching,observational study,propensity score,propensity score method},
month = {nov},
number = {9},
pages = {1092--1097},
pmid = {20802241},
publisher = {Oxford Academic},
title = {{Statistical Criteria for Selecting the Optimal Number of Untreated Subjects Matched to Each Treated Subject When Using Many-to-One Matching on the Propensity Score}},
url = {https://academic.oup.com/aje/article/172/9/1092/147493},
volume = {172},
year = {2010}
}
@article{Zakrison2018,
abstract = {Background: Propensity score methods are techniques commonly employed in observational research to account for confounding when estimating the effects of treatments and exposures. These methods have been increasingly employed in the acute care surgery literature in an attempt to infer causality; however, the adequacy of reporting and the appropriateness of statistical analyses when using propensity score matching remain unclear. Objectives: The goal of this systematic review is to assess the adequacy of reporting of propensity score methods, with an emphasis on propensity score matching (to assess balance and the use of appropriate statistical tests), in acute care surgery (ACS) studies and to provide suggestions for improvement for junior investigators. Methods: We searched three databases, and other relevant literature (from January 2005 to June 2015) to identify observational studies within the ACS literature using propensity score methods (PROSPERO No: CRD42016036432). Two reviewers extracted data and assessed the quality of the studies retrieved by reviewing the adequacy of both overall reporting and of the propensity score matching methods used. Results: A total of 49/71 (69%) of studies adequately reported propensity score methods overall. Matching was the most common propensity score method used in 46/71 (65%) studies, with 36/46 (78%) studies reporting matching methods adequately. Only 19/46 (41%) of matching studies reported the balance of baseline characteristics between treated and untreated subjects while 6/46 (13%) used correct statistical methods to assess balance. There were 35/46 (76%) of matching studies that explicitly used statistical methods appropriate for the analysis of matched data when estimating the treatment effect and its statistical significance. Conclusion: We have proposed reporting guidelines for the use of propensity score methods in the acute care surgery literature. This is to help investigators improve the adequacy of reporting and statistical analyses when using observational data to estimate effects of treatments and exposures.},
author = {Zakrison, T. L. and Austin, P. C. and McCredie, V. A.},
doi = {10.1007/S00068-017-0786-6/METRICS},
issn = {16153146},
journal = {European Journal of Trauma and Emergency Surgery},
keywords = {Acute care,Matching,Propensity score,Surgery,Trauma},
month = {jun},
number = {3},
pages = {385--395},
pmid = {28342097},
publisher = {Springer Berlin Heidelberg},
title = {{A systematic review of propensity score methods in the acute care surgery literature: avoiding the pitfalls and proposing a set of reporting guidelines}},
url = {https://link.springer.com/article/10.1007/s00068-017-0786-6},
volume = {44},
year = {2018}
}
@article{Thoemmes2011,
abstract = {The use of propensity scores in psychological and educational research has been steadily increasing in the last 2 to 3 years. However, there are some common misconceptions about the use of differen...},
author = {Thoemmes, Felix J. and Kim, Eun Sook},
doi = {10.1080/00273171.2011.540475},
issn = {00273171},
journal = {https://doi.org/10.1080/00273171.2011.540475},
month = {jan},
number = {1},
pages = {90--118},
publisher = { Taylor & Francis Group },
title = {{A Systematic Review of Propensity Score Methods in the Social Sciences}},
url = {https://www.tandfonline.com/doi/abs/10.1080/00273171.2011.540475},
volume = {46},
year = {2011}
}
@article{Ho2011,
abstract = {MatchIt implements the suggestions of Ho, Imai, King, and Stuart (2007) for improving parametric statistical models by preprocessing data with nonparametric matching methods. MatchIt implements a wide range of sophisticated matching methods, making it possible to greatly reduce the dependence of causal inferences on hard-to-justify, but commonly made, statistical modeling assumptions. The software also easily fits into existing research practices since, after preprocessing data with MatchIt, researchers can use whatever parametric model they would have used without MatchIt, but produce inferences with substantially more robustness and less sensitivity to modeling assumptions. MatchIt is an R program, and also works seamlessly with Zelig.},
author = {Ho, Daniel E. and Imai, Kosuke and King, Gary and Stuart, Elizabeth A.},
doi = {10.18637/JSS.V042.I08},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {Balance,Causal inference,Matching methods,Preprocessing,R},
number = {8},
pages = {1--28},
publisher = {American Statistical Association},
title = {{MatchIt: Nonparametric preprocessing for parametric causal inference}},
volume = {42},
year = {2011}
}
@article{Ho2007,
abstract = {Although published works rarely include causal estimates from more than a few model specifications, authors usually choose the presented estimates from numerous trial runs readers never see. Given the often large variation in estimates across choices of control variables, functional forms, and other modeling assumptions, how can researchers ensure that the few estimates presented are accurate or representative? How do readers know that publications are not merely demonstrations that it is possible to find a specification that fits the author's favorite hypothesis? And how do we evaluate or even define statistical properties like unbiasedness or mean squared error when no unique model or estimator even exists? Matching methods, which offer the promise of causal inference with fewer assumptions, constitute one possible way forward, but crucial results in this fast-growing methodological literature are often grossly misinterpreted. We explain how to avoid these misinterpretations and propose a unified approach that makes it possible for researchers to preprocess data with matching (such as with the easy-to-use software we offer) and then to apply the best parametric techniques they would have used anyway. This procedure makes parametric models produce more accurate and considerably less model-dependent causal inferences.},
author = {Ho, Daniel E and Imai, Kosuke and King, Gary and Stuart, Elizabeth A and Abadie, Alberto and Beck, Neal and Cook, Sam and Diamond, Alexis and Hansen, Ben and Imbens, Guido and Lau, Olivia and Lenz, Gabe and Rosenbaum, Paul and Rubin, Don},
doi = {10.1093/PAN/MPL013},
file = {::},
issn = {1047-1987},
journal = {Political Analysis},
month = {jun},
number = {3},
pages = {199--236},
publisher = {Cambridge University Press},
title = {{Matching as Nonparametric Preprocessing for Reducing Model Dependence in Parametric Causal Inference}},
url = {https://www.cambridge.org/core/journals/political-analysis/article/matching-as-nonparametric-preprocessing-for-reducing-model-dependence-in-parametric-causal-inference/4D7E6D07C9727F5A604E5C9FCCA2DD21},
volume = {15},
year = {2007}
}
@article{Stuart2013,
abstract = {Objective: Examining covariate balance is the prescribed method for determining the degree to which propensity score methods should be successful at reducing bias. This study assessed the performance of various balance measures, including a proposed balance measure based on the prognostic score (similar to a disease risk score), to determine which balance measures best correlate with bias in the treatment effect estimate. Study Design and Setting: The correlations of multiple common balance measures with bias in the treatment effect estimate produced by weighting by the odds, subclassification on the propensity score, and full matching on the propensity score were calculated. Simulated data were used, based on realistic data settings. Settings included both continuous and binary covariates and continuous covariates only. Results: The absolute standardized mean difference (ASMD) in prognostic scores, the mean ASMD (in covariates), and the mean t-statistic all had high correlations with bias in the effect estimate. Overall, prognostic scores displayed the highest correlations with bias of all the balance measures considered. Prognostic score measure performance was generally not affected by model misspecification, and the prognostic score measure performed well under a variety of scenarios. Conclusion: Researchers should consider using prognostic score-based balance measures for assessing the performance of propensity score methods for reducing bias in nonexperimental studies. {\textcopyright} 2013 Elsevier Inc. All rights reserved.},
author = {Stuart, Elizabeth A. and Lee, Brian K. and Leacy, Finbarr P.},
doi = {10.1016/J.JCLINEPI.2013.01.013},
file = {::},
issn = {1878-5921},
journal = {Journal of clinical epidemiology},
keywords = {Brian K Lee,Causality,Comparative Effectiveness Research / statistics & ,Computer Simulation,Confounding Factors,Control Groups,Data Interpretation,Elizabeth A Stuart,Epidemiologic,Extramural,Finbarr P Leacy,Humans,MEDLINE,Models,N.I.H.,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,PMC3713509,Prognosis*,Propensity Score*,PubMed Abstract,Research Support,Statistical,Statistical*,Treatment Outcome,doi:10.1016/j.jclinepi.2013.01.013,pmid:23849158},
number = {8 Suppl},
pmid = {23849158},
publisher = {J Clin Epidemiol},
title = {{Prognostic score-based balance measures can be a useful diagnostic for propensity score methods in comparative effectiveness research}},
url = {https://pubmed.ncbi.nlm.nih.gov/23849158/},
volume = {66},
year = {2013}
}
@article{Schafer2008a,
abstract = {In a well-designed experiment, random assignment of participants to treatments makes causal inference straightforward. However, if participants are not randomized (as in observational study, quasi-experiment, or nonequivalent control-group designs), group comparisons may be biased by confounders that influence both the outcome and the alleged cause. Traditional analysis of covariance, which includes confounders as predictors in a regression model, often fails to eliminate this bias. In this article, the authors review Rubin's definition of an average causal effect (ACE) as the average difference between potential outcomes under different treatments. The authors distinguish an ACE and a regression coefficient. The authors review 9 strategies for estimating ACEs on the basis of regression, propensity scores, and doubly robust methods, providing formulas for standard errors not given elsewhere. To illustrate the methods, the authors simulate an observational study to assess the effects of dieting on emotional distress. Drawing repeated samples from a simulated population of adolescent girls, the authors assess each method in terms of bias, efficiency, and interval coverage. Throughout the article, the authors offer insights and practical guidance for researchers who attempt causal inference with observational data. {\textcopyright} 2008 American Psychological Association.},
author = {Schafer, Joseph L. and Kang, Joseph},
doi = {10.1037/A0014268},
issn = {1082989X},
journal = {Psychological Methods},
keywords = {Rubin's causal model,nonequivalent control group design,propensity scores},
month = {dec},
number = {4},
pages = {279--313},
pmid = {19071996},
title = {{Average Causal Effects From Nonrandomized Studies: A Practical Guide and Simulated Example}},
url = {/doiLanding?doi=10.1037%2Fa0014268},
volume = {13},
year = {2008}
}
@article{Snowden2011,
abstract = {The growing body of work in the epidemiology literature focused on G-computation includes theoretical explanations of the method but very few simulations or examples of application. The small number of G-computation analyses in the epidemiology literature relative to other causal inference approaches may be partially due to a lack of didactic explanations of the method targeted toward an epidemiology audience. The authors provide a step-by-step demonstration of G-computation that is intended to familiarize the reader with this procedure. The authors simulate a data set and then demonstrate both G-computation and traditional regression to draw connections and illustrate contrasts between their implementation and interpretation relative to the truth of the simulation protocol. A marginal structural model is used for effect estimation in the G-computation example. The authors conclude by answering a series of questions to emphasize the key characteristics of causal inference techniques and the G-computation procedure in particular. {\textcopyright} 2011 The Author.},
author = {Snowden, Jonathan M. and Rose, Sherri and Mortimer, Kathleen M.},
doi = {10.1093/AJE/KWQ472},
file = {::},
issn = {0002-9262},
journal = {American Journal of Epidemiology},
keywords = {datasets,epidemiologic causality,epidemiology,methods,regression analysis},
month = {apr},
number = {7},
pages = {731--738},
pmid = {21415029},
publisher = {Oxford Academic},
title = {{Implementation of G-Computation on a Simulated Data Set: Demonstration of a Causal Inference Technique}},
url = {https://academic.oup.com/aje/article/173/7/731/104142},
volume = {173},
year = {2011}
}
@article{Inacio2015,
author = {Inacio, Maria C.S. and Chen, Yuexin and Paxton, Elizabeth W. and Namba, Robert S. and Kurtz, Steven M. and Cafri, Guy},
doi = {10.1007/S11999-015-4239-4},
file = {::},
issn = {15281132},
journal = {Clinical Orthopaedics and Related Research},
month = {aug},
number = {8},
pages = {2722},
pmid = {25773902},
publisher = {Association of Bone and Joint Surgeons},
title = {{Statistics in Brief: An Introduction to the Use of Propensity Scores}},
url = {/pmc/articles/PMC4488189/ https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4488189/},
volume = {473},
year = {2015}
}
@article{D2004,
abstract = {Clinical guidelines are only as good as the evidence and judgments they are based on. The GRADE approach aims to make it easier for users to assess the judgments behind recommendations.},
author = {D, Atkins and D, Best and PA, Briss and M, Eccles and Y, Falck-Ytter and S, Flottorp and GH, Guyatt and RT, Harbour and MC, Haugh and D, Henry and S, Hill and R, Jaeschke and G, Leng and A, Liberati and N, Magrini and J, Mason and P, Middleton and J, Mrukowicz and D, O'Connell and AD, Oxman and B, Phillips and HJ, Sch{\"{u}}nemann and T, Edejer and H, Varonen and GE, Vist and JW, Williams and S, Zaza},
doi = {10.1136/BMJ.328.7454.1490},
issn = {1756-1833},
journal = {BMJ (Clinical research ed.)},
keywords = {CollabAuthor(name='GRADE Working Group',Cost-Benefit Analysis,Dana Best,David Atkins,Evidence-Based Medicine*,Health Care,Humans,MEDLINE,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Non-U.S. Gov't,PMC428525,Practice Guidelines as Topic / standards*,PubMed Abstract,Quality Assurance,Research Support,affs=[]),doi:10.1136/bmj.328.7454.1490,investigators=[],pmid:15205295},
month = {jun},
number = {7454},
pages = {1490--1494},
pmid = {15205295},
publisher = {BMJ},
title = {{Grading quality of evidence and strength of recommendations}},
url = {https://pubmed.ncbi.nlm.nih.gov/15205295/},
volume = {328},
year = {2004}
}
@article{Kleopa1999,
abstract = {Amyotrophic Lateral Sclerosis (ALS) is a progressive motor neuron disease that frequently causes death within five years of diagnosis. The majority of deaths are due to pulmonary complications resulting from respiratory muscle weakness and bulbar involvement. A promising respiratory intervention is the recently introduced bi-level intermittent positive pressure (Bipap), which is a noninvasive ventilator modality shown to reduce the work of breathing and improve not only gas exchange, but also exercise tolerance and sleep quality. The aim of this study was to assess the utility of Bipap in prolonging survival in ALS. We retrospectively analyzed the results of Bipap use in 122 patients followed at Hahnemann University. All patients in this study were offered Bipap when their forced vital capacity (FVC) dropped below 50% of predicted value. Group 1 (n=38) accepted Bipap and used it more than 4 h/day. Group 2 (n= 32) did not tolerate Bipap well and used it less than 4 h/day. Group 3 (n=52) refused to try Bipap. There was a statistically significant improvement in survival from initiation of Bipap in Group 1 (14.2 months) compared to Group 2 (7.0 months, P=0.002) or 3 (4.6 months, P<0.001) respectively. Furthermore, when the slope of vital capacity decline was examined, the group that used Bipap more than 4 h/day had slower decline in vital capacity (-3.5% change/month) compared to Group 2 (-5.9% change/month, P=0.02) and Group 3 (-8.3% change/month,, P<0.001). We conclude that Bipap can significantly prolong survival and slow the decline of FVC in ALS. Our results suggest that all patients with ALS be offered Bipap when their FVC drops below 50%, at the onset of dyspnea, or when a rapid drop in %FVC is noted.},
author = {Kleopa, Kleopas A. and Sherman, Michael and Neal, Bettie and Romano, Gary J. and Heiman-Patterson, Terry},
doi = {10.1016/S0022-510X(99)00045-3},
issn = {0022-510X},
journal = {Journal of the neurological sciences},
keywords = {Adult,Aged,Amyotrophic Lateral Sclerosis / complications,Amyotrophic Lateral Sclerosis / mortality,Amyotrophic Lateral Sclerosis / physiopathology,Female,Humans,Intermittent Positive-Pressure Ventilation / metho,Intermittent Positive-Pressure Ventilation / morta,K A Kleopa,Lung / physiopathology,M Sherman,MEDLINE,Male,Middle Aged,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,PubMed Abstract,Respiratory Insufficiency / etiology,Respiratory Insufficiency / mortality,Respiratory Insufficiency / physiopathology,Respiratory Insufficiency / therapy*,Retrospective Studies,Survival Rate,T Heiman-Patterson,doi:10.1016/s0022-510x(99)00045-3,pmid:10385053},
month = {mar},
number = {1},
pages = {82--88},
pmid = {10385053},
publisher = {J Neurol Sci},
title = {{Bipap improves survival and rate of pulmonary function decline in patients with ALS}},
url = {https://pubmed.ncbi.nlm.nih.gov/10385053/},
volume = {164},
year = {1999}
}
@article{Sancho2018,
abstract = {There is general agreement that noninvasive ventilation (NIV) prolongs survival in amyotrophic lateral sclerosis (ALS) and that the main cause of NIV failure is the severity of bulbar dysfunction. However, there is no evidence that bulbar impairment is a contraindication for NIV. The aim of this study was to determine the effect of bulbar impairment on survival in ALS patients with NIV. ALS patients for whom NIV was indicated were included. Those patients who refused NIV were taken as the control group. 120 patients who underwent NIV and 20 who refused NIV were included. The NIV group presented longer survival (median 18.50 months, 95% CI 12.62–24.38 months) than the no-NIV group (3.00 months, 95% CI 0.82–5.18 months) (p<0.001) and also in those patients with severe bulbar dysfunction (13.00 months (95% CI 9.49–16.50 months) versus 3.00 months (95% CI 0.85–5.15 months), p<0.001). Prognostic factors for ALS using NIV, adjusted for NIV failure, were severity of bulbar dysfunction (hazard ratio (HR) 0.5, 95% CI 0.92–0.97; p=0.001) and time spent with oxygen saturation measured by pulse oximetry <90% (%sleepSpO2<90) using NIV (HR 1.12, 95% CI 1.01–1.24; p=0.02). Severe bulbar impairment in ALS does not always prevent NIV from being used, but the severity of bulbar dysfunction at NIV initiation and %sleepSpO2<90 while using NIV appear to be the main prognostic factors of NIV failure in ALS.},
author = {Sancho, Jes{\'{u}}s and Mart{\'{i}}nez, Daniel and Bures, Enric and D{\'{i}}az, Jos{\'{e}} Luis and Ponz, Alejandro and Servera, Emilio},
doi = {10.1183/23120541.00159-2017},
file = {::},
issn = {23120541},
journal = {ERJ Open Research},
month = {apr},
number = {2},
pmid = {29670892},
publisher = {European Respiratory Society},
title = {{Bulbar impairment score and survival of stable amyotrophic lateral sclerosis patients after noninvasive ventilation initiation}},
url = {/pmc/articles/PMC5900060/ /pmc/articles/PMC5900060/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5900060/},
volume = {4},
year = {2018}
}
@article{Berlowitz2016,
abstract = {Background: Respiratory failure is associated with significant morbidity and is the predominant cause of death in motor neurone disease/amyotrophic lateral sclerosis (MND/ALS). This study aimed to determine the effect of non-invasive ventilatory (NIV) support on survival and pulmonary function decline across MND/ALS phenotypes. Methods: Cohort recruited via a specialist, multidisciplinary clinic. Patients were categorised into four clinical phenotypes (ALS, flail arm, flail leg and primary lateral sclerosis) according to site of presenting symptom and the pattern of upper versus lower motor neurone involvement. NIV was initiated according to current consensus practice guidelines. Results: Between 1991 and 2011, 1198 patients diagnosed with ALS/MND were registered. 929 patients (77.5%) fulfilled the selection criteria and their data were analysed. Median tracheostomy free survival from symptom onset was 28 months in NIV-treated patients compared to 15 months in untreated (Univariate Cox regression HR=0.61 (0.51 to 0.73), p<0.001). The positive survival effect of NIV persisted when the model was adjusted for age, gender, riluzole and percutaneous endoscopic gastrostomy use (HR=0.72 (0.60 to 0.88, p=0.001). In contrast with the only randomised controlled trial, NIV statistically significantly increased survival by 19 months in those with ALS-bulbar onset (Univariate HR=0.50 (0.36 to 0.70), multivariate HR=0.59 (0.41 to 0.83)). These data confirm that NIV improves survival in MND/ALS. The overall magnitude of benefit is 13 months and was largest in those with ALS-bulbar disease. Future research should explore the optimal timing of NIV initiation within phenotypes in order to optimise respiratory function, quality of life and survival.},
author = {Berlowitz, David J. and Howard, Mark E. and Fiore, Julio F. and {Van Der Hoorn}, Stephen and O'Donoghue, Fergal J. and Westlake, Justine and Smith, Anna and Beer, Fiona and Mathers, Susan and Talman, Paul},
doi = {10.1136/JNNP-2014-310055},
issn = {1468-330X},
journal = {Journal of neurology, neurosurgery, and psychiatry},
keywords = {Amyotrophic Lateral Sclerosis / diagnosis,Amyotrophic Lateral Sclerosis / therapy*,Cohort Studies,Databases,David J Berlowitz,Factual,Female,Humans,MEDLINE,Male,Mark E Howard,Middle Aged,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Non-U.S. Gov't,Noninvasive Ventilation*,Paul Talman,PubMed Abstract,Quality of Life,Research Support,Retrospective Studies,Survival Analysis,Treatment Outcome,doi:10.1136/jnnp-2014-310055,pmid:25857659},
month = {mar},
number = {3},
pages = {280--286},
pmid = {25857659},
publisher = {J Neurol Neurosurg Psychiatry},
title = {{Identifying who will benefit from non-invasive ventilation in amyotrophic lateral sclerosis/motor neurone disease in a clinical cohort}},
url = {https://pubmed.ncbi.nlm.nih.gov/25857659/},
volume = {87},
year = {2016}
}
@article{Hirose2018,
abstract = {Introduction: We evaluated post–noninvasive ventilation survival and factors for the transition to tracheostomy in amyotrophic lateral sclerosis (ALS). Methods: We analyzed 197 patients using a prospectively collected database with 114 patients since 2000. Results: Among 114 patients, 59 patients underwent noninvasive ventilation (NIV), which prolonged the total median survival time to 43 months compared with 32 months without treatment. The best post-NIV survival was associated with a lack of bulbar symptoms, higher measured pulmonary function, and a slower rate of progression at diagnosis. The transition rate from NIV to tracheostomy gradually decreased over the years. Patients using NIV for more than 6 months were more likely to refuse tracheostomy and to be women. Discussion: This study confirmed a positive survival effect with NIV, which was less effective in patients with bulbar dysfunction. Additional studies are required to determine the best timing for using NIV with ALS in patients with bulbar dysfunction. Muscle Nerve 58:770–776 2018.},
author = {Hirose, Takahiko and Kimura, Fumiharu and Tani, Hiroki and Ota, Shin and Tsukahara, Akihiro and Sano, Eri and Shigekiyo, Taro and Nakamura, Yoshitsugu and Kakiuchi, Kensuke and Motoki, Mikiko and Unoda, Kiichi and Ishida, Simon and Nakajima, Hideto and Arawaka, Shigeki},
doi = {10.1002/MUS.26149},
issn = {1097-4598},
journal = {Muscle & nerve},
keywords = {Aged,Amyotrophic Lateral Sclerosis / mortality*,Amyotrophic Lateral Sclerosis / therapy*,Artificial / classification,Artificial / methods*,Disease Progression,Female,Fumiharu Kimura,Humans,MEDLINE,Male,Middle Aged,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,PubMed Abstract,Respiration,Retrospective Studies,Shigeki Arawaka,Survival Analysis,Takahiko Hirose,Time Factors,Tracheostomy / methods*,Treatment Outcome,Vital Capacity,doi:10.1002/mus.26149,pmid:29679377},
month = {dec},
number = {6},
pages = {770--776},
pmid = {29679377},
publisher = {Muscle Nerve},
title = {{Clinical characteristics of long-term survival with noninvasive ventilation and factors affecting the transition to invasive ventilation in amyotrophic lateral sclerosis}},
url = {https://pubmed.ncbi.nlm.nih.gov/29679377/},
volume = {58},
year = {2018}
}
@article{Bourke2006a,
abstract = {Background: Few patients with amyotrophic lateral sclerosis currently receive non-invasive ventilation (NIV), reflecting clinical uncertainty about the role of this intervention. We aimed to assess the effect of NIV on quality of life and survival in amyotrophic lateral sclerosis in a randomised controlled trial. Methods: 92 of 102 eligible patients participated. They were assessed every 2 months and randomly assigned to NIV (n=22) or standard care (n=19) when they developed either orthopnoea with maximum inspiratory pressure less than 60% of that predicted or symptomatic hypercapnia. Primary validated quality-of-life outcome measures were the short form 36 mental component summary (MCS) and the sleep apnoea quality-of-life index symptoms domain (sym). Both time maintained above 75% of baseline (TiMCS and Tisym) and mean improvement ($\mu$MCS and $\mu$sym) were measured. Findings: NIV improved T iMCS, Tisym, $\mu$MCS, $\mu$sym, and survival in all patients and in the subgroup with better bulbar function (n=20). This subgroup showed improvement in several measures of quality of life and a median survival benefit of 205 days (p=0{\textperiodcentered}006) with maintained quality of life for most of this period. NIV improved some quality-of-life indices in those with poor bulbar function, including $\mu$sym (p=0{\textperiodcentered}018), but conferred no survival benefit. Interpretation: In patients with amyotrophic lateral sclerosis without severe bulbar dysfunction, NIV improves survival with maintenance of, and improvement in, quality of life. The survival benefit from NIV in this group is much greater than that from currently available neuroprotective therapy. In patients with severe bulbar impairment, NIV improves sleep-related symptoms, but is unlikely to confer a large survival advantage.},
author = {Bourke, Stephen C. and Tomlinson, Mark and Williams, Tim L. and Bullock, Robert E. and Shaw, Pamela J. and Gibson, G. John},
doi = {10.1016/S1474-4422(05)70326-4},
issn = {1474-4422},
journal = {The Lancet. Neurology},
keywords = {Aged,Amyotrophic Lateral Sclerosis / therapy*,Female,G John Gibson,Humans,MEDLINE,Male,Mark Tomlinson,Middle Aged,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Non-U.S. Gov't,Positive-Pressure Respiration*,PubMed Abstract,Quality of Life,Randomized Controlled Trial,Research Support,Respiratory Insufficiency / etiology,Respiratory Insufficiency / therapy,Sleep Apnea Syndromes / etiology,Sleep Apnea Syndromes / therapy,Stephen C Bourke,Survival,Treatment Outcome,doi:10.1016/S1474-4422(05)70326-4,pmid:16426990},
month = {feb},
number = {2},
pages = {140--147},
pmid = {16426990},
publisher = {Lancet Neurol},
title = {{Effects of non-invasive ventilation on survival and quality of life in patients with amyotrophic lateral sclerosis: a randomised controlled trial}},
url = {https://pubmed.ncbi.nlm.nih.gov/16426990/},
volume = {5},
year = {2006}
}
@article{Forsgren1983,
abstract = {ABSTRACT All cases of motor neuron disease (MND), encompassing amyotrophic lateral sclerosis (ALS), progressive bulbar paralysis (PBP) and progressive spinal muscular atrophy (PSMA), in northern Sweden, diagnosed between 1969‐1980 have been analysed. 128 cases were found, corresponding to an average annual incidence rate of 1.67 per 100,000. The prevalence on December 31, 1980 was 4.8 per 100,000. Age‐specific incidence rates were higher in the high age groups with a maximum at 60‐64 years for males, at 70‐74 years for females and at 65‐69 years for the sexes combined. The median age at onset was 61 years. Clustering was not found in mining districts and overrepresentation of miners and stone treaters was not observed. Minor differences in incidence rates, as measured by the standardized morbidity ratio, SMR, were found between the inland, coastal and mountain areas. The median survival time after onset of disease was 32 months for ALS, 30 months for PBP and 70 months for PSMA. The combined survival rate for all MND cases was 28% after 5 years and 15% after 10 years. The male to female ratio was 1.1:1, and 4.7% were familial cases. 1983 Blackwell Munksgaard},
author = {Forsgren, Lars and Almay, Bela G.L. and Wall, Stig},
doi = {10.1111/J.1600-0404.1983.TB04810.X},
issn = {0001-6314},
journal = {Acta neurologica Scandinavica},
keywords = {Adult,Aged,Amyotrophic Lateral Sclerosis / epidemiology,B G Almay,Cross-Sectional Studies,Female,Humans,L Forsgren,MEDLINE,Male,Middle Aged,Motor Neurons*,Muscular Atrophy / epidemiology,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Neuromuscular Diseases / epidemiology*,Non-U.S. Gov't,Paralysis / epidemiology,PubMed Abstract,Research Support,S Wall,Spinal Cord Diseases / epidemiology,Sweden,doi:10.1111/j.1600-0404.1983.tb04810.x,pmid:6604389},
number = {1},
pages = {20--29},
pmid = {6604389},
publisher = {Acta Neurol Scand},
title = {{Epidemiology of motor neuron disease in northern Sweden}},
url = {https://pubmed.ncbi.nlm.nih.gov/6604389/},
volume = {68},
year = {1983}
}
@article{Dorst2019,
abstract = {Non-invasive ventilation (NIV) has become an important cornerstone of symptomatic treatment in amyotrophic lateral sclerosis (ALS), improving survival and quality of life. In this review, we summarize the most important recent developments and insights, including evidence of efficacy, indication criteria and time of initiation, ventilation parameters and adaptation strategies, treatment of complicating factors, transition from NIV to invasive ventilation, termination of NIV and end-of-life management. Recent publications have questioned former conventions and guideline recommendations, especially with regard to timing and prognostic factors; therefore, a fresh look and re-evaluation of current evidence is needed.},
author = {Dorst, Johannes and Ludolph, Albert C.},
doi = {10.1177/1756286419857040},
file = {::},
issn = {17562864},
journal = {Therapeutic Advances in Neurological Disorders},
keywords = {amyotrophic lateral sclerosis,motor neuron diseases,non-invasive ventilation},
month = {jun},
pmid = {31258624},
publisher = {SAGE Publications},
title = {{Non-invasive ventilation in amyotrophic lateral sclerosis}},
url = {/pmc/articles/PMC6589990/ /pmc/articles/PMC6589990/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6589990/},
volume = {12},
year = {2019}
}
@article{Baudouin2002,
abstract = {### Nomenclature

Non-invasive ventilation (NIV) refers to the provision of ventilatory support through the patient's upper airway using a mask or similar device. This technique is distinguished from those which bypass the upper airway with a tracheal tube, laryngeal mask, or tracheostomy and are therefore considered invasive. In this document NIV refers to non-invasive positive pressure ventilation, and other less commonly used techniques such as external negative pressure or rocking beds will not be discussed. (NIPPV is an alternative abbreviation but it is more cumbersome and involves ambiguity as to whether “N” is for “non-invasive” or “nasal”.)

Continuous positive airway pressure (CPAP) in this document refers to the non-invasive application of positive airway pressure, again using a face or nasal mask rather than in conjunction with invasive techniques. Although it might be open to debate as to whether the use of non-invasive CPAP in acute respiratory failure constitutes ventilatory support, it is included in this document because of the confusion which commonly arises between NIV and CPAP in clinical practice.

### Background

One of the first descriptions of the use of NIV using nasal masks was for the treatment of hypoventilation at night in patients with neuromuscular disease.1,2 This has proved to be so successful that it has become widely accepted as the standard method of non-invasive ventilation used in patients with chronic hypercapnic respiratory failure caused by chest wall deformity, neuromuscular disease, or impaired central respiratory drive. It has largely replaced other modalities such as external negative pressure ventilation and rocking beds.

Within a few years of its introduction, NIV was starting to be used in acute hypercapnic respiratory failure and in patients with abnormal lungs rather than an impaired respiratory pump. Initial anecdotal reports were followed by larger series and then by randomised trials. Analysis of these trials has shown {\ldots}},
author = {Baudouin, S. and Turner, L. and Blumenthal, S. and Cooper, B. and Davidson, C. and Davison, A. and Elliott, M. and Kinnear, William and Paton, R. and Sawicka, E.},
doi = {10.1136/THORAX.57.3.192},
file = {::},
issn = {0040-6376},
journal = {Thorax},
month = {mar},
number = {3},
pages = {192--211},
pmid = {11867822},
publisher = {BMJ Publishing Group Ltd},
title = {{Non-invasive ventilation in acute respiratory failure}},
url = {https://thorax.bmj.com/content/57/3/192 https://thorax.bmj.com/content/57/3/192.abstract},
volume = {57},
year = {2002}
}
@misc{rr,
title = {{R: The R Project for Statistical Computing}},
url = {https://www.r-project.org/},
urldate = {2022-11-25}
}
@article{Garcez2022,
author = {Garcez, Luis},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{Unified Framework for Dataset Quality Control [R package qualitycontrol]}},
url = {https://cran.r-project.org/web/packages/qualitycontrol/},
year = {2022}
}
@misc{MassachusettsInstituteofTechnology1987,
author = {{Massachusetts Institute of Technology}},
title = {{MIT License}},
url = {https://mit-license.org/},
urldate = {2022-11-25},
year = {1987}
}
@article{Pantazis2005,
abstract = {The main statistical problem in many epidemiological studies which involve repeated measurements of surrogate markers is the frequent occurrence of missing data. Standard likelihood-based approaches like the linear random-effects model fail to give unbiased estimates when data are non-ignorably missing. In human immunodeficiency virus (HIV) type 1 infection, two markers which have been widely used to track progression of the disease are CD4 cell counts and HIV-ribonucleic acid (RNA) viral load levels. Repeated measurements of these markers tend to be informatively censored, which is a special case of non-ignorable missingness. In such cases, we need to apply methods that jointly model the observed data and the missingness process. Despite their high correlation, longitudinal data of these markers have been analysed independently by using mainly random-effects models. Touloumi and co-workers have proposed a model termed the joint multivariate random-effects model which combines a linear random-effects model for the underlying pattern of the marker with a log-normal survival model for the drop-out process. We extend the joint multivariate random-effects model to model simultaneously the CD4 cell and viral load data while adjusting for informative drop-outs due to disease progression or death. Estimates of all the model's parameters are obtained by using the restricted iterative generalized least squares method or a modified version of it using the EM algorithm as a nested algorithm in the case of censored survival data taking also into account non-linearity in the HIV-RNA trend. The method proposed is evaluated and compared with simpler approaches in a simulation study. Finally the method is applied to a subset of the data from the 'Concerted action on seroconversion to AIDS and death in Europe' study. {\textcopyright} 2005 Royal Statistical Society.},
author = {Pantazis, N. and Touloumi, G. and Walker, A. S. and Babiker, A. G.},
doi = {10.1111/J.1467-9876.2005.00491.X},
issn = {1467-9876},
journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
keywords = {Bivariate response,Informative censoring,Missing data,Repeated measurements},
month = {apr},
number = {2},
pages = {405--423},
publisher = {John Wiley & Sons, Ltd},
title = {{Bivariate modelling of longitudinal measurements of two human immunodeficiency type 1 disease progression markers in the presence of informative drop-outs}},
url = {https://onlinelibrary.wiley.com/doi/full/10.1111/j.1467-9876.2005.00491.x https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9876.2005.00491.x https://rss.onlinelibrary.wiley.com/doi/10.1111/j.1467-9876.2005.00491.x},
volume = {54},
year = {2005}
}
@article{Philipson2008,
abstract = {Longitudinal data analysis is frequently complicated by drop-out. In this paper we consider several methods for dealing with drop-out afflicted data. Along with a general comparison, particular attention is paid to the consequences of model misspecification. The purpose of our approach is two-fold. We first deliberate the form of the drop-out model and compare two alternatives. Furthermore, the extent to which each method is dependent on its core assumptions is assessed through scenarios where one or more such assumptions are compromised. Second, the extent to which we can identify adequacy of model fit is investigated via recently developed diagnostics. These twin targets are pursued via simulation scenarios and application to a schizophrenia trial of over 500 patients with near 50 per cent drop-out. Copyright {\textcopyright} 2008 John Wiley. & Sons, Ltd.},
author = {Philipson, Peter M. and Ho, Weang Kee and Henderson, Robin},
doi = {10.1002/SIM.3450},
issn = {0277-6715},
journal = {Statistics in medicine},
keywords = {Computer Simulation,Data Interpretation,Humans,Longitudinal Studies*,MEDLINE,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Patient Dropouts*,Peter M Philipson,PubMed Abstract,Review,Robin Henderson,Schizophrenia / drug therapy,Statistical,Statistics as Topic / methods*,Weang Kee Ho,doi:10.1002/sim.3450,pmid:18937274},
month = {dec},
number = {30},
pages = {6276--6298},
pmid = {18937274},
publisher = {Stat Med},
title = {{Comparative review of methods for handling drop-out in longitudinal studies}},
url = {https://pubmed.ncbi.nlm.nih.gov/18937274/},
volume = {27},
year = {2008}
}
@misc{hender,
title = {{Henderson R, Diggle P, Dobson A. Joint modelling of longitudinal measurements and event time data. Biostatistics 2000;1(4):465-80. - Pesquisa Google}},
url = {https://www.google.com/search?client=firefox-b-d&q=Henderson+R%2C+Diggle+P%2C+Dobson+A.+Joint+modelling+of+longitudinal+measurements+and+event+time+data.+Biostatistics+2000%3B1%284%29%3A465-80.},
urldate = {2022-11-08}
}
@article{Conde2018,
abstract = {(1) Background: Amyotrophic lateral sclerosis (ALS) is a progressive neurodegenerative condition, whose bulbar involvement compromises language, swallowing, and airway protection. When oral nutrition is no longer adequate, percutaneous endoscopic gastroscopy (PEG) may be indicated. However, as exact timing is still debatable, we tried to find it. (2) Methods: A prospective cohort study was performed using fiber-optic endoscopic evaluation of swallowing (FEES), functional evaluation scales (ALS Functional Rating Scale-Revised (ALSFRS-R) and bulbar sub-score (ALSFRS-R-B)), lung function tests (like Forced Vital Capacity (FVC), Cough Peak Flow (CPF)) and anthropometric data. (3) Results: Twenty-three patients were enrolled (mean 65.4 ± 9.1 years, 60.9% males), 12 with spinal-onset. During the study period, 58 FEES were performed (1–4/patients). Even before formal PEG indication, suggestions were given to correct the alterations found. PEG was placed in 12 patients, on average 21.8 months after diagnosis (FVC = 69.9% ± 26.7%, ALSFRS-R-B = 7.7 ± 3.7, ALSFRS-R = 28.9 ± 12.3), and being 91.7% under ventilatory support. ALSFRS-R-B, CPF, FVC, and ALSFRS-R showed significant discriminant ability for PEG placement. Sensitivity and specificity were, respectively, ALSFRS-R-B ≤ 8 (100/90.9), CPF ≤ 205 (83.3), FVC ≤ 74 (83.3/74.2), and ALSFRS-R < 29 (83.3/65.1). (4) Conclusions: FEES provide additional information beyond formal PEG indication. ALSFRS-R-B score ≤ 8 was found as a best functional and noninvasive indicator for PEG placement in ALS patients.},
author = {Conde, Bebiana and Martins, Nat{\'{a}}lia and Rodrigues, In{\^{e}}s and Pimenta, Ana Cl{\'{a}}udia and Winck, Jo{\~{a}}o Carlos},
doi = {10.3390/JCM7100352},
file = {::},
issn = {20770383},
journal = {Journal of Clinical Medicine},
keywords = {Amyotrophic Lateral Sclerosis,Functional,Parameters percutaneous endoscopic gastrostomy},
month = {oct},
number = {10},
pmid = {30322191},
publisher = {Multidisciplinary Digital Publishing Institute  (MDPI)},
title = {{Functional and Endoscopic Indicators for Percutaneous Endoscopic Gastrostomy (PEG) in Amyotrophic Lateral Sclerosis Patients}},
url = {/pmc/articles/PMC6210317/ /pmc/articles/PMC6210317/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6210317/},
volume = {7},
year = {2018}
}
@article{Lopez-Gomez2021,
abstract = {Dysphagia is a highly prevalent symptom in Amyotrophic Lateral Sclerosis (ALS), and the implantation of a percutaneous endoscopic gastrostomy (PEG) is a very frequent event. The aim of this study was to evaluate the influence of PEG implantation on survival and complications in ALS. An interhospital registry of patients with ALS of six hospitals in the Castilla-Le{\'{o}}n region (Spain) was created between January 2015 and December 2017. The data were compared for those in whom a PEG was implanted and those who it was not. A total of 93 patients were analyzed. The mean age of the patients was 64.63 (17.67) years. A total of 38 patients (38.8%) had a PEG implantation. An improvement in the anthropometric parameters was observed among patients who had a PEG from the beginning of nutritional follow-up compared to those who did not, both in BMI (kg/m2 ) (PEG: 0 months, 22.06; 6 months, 23.04; p < 0.01; NoPEG: 0 months, 24.59–23.87; p > 0.05). Among the deceased patients, 38 (40.4%) those who had an implanted PEG (20 patients (52.6%) had a longer survival time (PEG: 23 (15–35.5) months; NoPEG 11 (4.75–18.5) months; p = 0.01). A PEG showed a survival benefit among ALS patients. Early implantation of a PEG produced a reduction in admissions associated with complications derived from it.},
author = {L{\'{o}}pez-G{\'{o}}mez, Juan J. and Ballesteros-Pomar, Mar{\'{i}}a D. and Torres-Torres, Beatriz and {Pintor-De la Maza}, Bego{\~{n}}a and Penacho-L{\'{a}}zaro, Mar{\'{i}}a A. and Palacio-Mures, Jos{\'{e}} M. and Abreu-Pad{\'{i}}n, Cristina and Sanz-Gallego, Irene and {De Luis-Rom{\'{a}}n}, Daniel A.},
doi = {10.3390/NU13082765},
file = {::},
issn = {2072-6643},
journal = {Nutrients},
keywords = {Aged,Amyotrophic Lateral Sclerosis / complications,Amyotrophic Lateral Sclerosis / mortality*,Anthropometry,Daniel A De Luis-Rom{\'{a}}n,Deglutition Disorders / etiology,Deglutition Disorders / mortality,Deglutition Disorders / surgery*,Digestive System / methods,Digestive System / mortality*,Endoscopy,Female,Gastrostomy / methods,Gastrostomy / mortality*,Humans,Juan J L{\'{o}}pez-G{\'{o}}mez,MEDLINE,Male,Malnutrition / mortality,Malnutrition / prevention & control*,Mar{\'{i}}a D Ballesteros-Pomar,Middle Aged,Multicenter Study,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Nutritional Status,Observational Study,PMC8401888,Prospective Studies,PubMed Abstract,Registries,Spain,Treatment Outcome,doi:10.3390/nu13082765,pmid:34444925},
month = {aug},
number = {8},
pmid = {34444925},
publisher = {Nutrients},
title = {{Impact of Percutaneous Endoscopic Gastrostomy (PEG) on the Evolution of Disease in Patients with Amyotrophic Lateral Sclerosis (ALS)}},
url = {https://pubmed.ncbi.nlm.nih.gov/34444925/},
volume = {13},
year = {2021}
}
@article{Burkhardt2017,
abstract = {Background Non-invasive ventilation (NIV) and percutaneous gastrostomy (PEG) are guideline-recommended interventions for symptom management in amyotrophic lateral sclerosis (ALS). Their effect on survival is controversial and the impact on causes of death is unknown. Objective To investigate the effect of NIV and PEG on survival and causes of death in ALS patients. Methods Eighty deceased ALS patients underwent a complete post mortem analysis for causes of death between 2003 and 2015. Forty-two of these patients consented for genetic testing. Effects of NIV and PEG on survival and causes of death were analyzed in a multivariable Cox proportional hazard regression. Results Six patients, who requested assisted suicide causing drug-induced hypoxia, were excluded from final analysis. Respiratory failure was the main cause of death in 72 out of 74 patients. Fifteen out of 74 died of aspiration pneumonia 23/74 of bronchopneumonia and 8/74 of a combination of aspiration pneumonia and bronchopneumonia. Twenty died of hypoxia without concomitant infection, and six patients had pulmonary embolism alone or in combination with pneumonia. NIV (p = 0.01) and PEG (p<0.01) had a significant impact on survival. In patients using NIV bronchopneumonia was significantly more frequent (p <0.04) compared to non-NIV patients. This effect was even more pronounced in limb onset patients (p<0.002). Patients with C9orf72 hexanucleotide repeat expansions showed faster disease progression and shorter survival (p = 0.01). Conclusion The use of NIV and PEG prolongs survival in ALS. This study supports current AAN and EFNS guidelines which recommend NIV and PEG as a treatment option in ALS. The risk of bronchopneumonia as cause of death may be increased by NIV.},
author = {Burkhardt, Christian and Neuwirth, Christoph and Sommacal, Andreas and Andersen, Peter M. and Weber, Markus},
doi = {10.1371/JOURNAL.PONE.0177555},
file = {::},
issn = {1932-6203},
journal = {PloS one},
keywords = {80 and over,Adult,Aged,Amyotrophic Lateral Sclerosis / complications,Amyotrophic Lateral Sclerosis / genetics,Amyotrophic Lateral Sclerosis / mortality,Amyotrophic Lateral Sclerosis / therapy*,C9orf72 Protein,Cause of Death,Christian Burkhardt,Christoph Neuwirth,Disease Progression,Female,Gastrostomy*,Humans,MEDLINE,Male,Markus Weber,Middle Aged,Multivariate Analysis,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Noninvasive Ventilation*,Nuclear Respiratory Factors,PMC5441602,Proportional Hazards Models,Proteins / genetics,PubMed Abstract,Retrospective Studies,Survival Analysis,doi:10.1371/journal.pone.0177555,pmid:28542233},
month = {may},
number = {5},
pmid = {28542233},
publisher = {PLoS One},
title = {{Is survival improved by the use of NIV and PEG in amyotrophic lateral sclerosis (ALS)? A post-mortem study of 80 ALS patients}},
url = {https://pubmed.ncbi.nlm.nih.gov/28542233/},
volume = {12},
year = {2017}
}
@article{Castanheira2021,
abstract = {Amyotrophic lateral sclerosis (ALS) causes dysphagia and consequent poor nutrition. Sometimes enteral feeding is offered. Percutaneous endoscopic gastrostomy (PEG) is currently the technique of cho...},
author = {Castanheira, Andr{\'{E}} and Swash, Michael and {De Carvalho}, Mamede},
doi = {10.1080/21678421.2021.1946089},
issn = {21679223},
journal = {https://doi.org/10.1080/21678421.2021.1946089},
keywords = {Amyotrophic lateral sclerosis,complications,nutrition,percutaneous gastrostomy,prognostic,progression,survival,therapy},
number = {3-4},
pages = {176--189},
pmid = {34196236},
publisher = {Taylor & Francis},
title = {{Percutaneous gastrostomy in amyotrophic lateral sclerosis: a review}},
url = {https://www.tandfonline.com/doi/abs/10.1080/21678421.2021.1946089},
volume = {23},
year = {2021}
}
@article{Abadie2002a,
abstract = {Matching estimators for average treatment effects are widely used in evaluation research despite the fact that their large sample properties have not been established in many cases. In this article, we develop a new framework to analyze the properties of matching estimators and establish a number of new results. First, we show that matching estimators include a conditional bias term which may not vanish at a rate faster than root-N when more than one continuous variable is used for matching. As a result, matching estimators may not be root-N-consistent. Second, we show that even after removing the conditional bias, matching estimators with a fixed number of matches do not reach the semiparametric efficiency bound for average treatment effects, although the efficiency loss may be small. Third, we propose a bias-correction that removes the conditional bias asymptotically, making matching estimators root-N-consistent. Fourth, we provide a new estimator for the conditional variance that does not require consistent nonparametric estimation of unknown functions. We apply the bias-corrected matching estimators to the study of the effects of a labor market program previously analyzed by Lalonde (1986). We also carry out a small simulation study based on Lalonde's example where a simple implementation of the biascorrected matching estimator performs well compared to both simple matching estimators and to regression estimators in terms of bias and root-mean-squared-error. Software for implementing the proposed estimators in STATA and Matlab is available from the authors on the web.},
address = {Cambridge, MA},
author = {Abadie, Alberto and Imbens, Guido and Kennedy, John F},
doi = {10.3386/T0283},
file = {::},
institution = {National Bureau of Economic Research},
keywords = {Alberto Abadie,Guido W. Imbens},
month = {oct},
title = {{Simple and Bias-Corrected Matching Estimators for Average Treatment Effects}},
url = {https://www.nber.org/papers/t0283},
year = {2002}
}
@article{Collier2021,
abstract = {Background: The generalized propensity score (GPS) addresses selection bias due to observed confounding variables and provides a means to demonstrate causality of continuous treatment doses with propensity score analyses. Estimating the GPS with parametric models obliges researchers to meet improbable conditions such as correct model specification, normal distribution of variables, and large sample sizes. Objectives: The purpose of this Monte Carlo simulation study is to examine the performance of neural networks as compared to full factorial regression models to estimate GPS in the presence of Gaussian and skewed treatment doses and small to moderate sample sizes. Research design: A detailed conceptual introduction of neural networks is provided, as well as an illustration of selection of hyperparameters to estimate GPS. An example from public health and nutrition literature uses residential distance as a treatment variable to illustrate how neural networks can be used in a propensity score analysis to estimate a dose–response function of grocery spending behaviors. Results: We found substantially higher correlations and lower mean squared error values after comparing true GPS with the scores estimated by neural networks. The implication is that more selection bias was removed using GPS estimated with neural networks than using GPS estimated with classical regression. Conclusions: This study proposes a new methodological procedure, neural networks, to estimate GPS. Neural networks are not sensitive to the assumptions of linear regression and other parametric models and have been shown to be a contender against parametric approaches to estimate propensity scores for continuous treatments.},
author = {Collier, Zachary K. and Leite, Walter L. and Karpyn, Allison},
doi = {10.1177/0193841X21992199},
issn = {15523926},
journal = {Evaluation Review},
keywords = {data mining,propensity scores,selection bias},
month = {feb},
number = {1-2},
pages = {3--33},
publisher = {SAGE Publications Inc.},
title = {{Neural Networks to Estimate Generalized Propensity Scores for Continuous Treatment Doses}},
url = {https://www.researchgate.net/publication/349769459_Neural_Networks_to_Estimate_Generalized_Propensity_Scores_for_Continuous_Treatment_Doses},
volume = {45},
year = {2021}
}
@article{Collier2021a,
abstract = {Neural networks are a contending data mining procedure to estimate propensity scores due to its robustness to non-normal residual distributions, ability to detect complex nonlinear relationships be...},
author = {Collier, Zachary K. and Leite, Walter L. and Zhang, Haobai},
doi = {10.1080/03610918.2021.1963455},
issn = {15324141},
journal = {https://doi.org/10.1080/03610918.2021.1963455},
keywords = {Data mining,Neural networks,Propensity scores},
publisher = {Taylor & Francis},
title = {{Estimating propensity scores using neural networks and traditional methods: a comparative simulation study}},
url = {https://www.tandfonline.com/doi/abs/10.1080/03610918.2021.1963455},
year = {2021}
}
@article{VanDerArk2015,
abstract = {These research articles from the 79th Annual Meeting of the Psychometric Society (IMPS) cover timely quantitative psychology topics, including new methods in item response theory, computerized adaptive testing, cognitive diagnostic modeling, and psychological scaling. Topics within general quantitative methodology include structural equation modeling, factor analysis, causal modeling, mediation, missing data methods, and longitudinal data analysis. These methods will appeal, in particular, to researchers in the social sciences. The 79th annual meeting took place in Madison, WI between July 21nd and 25th, 2014. Previous volumes to showcase work from the Psychometric Society's Meeting are New Developments in Quantitative Psychology: Presentations from the 77th Annual Psychometric Society Meeting (Springer, 2013) and Quantitative Psychology Research: The 78th Annual Meeting of the Psychometric Society (Springer, 2015).},
author = {{Van Der Ark}, L. Andries and Bolt, Daniel M. and Wang, Wen Chung and Douglas, Jeffrey A. and Chow, Sy Miin},
doi = {10.1007/978-3-319-19977-1},
file = {:Users/jferreira-admin/Downloads/Keller_Kim_Steiner_2015(1).pdf:pdf},
isbn = {9783319199771},
journal = {Quantitative Psychology Research: The 79th Annual Meeting of the Psychometric Society, Madison, Wisconsin, 2014},
keywords = {covariate balance,data mining,logistic regression,neural networks,propensity score analysis},
pages = {1--387},
title = {{Quantitative Psychology Research: The 79th Annual Meeting of the Psychometric Society, Madison, Wisconsin, 2014}},
volume = {140},
year = {2015}
}
@article{Albert2001,
abstract = {In the Project on Death in America ALS cohort, 121 patients were followed to examine the timing of key milestones in the course of the disease, such as tracheostomy and PEG placement. During the 2- to 4-year follow-up period, 26.5% of patients received PEG, yielding a cumulative incidence of 48%. PEG placement occurred, on average, 16 months after patients received confirmation of the diagnosis at our Center. Patients who received PEG were more likely to have tracheostomies than patients not using PEG (p < 0.01). In multivariate proportional hazard models that included both sociodemographic and disease indicators, the strongest predictor of PEG use was a patient's baseline preference for PEG: 57.1% of patients "absolutely in favor" went on to have PEG, compared to only 9.3% of those "absolutely against" (p < 0.01). PEG users were more likely to have initiated health care proxies. These findings suggest that patients who use PEG may be consistently proactive in the face of the disease. {\textcopyright} 2001 Elsevier Science B.V. All rights reserved.},
author = {Albert, Steven M. and Murphy, Peregrine L. and {Del Bene}, Maura and Rowland, Lewis P. and Mitsumoto, Hiroshi},
doi = {10.1016/S0022-510X(01)00614-1},
issn = {0022510X},
journal = {Journal of the Neurological Sciences},
keywords = {Amyotrophic lateral sclerosis,Epidemiology,Incidence,PEG,Patient decision-making,Tracheostomy},
month = {oct},
number = {1-2},
pages = {115--119},
pmid = {11677001},
title = {{Incidence and predictors of PEG placement in ALS/MND}},
volume = {191},
year = {2001}
}
@article{Procaccini2008,
author = {Procaccini, Nicholas J. and Nemergut, Edward C},
file = {:Users/jferreira-admin/Downloads/ProcacciniArticle-March-08.pdf:pdf},
journal = {PRACTICAL GASTROENTEROLOGY},
title = {{Gastrostomy in the Patient with Amyotrophic Lateral Sclerosis : Risk vs Benefit ?}},
year = {2008}
}
@article{Mitsumoto2003,
abstract = {Objective: To compare characteristics of ALS patients with and without percutaneous endoscopic gastrostomy (PEG). Methods: Using the ALS Patient Care Database, data from patients with and without PEG with ALS Functional Rating Scale-bulbar subscale (ALSFRSb) scores ≤5 were analyzed; follow-up data were also collected. Results: PEG use was markedly increased with declining ALSFRSb scores. Demographics did not differ, but ALSFRS composite scores and bulbar and arm subscale scores were lower (P<0.0001). PEG patients used significantly more assistive devices, multidisciplinary care, home care nurses and aides, had more frequent physician and emergency department visits and hospital admissions (P<0.0001), and had lower health status based on the mini-SIP scale (P=0.0047). PEG use varied greatly between ALS centers. In the follow-up study, positive impact of PEG was noted in 79% of PEG patients but in only 37.5% of patients who received PEG later, based on a small number of patients. PEG use showed no survival benefit. Conclusion: Patients did not receive PEG until bulbar function was severely reduced and overall ALS had markedly progressed. PEG may have been performed too late to demonstrate survival benefits. Aggressive proactive nutritional management appears essential in patients with ALS. To determine whether PEG provides benefits, it must be performed at earlier stages of the disease and prospectively studied.},
author = {Mitsumoto, Hiroshi and Davidson, M. and Moore, D. and Gad, N. and Brandis, M. and Ringel, S. and Rosenfeld, J. and Shefner, J. M. and Strong, M. J. and Sufit, R. and Anderson, F. A. and Echols, Carol and Heiman-Patterson, Terry and Paylor, Theresa and Huffnagles, Vera and Murphy, Julie and Lou, Jau Shin and Tullar, David and McClusky, Leo and Damiano, Pat and Miller, Robert and Tkachenko, Igor and Neville, Hans and Blackwell, Kristin Howell and Oh, Shin and Olney, Richard and Mass, Jason and Pascuzzi, Robert and Michaels, Angi and Pioro, Erik and Andrews-Hinders, Doreen and Rosenfeld, Jeffrey and King, Ruth and Scelsa, Stephen and Shefner, Jeremy and Lepsky, Tina and Strong, Michael and Rowe, Ann and Sufit, Robert L. and Casey, Patricia},
doi = {10.1080/14660820310011728},
issn = {14660822},
journal = {Amyotrophic Lateral Sclerosis and Other Motor Neuron Disorders},
keywords = {Amyotrophic lateral sclerosis,Enteral feeding,Motor neuron disease,Nutritional care,Percutaneous endoscopic gastrostomy},
month = {sep},
number = {3},
pages = {177--185},
pmid = {13129795},
title = {{Percutaneous endoscopic gastrostomy (PEG) in patients with ALS and bulbar dysfunction}},
volume = {4},
year = {2003}
}
@article{Barone2019,
abstract = {Objective: There are conflicting data on nutritional factors influencing survival in amyotrophic lateral sclerosis (ALS) patients after percutaneous endoscopic gastrostomy (PEG) placement. We perfo...},
author = {Barone, Michele and Viggiani, Maria Teresa and Introna, Alessandro and D'errico, Eustachio and Scarafino, Antonio and Iannone, Andrea and {Di Leo}, Alfredo and Simone, Isabella Laura},
doi = {10.1080/21678421.2019.1643374},
issn = {21679223},
journal = {https://doi.org/10.1080/21678421.2019.1643374},
keywords = {Nutrition,prognostic,survival},
month = {oct},
number = {7-8},
pages = {490--496},
pmid = {31347407},
publisher = {Taylor & Francis},
title = {{Nutritional prognostic factors for survival in amyotrophic lateral sclerosis patients undergone percutaneous endoscopic gastrostomy placement}},
url = {https://www.tandfonline.com/doi/abs/10.1080/21678421.2019.1643374},
volume = {20},
year = {2019}
}
@article{Cui2018,
abstract = {Percutaneous endoscopic gastrostomy (PEG) is a method widely used for patients with amyotrophic lateral sclerosis (ALS); nevertheless, its effect on survival remains unclear. The purpose of this meta-analysis study was to determine the effects of PEG on survival in ALS patients. Relevant studies were retrieved from PubMed, EmBase, and the Cochrane Library databases, from inception to June 2017. Studies comparing PEG with other procedures in ALS patients were included. Odds ratios (ORs) in a random-effects model were used to assess the survival at different follow-up periods. Briefly, ten studies involving a total of 996 ALS patients were included. Summary ORs indicated that PEG administration was not associated with 30-day (OR = 1.59; 95%CI 0.93–2.71; P = 0.092), 10-month (OR = 1.25; 95%CI 0.72–2.17; P = 0.436), and 30-month (OR = 1.28; 95% CI 0.77–2.11; P = 0.338) survival rates, while they showed a beneficial effect in 20-month survival rate (OR = 1.97; 95% CI 1.21–3.21; P = 0.007). The survival rate was significantly prominent in reports published before 2005, and in studies with a retrospective design, sample size <100, mean age <60.0 years, and percentage male 50.0%. To sum up, these findings suggested that ALS patients administered with PEG had an increased 20-month survival rates, while there was no significant effect in 30-day, 10-month, and 30-month survival rates.},
author = {Cui, Fang and Sun, Liuqing and Xiong, Jianmei and Li, Jianyong and Zhao, Yangang and Huang, Xusheng},
doi = {10.1371/JOURNAL.PONE.0192243},
issn = {19326203},
journal = {PLoS ONE},
month = {feb},
number = {2},
pmid = {29408898},
publisher = {Public Library of Science},
title = {{Therapeutic effects of percutaneous endoscopic gastrostomy on survival in patients with amyotrophic lateral sclerosis: A meta-analysis}},
url = {/pmc/articles/PMC5800689/ /pmc/articles/PMC5800689/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5800689/},
volume = {13},
year = {2018}
}
@article{Lopez-Gomez2021a,
abstract = {Dysphagia is a highly prevalent symptom in Amyotrophic Lateral Sclerosis (ALS), and the implantation of a percutaneous endoscopic gastrostomy (PEG) is a very frequent event. The aim of this study was to evaluate the influence of PEG implantation on survival and complications in ALS. An interhospital registry of patients with ALS of six hospitals in the Castilla-Le{\'{o}}n region (Spain) was created between January 2015 and December 2017. The data were compared for those in whom a PEG was implanted and those who it was not. A total of 93 patients were analyzed. The mean age of the patients was 64.63 (17.67) years. A total of 38 patients (38.8%) had a PEG implantation. An improvement in the anthropometric parameters was observed among patients who had a PEG from the beginning of nutritional follow-up compared to those who did not, both in BMI (kg/m2 ) (PEG: 0 months, 22.06; 6 months, 23.04; p < 0.01; NoPEG: 0 months, 24.59–23.87; p > 0.05). Among the deceased patients, 38 (40.4%) those who had an implanted PEG (20 patients (52.6%) had a longer survival time (PEG: 23 (15–35.5) months; NoPEG 11 (4.75–18.5) months; p = 0.01). A PEG showed a survival benefit among ALS patients. Early implantation of a PEG produced a reduction in admissions associated with complications derived from it.},
author = {L{\'{o}}pez-G{\'{o}}mez, Juan J. and Ballesteros-Pomar, Mar{\'{i}}a D. and Torres-Torres, Beatriz and {Pintor-De la Maza}, Bego{\~{n}}a and Penacho-L{\'{a}}zaro, Mar{\'{i}}a A. and Palacio-Mures, Jos{\'{e}} M. and Abreu-Pad{\'{i}}n, Cristina and Sanz-Gallego, Irene and {De Luis-Rom{\'{a}}n}, Daniel A.},
doi = {10.3390/NU13082765},
issn = {20726643},
journal = {Nutrients},
keywords = {Amyotrophic lateral sclerosis,Nutritional status,Percutaneous endoscopic gastrostomy,Survival},
month = {aug},
number = {8},
pmid = {34444925},
publisher = {Multidisciplinary Digital Publishing Institute  (MDPI)},
title = {{Impact of Percutaneous Endoscopic Gastrostomy (PEG) on the Evolution of Disease in Patients with Amyotrophic Lateral Sclerosis (ALS)}},
url = {/pmc/articles/PMC8401888/ /pmc/articles/PMC8401888/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8401888/},
volume = {13},
year = {2021}
}
@article{BArnold2017,
abstract = {Keras is a high-level neural networks API, originall written in Python, and capable of running on top of either TensorFlow or Theano. It was developed with a focus on enabling fast experimentation. This package provides an interface to Keras from within R. All of the returned objects from functions in this package are either native R objects or raw pointers to python objects, making it possible for users to access the entire keras API. The main benefits of the package are (1) correct, manual parsing of R inputs to python, (2) R-sided documentation, and (3) examples written using the API. It allows, amongst other things, users to load and run popular pre-trained models such as VGG-19 (He et al. 2015), ResNet50 (He et al. 2016), and Inception (Szegedy et al. 2015). Most functions have associated examples showing a working example of how a layer or object may be used. These are mostly toy examples, made with small datasets with little regard to whether these are the correct models for a particular task. See the package vignettes for a more thorough explaination and several larger, more practical examples.},
author = {{B Arnold}, Taylor},
doi = {10.21105/JOSS.00296},
file = {::},
journal = {The Journal of Open Source Software},
month = {jun},
number = {14},
pages = {296},
publisher = {The Open Journal},
title = {{kerasR: R Interface to the Keras Deep Learning Library}},
volume = {2},
year = {2017}
}
@misc{earky,
title = {{EarlyStopping}},
url = {https://keras.io/api/callbacks/early_stopping/},
urldate = {2022-08-16}
}
@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
file = {:Users/jferreira-admin/Downloads/lstm.pdf:pdf},
issn = {08997667},
journal = {Neural Computation},
number = {8},
pages = {1735--1780},
pmid = {9377276},
title = {{Long Short-Term Memory}},
volume = {9},
year = {1997}
}
@article{Virtanen2020,
abstract = {SciPy is an open-source scientific computing library for the Python programming language. Since its initial release in 2001, SciPy has become a de facto standard for leveraging scientific algorithms in Python, with over 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories and millions of downloads per year. In this work, we provide an overview of the capabilities and development practices of SciPy 1.0 and highlight some recent technical developments.},
archivePrefix = {arXiv},
arxivId = {1907.10121},
author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and van der Walt, St{\'{e}}fan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R.J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C. J. and Polat, İlhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Ant{\^{o}}nio H. and Pedregosa, Fabian and van Mulbregt, Paul and Vijaykumar, Aditya and Bardelli, Alessandro Pietro and Rothberg, Alex and Hilboll, Andreas and Kloeckner, Andreas and Scopatz, Anthony and Lee, Antony and Rokem, Ariel and Woods, C. Nathan and Fulton, Chad and Masson, Charles and H{\"{a}}ggstr{\"{o}}m, Christian and Fitzgerald, Clark and Nicholson, David A. and Hagen, David R. and Pasechnik, Dmitrii V. and Olivetti, Emanuele and Martin, Eric and Wieser, Eric and Silva, Fabrice and Lenders, Felix and Wilhelm, Florian and Young, G. and Price, Gavin A. and Ingold, Gert Ludwig and Allen, Gregory E. and Lee, Gregory R. and Audren, Herv{\'{e}} and Probst, Irvin and Dietrich, J{\"{o}}rg P. and Silterra, Jacob and Webber, James T. and Slavi{\v{c}}, Janko and Nothman, Joel and Buchner, Johannes and Kulick, Johannes and Sch{\"{o}}nberger, Johannes L. and {de Miranda Cardoso}, Jos{\'{e}} Vin{\'{i}}cius and Reimer, Joscha and Harrington, Joseph and Rodr{\'{i}}guez, Juan Luis Cano and Nunez-Iglesias, Juan and Kuczynski, Justin and Tritz, Kevin and Thoma, Martin and Newville, Matthew and K{\"{u}}mmerer, Matthias and Bolingbroke, Maximilian and Tartre, Michael and Pak, Mikhail and Smith, Nathaniel J. and Nowaczyk, Nikolai and Shebanov, Nikolay and Pavlyk, Oleksandr and Brodtkorb, Per A. and Lee, Perry and McGibbon, Robert T. and Feldbauer, Roman and Lewis, Sam and Tygier, Sam and Sievert, Scott and Vigna, Sebastiano and Peterson, Stefan and More, Surhud and Pudlik, Tadeusz and Oshima, Takuya and Pingel, Thomas J. and Robitaille, Thomas P. and Spura, Thomas and Jones, Thouis R. and Cera, Tim and Leslie, Tim and Zito, Tiziano and Krauss, Tom and Upadhyay, Utkarsh and Halchenko, Yaroslav O. and V{\'{a}}zquez-Baeza, Yoshiki},
doi = {10.1038/S41592-019-0686-2},
eprint = {1907.10121},
file = {::},
issn = {15487105},
journal = {Nature Methods},
month = {mar},
number = {3},
pages = {261--272},
pmid = {32015543},
publisher = {Nature Research},
title = {{SciPy 1.0: fundamental algorithms for scientific computing in Python}},
volume = {17},
year = {2020}
}
@article{Madakkatel2021,
abstract = {We present a simple and efficient hypothesis-free machine learning pipeline for risk factor discovery that accounts for non-linearity and interaction in large biomedical databases with minimal variable pre-processing. In this study, mortality models were built using gradient boosting decision trees (GBDT) and important predictors were identified using a Shapley values-based feature attribution method, SHAP values. Cox models controlled for false discovery rate were used for confounder adjustment, interpretability, and further validation. The pipeline was tested using information from 502,506 UK Biobank participants, aged 37–73 years at recruitment and followed over seven years for mortality registrations. From the 11,639 predictors included in GBDT, 193 potential risk factors had SHAP values ≥ 0.05, passed the correlation test, and were selected for further modelling. Of the total variable importance summed up, 60% was directly health related, and baseline characteristics, sociodemographics, and lifestyle factors each contributed about 10%. Cox models adjusted for baseline characteristics, showed evidence for an association with mortality for 166 out of the 193 predictors. These included mostly well-known risk factors (e.g., age, sex, ethnicity, education, material deprivation, smoking, physical activity, self-rated health, BMI, and many disease outcomes). For 19 predictors we saw evidence for an association in the unadjusted but not adjusted analyses, suggesting bias by confounding. Our GBDT-SHAP pipeline was able to identify relevant predictors ‘hidden' within thousands of variables, providing an efficient and pragmatic solution for the first stage of hypothesis free risk factor identification.},
author = {Madakkatel, Iqbal and Zhou, Ang and McDonnell, Mark D. and Hypp{\"{o}}nen, Elina},
doi = {10.1038/s41598-021-02476-9},
file = {::},
isbn = {0123456789},
issn = {2045-2322},
journal = {Scientific Reports 2021 11:1},
keywords = {Epidemiology,Lifestyle modification,Risk factors},
month = {nov},
number = {1},
pages = {1--11},
pmid = {34837000},
publisher = {Nature Publishing Group},
title = {{Combining machine learning and conventional statistical approaches for risk factor discovery in a large cohort study}},
url = {https://www.nature.com/articles/s41598-021-02476-9},
volume = {11},
year = {2021}
}
@misc{rrr,
title = {{R: The R Stats Package}},
url = {https://stat.ethz.ch/R-manual/R-devel/library/stats/html/00Index.html},
urldate = {2022-04-18}
}
@article{VanderBurgh2017a,
abstract = {Amyotrophic lateral sclerosis (ALS) is a progressive neuromuscular disease, with large variation in survival between patients. Currently, it remains rather difficult to predict survival based on clinical parameters alone. Here, we set out to use clinical characteristics in combination with MRI data to predict survival of ALS patients using deep learning, a machine learning technique highly effective in a broad range of big-data analyses. A group of 135 ALS patients was included from whom high-resolution diffusion-weighted and T1-weighted images were acquired at the first visit to the outpatient clinic. Next, each of the patients was monitored carefully and survival time to death was recorded. Patients were labeled as short, medium or long survivors, based on their recorded time to death as measured from the time of disease onset. In the deep learning procedure, the total group of 135 patients was split into a training set for deep learning (n = 83 patients), a validation set (n = 20) and an independent evaluation set (n = 32) to evaluate the performance of the obtained deep learning networks. Deep learning based on clinical characteristics predicted survival category correctly in 68.8% of the cases. Deep learning based on MRI predicted 62.5% correctly using structural connectivity and 62.5% using brain morphology data. Notably, when we combined the three sources of information, deep learning prediction accuracy increased to 84.4%. Taken together, our findings show the added value of MRI with respect to predicting survival in ALS, demonstrating the advantage of deep learning in disease prognostication.},
author = {van der Burgh, Hannelore K. and Schmidt, Ruben and Westeneng, Henk Jan and de Reus, Marcel A. and van den Berg, Leonard H. and van den Heuvel, Martijn P.},
doi = {10.1016/j.nicl.2016.10.008},
file = {:Users/jferreira-admin/Downloads/1-s2.0-S2213158216301899-main.pdf:pdf},
issn = {22131582},
journal = {NeuroImage: Clinical},
keywords = {Amyotrophic lateral sclerosis,Deep learning,Neural network,Prediction,Survival,White matter connectivity},
pages = {361--369},
pmid = {28070484},
publisher = {The Authors},
title = {{Deep learning predictions of survival based on MRI in amyotrophic lateral sclerosis}},
url = {http://dx.doi.org/10.1016/j.nicl.2016.10.008},
volume = {13},
year = {2017}
}
@article{Pires2019,
abstract = {Amyotrophic Lateral Sclerosis (ALS) is a neurode-generative disease highly known for its rapid progression, leading to death usually within a few years. Respiratory failure is the most common cause of death. Therefore, efforts must be taken to prevent respiratory insufficiency. Preventive administration of non-invasive ventilation (NIV) has proven to improve survival in ALS patients. Using disease progression groups revealed to be of great importance to ALS studies, since the heterogeneous nature of disease presentation and progression presents challenges to the learn of predictive models that work for all patients. In this context, we propose an approach to stratify patients in three progression groups (Slow, Neutral and Fast) enabling the creation of specialized learning models that predict the need of NIV within a time window of 90, 180 or 365 days of their current medical appointment. The models are built using a collection of classifiers and 5x10-fold cross validation. We also test the use of a Feature Selection Ensemble to test which features are more relevant to predict this outcome. Our specialized predictive models showed promising results, proving the utility of patient stratification when predicting NIV in ALS patients.},
author = {Pires, Sofia and Gromicho, Marta and Pinto, Susana and Carvalho, Mamede and Madeira, Sara C.},
doi = {10.1109/ICDMW.2018.00113},
isbn = {9781538692882},
issn = {23759259},
journal = {IEEE International Conference on Data Mining Workshops, ICDMW},
keywords = {Amyotrophic Lateral Sclerosis,Disease Progression Groups,Patient Stratification,Prognostic Prediction},
month = {feb},
pages = {748--757},
publisher = {IEEE Computer Society},
title = {{Predicting non-invasive ventilation in ALS patients using stratified disease progression groups}},
volume = {2018-Novem},
year = {2019}
}
@article{Desport1999,
abstract = {Objective: To evaluate the occurrence of malnutrition in patients with ALS, to assess the relation of malnutrition to the neurologic deficit, and to determine the impact of nutritional status on patient survival. Background: Although ALS may be associated with significant malnutrition, the relative impact on patient survival has not yet been well established. Methods: In a prospective 7-month study of 55 ALS patients in a referral neurology practice, nutritional status was assessed by calculating body mass index. Neurologic evaluation includes four functional scores and identifies the form of disease onset. Slow vital capacity (VC) was also measured. Results: Occurrence of malnutrition in patients studied was 16.4%. Survival (using the Kaplan-Meier method) was worse for malnourished patients (p < 0.0001), with a 7.7-fold increased risk of death. Using multivariate analysis, only reduced VC (p < 0.0001) and malnutrition (p < 0.01) were found to have significant independent prognostic value. The degree of malnutrition is independent of neurologic scores and of forms of ALS onset. Conclusion: Nutritional surveillance of ALS patients is very important, both in bulbar-onset and spinal-onset patients.},
author = {Desport, J. C. and Preux, P. M. and Truong, T. C. and Vallat, J. M. and Sautereau, D. and Couratier, P.},
doi = {10.1212/WNL.53.5.1059},
issn = {0028-3878},
journal = {Neurology},
keywords = {Aged,Amyotrophic Lateral Sclerosis / mortality*,Female,Humans,J C Desport,MEDLINE,Male,Middle Aged,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Non-U.S. Gov't,Nutrition Disorders / mortality,Nutritional Status*,P Couratier,P M Preux,Prognosis,PubMed Abstract,Research Support,Risk Factors,Survival Analysis,doi:10.1212/wnl.53.5.1059,pmid:10496266},
month = {sep},
number = {5},
pages = {1059--1063},
pmid = {10496266},
publisher = {Neurology},
title = {{Nutritional status is a prognostic factor for survival in ALS patients}},
url = {https://pubmed.ncbi.nlm.nih.gov/10496266/},
volume = {53},
year = {1999}
}
@misc{als,
title = {{Who Gets ALS? - ALS Age of Onset | The ALS Association}},
url = {https://www.als.org/understanding-als/who-gets-als},
urldate = {2022-03-29}
}
@article{Jellinger2001,
abstract = {4th ed. Previous edition: 1986. This short pictorial guide to the examination of the peripheral nervous system contains new illustrations to ensure it retains its place as the standard short text on the subject. Introduction -- Spinal accessory nerve -- Brachial plexus -- Musculocutaneous nerve -- Axillary nerve -- Radial nerve -- Median nerve -- Ulnar nerve -- Lumbosacral plexus -- Nerves of the lower limb -- Dermatomes -- Nerves and root supply of muscles -- Commonly tested movements.},
author = {Jellinger, K A},
doi = {10.1046/j.1468-1331.2001.0222d.x},
file = {:Users/jferreira-admin/Downloads/MRC-011221-AidsToTheExaminationOfThePeripheralNervousSystem(1).pdf:pdf},
issn = {1351-5101},
journal = {European Journal of Neurology},
number = {4},
pages = {377--377},
title = {{Aids to the Examination of the Peripheral Nervous System}},
volume = {8},
year = {2001}
}
@book{MedicalResearchCouncilGreatBritain1976,
abstract = {Supersedes the Council's War memorandum no. 7, with title: Aids to the investigation of peripheral nerve injuries, prepared by the staff of the Dept. of Surgery, University of Edinburgh.},
address = {London},
author = {{Medical Research Council (Great Britain)} and {University of Edinburgh. Department of Surgery.}},
isbn = {9780114500337},
pages = {vi, 62 pages : illustrations.},
publisher = {H.M. Stationery Off.},
title = {{Aids to the examination of the peripheral nervous system}},
year = {1976}
}
@misc{aids,
title = {{Aids to the examination of the peripheral nervous system – UKRI}},
url = {https://www.ukri.org/publications/aids-to-the-examination-of-the-peripheral-nervous-system/},
urldate = {2022-03-28}
}
@article{Cedarbaum1999,
abstract = {The ALS Functional Rating Scale (ALSFRS) is a validated rating instrument for monitoring the progression of disability in patients with amyotrophic lateral sclerosis (ALS). One weakness of the ALSFRS as originally designed was that it granted disproportionate weighting to limb and bulbar, as compared to respiratory, dysfunction. We have now validated a revised version of the ALSFRS, which incorporates additional assessments of dyspnea, orthopnea, and the need for ventilatory support. The Revised ALSFRS (ALSFRS- R) retains the properties of the original scale and shows strong internal consistency and construct validity. ALSFRS-R scores correlate significantly with quality of life as measured by the Sickness Impact Profile, indicating that the quality of function is a strong determinant of quality of life in ALS.},
author = {Cedarbaum, Jesse M. and Stambler, Nancy and Malta, Errol and Fuller, Cynthia and Hilt, Dana and Thurmond, Barbara and Nakanishi, Arline},
doi = {10.1016/S0022-510X(99)00210-5},
issn = {0022-510X},
journal = {Journal of the neurological sciences},
keywords = {A Nakanishi,Activities of Daily Living*,Adult,Aged,Amyotrophic Lateral Sclerosis*,Clinical Trial,Disease Progression,Factor Analysis,Female,Follow-Up Studies,Humans,J M Cedarbaum,MEDLINE,Male,Middle Aged,Motor Neuron Disease / physiopathology,Multicenter Study,N Stambler,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Non-U.S. Gov't,PubMed Abstract,Quality of Life*,Research Support,Respiration*,Severity of Illness Index,Statistical,Survival Analysis,doi:10.1016/s0022-510x(99)00210-5,pmid:10540002},
month = {oct},
number = {1-2},
pages = {13--21},
pmid = {10540002},
publisher = {J Neurol Sci},
title = {{The ALSFRS-R: a revised ALS functional rating scale that incorporates assessments of respiratory function. BDNF ALS Study Group (Phase III)}},
url = {https://pubmed.ncbi.nlm.nih.gov/10540002/},
volume = {169},
year = {1999}
}
@misc{Cox1972,
author = {Cox, D. R.},
booktitle = {Journal of the Royal Statistical Society. Series B (Methodological)},
file = {:Users/jferreira-admin/Downloads/cox_jrssB_1972_hi_res(1).pdf:pdf},
keywords = {Hazard function,Life Table},
number = {2},
pages = {187--220},
title = {{Regress Models and Life Tables}},
volume = {34},
year = {1972}
}
@article{Aalen1989,
abstract = {A linear model is suggested for the influence of covariates on the intensity function. This approach is less vulnerable than the Cox model to problems of inconsistency when covariates are deleted or the precision of covariate measurements is changed. A method of non‐parametric estimation of regression functions is presented. This results in plots that may give information on the change over time in the influence of covariates. A test method and two goodness of fit plots are also given. The approach is illustrated by simulation as well as by data from a clinical trial of treatment of carcinoma of the oropharynx. Copyright {\textcopyright} 1989 John Wiley & Sons, Ltd.},
author = {Aalen, Odd O.},
doi = {10.1002/SIM.4780080803},
issn = {1097-0258},
journal = {Statistics in Medicine},
keywords = {Counting processes,Cox model,Empirical process,Linear models,Non,Regression,Survival analysis,parametrics},
month = {aug},
number = {8},
pages = {907--925},
pmid = {2678347},
publisher = {John Wiley & Sons, Ltd},
title = {{A linear regression model for the analysis of life times}},
url = {https://onlinelibrary.wiley.com/doi/full/10.1002/sim.4780080803 https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.4780080803 https://onlinelibrary.wiley.com/doi/10.1002/sim.4780080803},
volume = {8},
year = {1989}
}
@misc{jstor,
title = {{Regression Models and Life-Tables on JSTOR}},
url = {https://www.jstor.org/stable/2985181},
urldate = {2022-03-27}
}
@article{Torner2004,
abstract = {The primary objective of this work was to investigate and compare the use of the Cox proportional hazards model and Aalen's additive model in analysing survival data. Survival data from a study of 52 patients with advanced breast cancer was investigated using the Cox proportional hazards model. The model was optimized by examining different aspects by use of ap- propriate residual plots. Covariates judged not to improve the model significantly were removed. The model was stratified with regard to tumour size to account for different baseline hazards. After optimizing the Cox model, the same data was used to fit an additive model according to Aalen. Plots of the martingale residual process and Arja's plot was used to check model fit and optimize model options. The information gained from fitting of the two models is similar in some respects but also quite different in others. Both procedures resulted in the same covariates selected to remain in the model. The Cox model yield easily interpreted estimates of the covariates effects, but the assumption of proportional hazard is necessary to make these estimates valid. The additive model and the plots of the cumula- tive regressions functions give an appealing understanding of how the hazard profile is distributed. Most often however, these cumulative regression functions do not easily transform into a single numerical estimate of the covariate effect.},
author = {T{\"{o}}rner, Anna},
file = {:Users/jferreira-admin/Downloads/report(1).pdf:pdf},
journal = {Examensarbete},
pages = {40},
title = {{Proportional hazards and additive regression analysis of survival for severe breast cancer}},
volume = {3},
year = {2004}
}
@article{Basar2017,
abstract = {The Cox proportional hazards model is most widely used in survival analysis for modeling censored survival data. In this model, the effect of the covariates is assumed to act multiplicatively on the baseline hazard rate and the ratio of the hazards is constant over survival time. This is an important assumption and sometimes may not hold in some survival studies. The Cox model can lead to biased results when the proportionality assumption is not satisfied. In such a situation, the additive hazards regression models have been an alternative to proportional hazards models. The Aalen model allows for time-varying covariate effects. In some situations, some covariate effects may be constant but the others may not. In such cases, the Cox-Aalen model is a better alternative since it allows to combine both kinds of covariates in the same model. In this study the Cox proportional hazards model, Aalen's additive hazards model and the Cox-Aalen model have been considered. These models have been applied to kidney transplant data and the differences in estimates of the unknown parameters obtained by the Aalen's model, the Cox model and the Cox-Aalen model are investigated.},
author = {Başar, Emel},
doi = {10.17576/jsm-2017-4603-15},
file = {:Users/jferreira-admin/Downloads/15 Aditif Aalen.pdf:pdf},
issn = {01266039},
journal = {Sains Malaysiana},
keywords = {Aalen's additive hazards model,Cox proportional hazards model,Cox-Aalen model,Kidney transplant data,Survival analysis},
number = {3},
pages = {469--476},
title = {{Aalen's additive, cox proportional hazards and the cox-aalen model: Application to kidney transplant data}},
volume = {46},
year = {2017}
}
@article{Miller1999,
abstract = {Background: In 1994 consensus guidelines were developed for conducting clinical trials in ALS. With growing experience in clinical trials, it has become clear that a number of further guidelines were needed. Methods: Under the auspices of the World Federation of Neurology Committee on Research, a multinational group of neurologists, statisticians, patient advocates, representatives from the pharmaceutical industry as well as regulatory agencies developed consensus about a number of revisions to the existing guidelines during a 2 day conference in April 1998. Results: Expanded areas of focus include greater protection of patient rights, more detailed guidelines for outcome measures statistical analyses, disclosure of study results and improved interaction between investigators and the corporate sector. Comment: Substantial progress has been made in standardizing and improving the quality of clinical trials in ALS through these consensus guidelines.},
author = {Miller, Robert G. and Munsat, Theodore L. and Swash, Michael and Brooks, Benjamin R.},
doi = {10.1016/S0022-510X(99)00209-9},
issn = {0022-510X},
journal = {Journal of the neurological sciences},
keywords = {Amyotrophic Lateral Sclerosis*,B R Brooks,Clinical Trials as Topic / methods*,Clinical Trials as Topic / statistics & numerical ,Consensus Development Conference,Guideline,Humans,MEDLINE,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Practice Guideline,PubMed Abstract,Quality of Life,R G Miller,Review,T L Munsat,doi:10.1016/s0022-510x(99)00209-9,pmid:10540001},
month = {oct},
number = {1-2},
pages = {2--12},
pmid = {10540001},
publisher = {J Neurol Sci},
title = {{Consensus guidelines for the design and implementation of clinical trials in ALS. World Federation of Neurology committee on Research}},
url = {https://pubmed.ncbi.nlm.nih.gov/10540001/},
volume = {169},
year = {1999}
}
@article{Albert1999,
abstract = {Objective: To determine whether ALS patients' preferences for ameliorative or life-extending technologies elicited early in the disease were related to later treatment choices. Methods: In this prospective cohort study, 121 patients were seen at a tertiary ALS care center and followed for a median of 12 months. At baseline, patient preferences for use of tracheostomy and percutaneous endoscopic gastrostomy (PEG) placement were elicited. All patients received the same educational information before being interviewed about treatment preferences. Patients were then followed to determine if patients who viewed the interventions favorably at baseline were significantly more likely to use the interventions over follow-up. Results: Six to twelve percent of patients were certain they wanted tracheostomy and 28.2% wanted PEG. Preferences were related to later treatment choices: 20% of patients who found tracheostomy acceptable had one in the follow-up period, compared with 3.4% of those not in favor (p < 0.001). For PEG, similar findings were obtained: 48.5% who initially found it acceptable had PEG, versus 8.1% of those not in favor of this treatment (p < 0.001). Patients who found the interventions acceptable were more likely to be recently diagnosed, expressed greater attachment to life, and showed greater declines in pulmonary function over follow-up. Conclusions: Patients with ALS were able to express their preferences for life-extending or ameliorative technologies and made choices consistent with these preferences. However, patient preferences may change over time, and clinical education efforts are required throughout the course of disease.},
author = {Albert, Steven M. and Murphy, P. L. and {Del Bene}, M. L. and Rowland, L. P.},
doi = {10.1212/WNL.53.2.278},
issn = {0028-3878},
journal = {Neurology},
keywords = {80 and over,Adult,Aged,Amyotrophic Lateral Sclerosis / psychology*,Amyotrophic Lateral Sclerosis / surgery,Choice Behavior*,Female,Humans,L P Rowland,MEDLINE,Male,Middle Aged,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Non-U.S. Gov't,P L Murphy,Patient Participation*,Predictive Value of Tests,Prospective Studies,PubMed Abstract,Research Support,S M Albert,Tracheostomy / psychology,doi:10.1212/wnl.53.2.278,pmid:10430414},
month = {jul},
number = {2},
pages = {278--283},
pmid = {10430414},
publisher = {Neurology},
title = {{A prospective study of preferences and actual treatment choices in ALS}},
url = {https://pubmed.ncbi.nlm.nih.gov/10430414/},
volume = {53},
year = {1999}
}
@article{FarzanMadadizadehAminGhanbarnejad3VahidGhavami2*MohammadZareBandamiri4,
abstract = {Introduction Cancer ranks as the second and the third cause of death in developing countries and in Iran respectively (Mousavi et al., 2009). Colorectal cancer (CRC) (also called colon or bowel cancer) is one of the gastrointestinal cancer types invading the tissues of the colon. CRC is the most common fatal cancer and ranks as the third cause of death worldwide and the third and the fifth in Iranian women and men respectively (Zare-Bandamiri et al., 2016). Studies reveal that the incidence of colorectal cancer is hereditary only in 20% patients and the other 80% patients are affected by several modifiable factors including, physical inactivity, obesity and overweight, high consumption of red meat, and smoking (Naccarati et al., 2007). Correction of each one of these risk factors has a significant impact on the occurrence of the disease; Abstract Introduction: Colorectal cancer (CRC) is a commonly fatal cancer that ranks as third worldwide and third and the fifth in Iranian women and men, respectively. There are several methods for analyzing time to event data. Additive hazards regression models take priority over the popular Cox proportional hazards model if the absolute hazard (risk) change instead of hazard ratio is of primary concern, or a proportionality assumption is not made. Methods: This study used data gathered from medical records of 561 colorectal cancer patients who were admitted to Namazi Hospital, Shiraz, Iran, during 2005 to 2010 and followed until December 2015. The nonparametric Aalen's additive hazards model, semiparametric Lin and Ying's additive hazards model and Cox proportional hazards model were applied for data analysis. The proportionality assumption for the Cox model was evaluated with a test based on the Schoenfeld residuals and for test goodness of fit in additive models, Cox-Snell residual plots were used. Analyses were performed with SAS 9.2 and R3.2 software. Results: The median follow-up time was 49 months. The five-year survival rate and the mean survival time after cancer diagnosis were 59.6% and 68.1±1.4 months, respectively. Multivariate analyses using Lin and Ying's additive model and the Cox proportional model indicated that the age of diagnosis, site of tumor, stage, and proportion of positive lymph nodes, lymphovascular invasion and type of treatment were factors affecting survival of the CRC patients. Conclusion: Additive models are suitable alternatives to the Cox proportionality model if there is interest in evaluation of absolute hazard change, or no proportionality assumption is made.},
author = {{Farzan Madadizadeh Amin Ghanbarnejad3, Vahid Ghavami2*, Mohammad Zare Bandamiri4}, Mohammad Mohammadianpanah},
doi = {10.22034/APJCP.2017.18.4.1077},
file = {::},
title = {{Applying Additive Hazards Models for Analyzing Survival in Patients with Colorectal Cancer in Fars Province, Southern Iran}}
}
@article{Chio2009,
abstract = {We have performed a systematic review to summarize current knowledge concerning factors related to survival in ALS and to evaluate the implications of these data for clinical trials design. The med...},
author = {Chi{\`{o}}, Adriano and Logroscino, Giancarlo and Hardiman, Orla and Swingler, Robert and Mitchell, Douglas and Beghi, Ettore and Traynor, Bryan G.},
doi = {10.3109/17482960802566824},
issn = {17482968},
journal = {https://doi.org/10.3109/17482960802566824},
keywords = {Amyotrophic lateral sclerosis,prognostic factors,survival,trials},
number = {5-6},
pages = {310--323},
pmid = {19922118},
publisher = {Taylor & Francis},
title = {{Prognostic factors in ALS: A critical review}},
url = {https://www.tandfonline.com/doi/abs/10.3109/17482960802566824},
volume = {10},
year = {2009}
}
@article{Bettencourt2015,
author = {Bettencourt, Concei{\c{c}}{\~{a}}o and Houlden, Henry},
doi = {10.1038/NN.4012},
issn = {1546-1726},
journal = {Nature neuroscience},
keywords = {Amyotrophic Lateral Sclerosis / genetics*,Concei{\c{c}}{\~{a}}o Bettencourt,DNA*,Exome*,Genetic,Genetic Heterogeneity,Genome-Wide Association Study,Henry Houlden,Humans,MEDLINE,Missense,Models,Mutation,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Protein Serine-Threonine Kinases / genetics,Protein Serine-Threonine Kinases / physiology*,PubMed Abstract,Sequence Analysis,Superoxide Dismutase / genetics*,Superoxide Dismutase-1,doi:10.1038/nn.4012,pmid:25919956},
month = {may},
number = {5},
pages = {611--613},
pmid = {25919956},
publisher = {Nat Neurosci},
title = {{Exome sequencing uncovers hidden pathways in familial and sporadic ALS}},
url = {https://pubmed.ncbi.nlm.nih.gov/25919956/},
volume = {18},
year = {2015}
}
@article{Bettencourt2015a,
author = {Bettencourt, Concei{\c{c}}{\~{a}}o and Houlden, Henry},
doi = {10.1038/NN.4012},
issn = {1546-1726},
journal = {Nature neuroscience},
keywords = {Amyotrophic Lateral Sclerosis / genetics*,Concei{\c{c}}{\~{a}}o Bettencourt,DNA*,Exome*,Genetic,Genetic Heterogeneity,Genome-Wide Association Study,Henry Houlden,Humans,MEDLINE,Missense,Models,Mutation,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Protein Serine-Threonine Kinases / genetics,Protein Serine-Threonine Kinases / physiology*,PubMed Abstract,Sequence Analysis,Superoxide Dismutase / genetics*,Superoxide Dismutase-1,doi:10.1038/nn.4012,pmid:25919956},
month = {may},
number = {5},
pages = {611--613},
pmid = {25919956},
publisher = {Nat Neurosci},
title = {{Exome sequencing uncovers hidden pathways in familial and sporadic ALS}},
url = {https://pubmed.ncbi.nlm.nih.gov/25919956/},
volume = {18},
year = {2015}
}
@article{Chio2013,
abstract = {<b><i>Background:</i></b> Amyotrophic lateral sclerosis (ALS) is relatively rare, yet the economic and social burden is substantial. Having accurate incidence and prevalence estimates would facilitate efficient allocation of healthcare resources. <b><i>Objective:</i></b> To provide a comprehensive and critical review of the epidemiological literature on ALS. <b><i>Methods:</i></b> MEDLINE and EMBASE (1995-2011) databases of population-based studies on ALS incidence and prevalence reporting quantitative data were analyzed. Data extracted included study location and time, design and data sources, case ascertainment methods and incidence and/or prevalence rates. Medians and interquartile ranges (IQRs) were calculated, and ALS case estimates were derived using 2010 population estimates. <b><i>Results:</i></b> In all, 37 articles met the inclusion criteria. In Europe, the median incidence rate (/100,000 population) was 2.08 (IQR 1.47-2.43), corresponding to an estimated 15,355 (10,852-17,938) cases. Median prevalence (/100,000 population) was 5.40 (IQR 4.06-7.89), or 39,863 (29,971-58,244) prevalent cases. <b><i>Conclusions:</i></b> Disparity in rates among ALS incidence and prevalence studies may be due to differences in study design or true variations in population demographics such as age and geography, including environmental factors and genetic predisposition. Additional large-scale studies that use standardized case ascertainment methods are needed to more accurately assess the true global burden of ALS.},
author = {Chi{\`{o}}, A. and Logroscino, G. and Traynor, B. J. and Collins, J. and Simeone, J. C. and Goldstein, L. A. and White, L. A.},
doi = {10.1159/000351153},
file = {::},
issn = {0251-5350},
journal = {Neuroepidemiology},
keywords = {261057,Amyotrophic lateral sclerosis,Case ascertainment,Demographics,Diagnostic criteria,Epidemiology,Incidence,Neuroepidemiology 2013,No. 2,Prevalence,Temporal and age-related trends,Vol. 41},
month = {aug},
number = {2},
pages = {118--130},
pmid = {23860588},
publisher = {Karger Publishers},
title = {{Global Epidemiology of Amyotrophic Lateral Sclerosis: A Systematic Review of the Published Literature}},
url = {https://www.karger.com/Article/FullText/351153 https://www.karger.com/Article/Abstract/351153},
volume = {41},
year = {2013}
}
@article{Hilton2015,
abstract = {Mutations to the ubiquitous antioxidant enzyme Cu/Zn superoxide dismutase (SOD1) were the first established genetic cause of the fatal, adult-onset neurodegenerative disease amyotrophic lateral sclerosis (ALS). It is widely accepted that these mutations do not cause ALS via a loss of antioxidant function, but elucidating the alternate toxic gain of function has proven to be elusive. Under physiological conditions, SOD1 binds one copper ion and one zinc ion per monomer to form a highly stable and functional homodimer, but there is now ample evidence to indicate aberrant persistence of SOD1 in an intermediate metal-deficient state may contribute to the protein's involvement in ALS. This review briefly discusses some of the data to support a role for metal-deficient SOD1 in the development of ALS and some of the outcomes from drug development studies that have aimed to modify the symptoms of ALS by targeting the metal state of SOD1. The implications for the metal state of SOD1 in cases of sporadic ALS that do not involve mutant SOD1 are also discussed.},
author = {Hilton, James B. and White, Anthony R. and Crouch, Peter J.},
doi = {10.1007/S00109-015-1273-3},
file = {::},
issn = {14321440},
journal = {Journal of Molecular Medicine (Berlin, Germany)},
keywords = {Amyotrophic lateral sclerosis (ALS),Copper (Cu),Cu/Zn superoxide dismutase (SOD1),Diacetylbis(4-methylthiosemicarbazonato)copperII,Motor neuron disease (MND),Protein misfolding,Zinc (Zn)},
month = {may},
number = {5},
pages = {481},
pmid = {25754173},
publisher = {Springer},
title = {{Metal-deficient SOD1 in amyotrophic lateral sclerosis}},
url = {/pmc/articles/PMC4408375/ /pmc/articles/PMC4408375/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4408375/},
volume = {93},
year = {2015}
}
@article{Mulder1986,
abstract = {We analyzed the medical records of 103 patients with familial adult motor neuron disease (MND). In the 72 families, 329 members were known to be affected. Observations were compared with the sporadic and Mariana forms of MND. Clinical and laboratory examinations of all three forms were similar in clinical course and findings, but there were minor variations in age at onset, sex ratio, survival, and the frequency with which onset occurred in the lower extremities. Recognition of the familial form still depends on diagnosis of the disease in more than one member of a family.},
author = {Mulder, D. W. and Kurland, L. T. and Offord, K. P. and Beard, M.},
doi = {10.1212/WNL.36.4.511},
issn = {0028-3878},
journal = {Neurology},
keywords = {Adult,Aged,Amyotrophic Lateral Sclerosis / genetics*,Amyotrophic Lateral Sclerosis / physiopathology,C M Beard,D W Mulder,Female,Humans,L T Kurland,MEDLINE,Male,Middle Aged,Motor Neurons,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,P.H.S.,PubMed Abstract,Research Support,U.S. Gov't,doi:10.1212/wnl.36.4.511,pmid:3960325},
number = {4},
pages = {511--517},
pmid = {3960325},
publisher = {Neurology},
title = {{Familial adult motor neuron disease: amyotrophic lateral sclerosis}},
url = {https://pubmed.ncbi.nlm.nih.gov/3960325/},
volume = {36},
year = {1986}
}
@misc{alsfs,
title = {{Amyotrophic Lateral Sclerosis (ALS) Fact Sheet | National Institute of Neurological Disorders and Stroke}},
url = {https://www.ninds.nih.gov/Disorders/Patient-Caregiver-Education/Fact-Sheets/Amyotrophic-Lateral-Sclerosis-ALS-Fact-Sheet},
urldate = {2022-03-18}
}
@article{Phukan2007,
abstract = {<h2>Summary</h2><p>Amyotrophic lateral sclerosis (ALS) is a motor neuron disease that has sporadic and inherited forms. ALS is the most common neurodegenerative disorder of young and middle-aged adults, and few treatments are available. Although the degeneration predominantly affects the motor system, cognitive and behavioural symptoms have been described for over a century, and there is evidence that ALS and frontotemporal dementia overlap clinically, radiologically, pathologically, and genetically. Cognitive decline in ALS is characterised by personality change, irritability, obsessions, poor insight, and pervasive deficits in frontal executive tests. This presentation is consistent with the changes to character, social conduct, and executive function in frontotemporal dementia. We highlight genetic, imaging, and neuropathological evidence that non-motor systems are affected in ALS and explain the importance of recent discoveries. We review studies of cognitive impairment in ALS and common neuropsychological test results. We also provide advice about clinical assessment of frontotemporal dysfunction in patients with ALS, and suggest future research. Understanding of cognitive impairment in ALS will improve care for patients and their families and provide valuable insights into the pathogenesis of neurodegeneration.</p>},
author = {Phukan, Julie and Pender, Niall P. and Hardiman, Orla},
doi = {10.1016/S1474-4422(07)70265-X},
issn = {1474-4422},
journal = {The Lancet Neurology},
month = {nov},
number = {11},
pages = {994--1003},
pmid = {17945153},
publisher = {Elsevier},
title = {{Cognitive impairment in amyotrophic lateral sclerosis}},
url = {http://www.thelancet.com/article/S147444220770265X/fulltext http://www.thelancet.com/article/S147444220770265X/abstract https://www.thelancet.com/journals/laneur/article/PIIS1474-4422(07)70265-X/abstract},
volume = {6},
year = {2007}
}
@article{Chio2009a,
abstract = {We have performed a systematic review to summarize current knowledge concerning factors related to survival in ALS and to evaluate the implications of these data for clinical trials design. The median survival time from onset to death ranges from 20 to 48 ;months, but 1020% of ALS patients have a survival longer than 10 ;years. Older age and bulbar onset are consistently reported to have a worse outcome. There are conflicting data on gender, diagnostic delay and El Escorial criteria. The rate of symptom progression was revealed to be an independent prognostic factor. Psychosocial factors, FTD, nutritional status, and respiratory function are also related to ALS outcome. The effect of enteral nutrition on survival is still unclear, while NIPPV has been found to improve survival. There are no well established biological markers of progression, although some are likely to emerge in the near future. These findings have relevant implications for the design of future trials. Randomization, besides the type of onset, should take into account age, respiratory status at entry, and a measure of disease progression pre-entry. Alternative trial designs can include the use of natural history controls, the so-called minimization method for treatment allocation, and the futility approach. {\textcopyright} 2009 Informa UK Ltd All rights reserved.},
annote = {The median survival time from onset to death ranges from 20 to 48 months, but 10–20% of ALS patients have a survival longer than 10 years. 

Older age and bulbar onset are consistently reported to have a worse outcome. There are conflicting data on gender, diagnostic delay and El Escorial criteria. 

The rate of symptom progression was revealed to be an independent prognostic factor. Psychosocial factors, FTD, nutritional status, and respiratory function are also related to ALS outcome.},
author = {Chi{\`{o}}, Adriano and Logroscino, Giancarlo and Hardiman, Orla and Swingler, Robert and Mitchell, Douglas and Beghi, Ettore and Traynor, Bryan G.},
doi = {10.3109/17482960802566824},
file = {:Users/jferreira-admin/Downloads/nihms-423781.pdf:pdf},
issn = {17482968},
journal = {Amyotrophic Lateral Sclerosis},
keywords = {Amyotrophic lateral sclerosis,Prognostic factors,Survival,Trials},
number = {5-6},
pages = {310--323},
pmid = {19922118},
title = {{Prognostic factors in ALS: A critical review}},
volume = {10},
year = {2009}
}
@article{Wolf2014,
abstract = {Background: Survival in amyotrophic lateral sclerosis varies considerably. About one third of the patients die within 12 months after first diagnosis. The early recognition of fast progression is essential for patients and neurologists to weigh up invasive therapeutic interventions. In a prospective, population-based cohort of ALS patients in Rhineland-Palatinate, Germany, we identified significant prognostic factors at time of diagnosis that allow prediction of early death within first 12 months.Methods: Incident cases, diagnosed between October 2009 and September 2012 were enrolled and followed up at regular intervals of 3 to 6 months. Univariate analysis utilized the Log-Rank Test to identify association between candidate demographic and disease variables and one-year mortality. In a second step we investigated a multiple logistic regression model for the optimal prediction of one-year mortality rate. Results: In the cohort of 176 ALS patients (mean age 66.2 years; follow-up 100%) one-year mortality rate from diagnosis was 34.1%. Multivariate analysis revealed that age over 75 years, interval between symptom onset and diagnosis below 7 months, decline of body weight before diagnosis exceeding 2 BMI units and Functional Rating Score below 31 points were independent factors predicting early death. Conclusions: Probability of early death within 12 months from diagnosis is predicted by advanced age, short interval between symptom onset and first diagnosis, rapid decline of body weight before diagnosis and advanced functional impairment. Trial registration: ClinicalTrials.gov (NCT01955369, registered September 28, 2013).},
author = {Wolf, Joachim and Safer, Anton and W{\"{o}}hrle, Johannes C. and Palm, Frederick and Nix, Wilfred A. and Maschke, Matthias and Grau, Armin J.},
doi = {10.1186/s12883-014-0197-9},
file = {:Users/jferreira-admin/Downloads/s12883-014-0197-9.pdf:pdf},
issn = {14712377},
journal = {BMC Neurology},
keywords = {Amyotrophic lateral sclerosis,Mortality rate,Population register,Prognostic factors},
number = {1},
pages = {4--9},
pmid = {25280575},
title = {{Factors predicting one-year mortality in amyotrophic lateral sclerosis patients - data from a population-based registry}},
volume = {14},
year = {2014}
}
@article{Dardiotis2018,
abstract = {BackgroundSeveral studies have examined the relationship between body mass index (BMI) and survival from amyotrophic lateral sclerosis (ALS). Many indicate that low BMI at diagnosis or during follow-up may be associated with accelerated progression and shortened survival. This study systematically evaluated the relationship between BMI and survival in patients with ALS.MethodsThe PubMed database was searched to identify all available studies reporting time-to-event data. Eight studies with 6,098 patients fulfilled the eligibility criteria. BMI was considered a continuous and ordered variable. Interstudy heterogeneity was assessed by the Cochran Q test and quantified by the I2 metric. Fixed- or random-effects odds ratios summarized pooled effects after taking interstudy variability into account. Significance was set at p < 0.05.ResultsThe ALS survival hazard ratio (HR) decreased approximately by 3% (95% confidence interval [CI]: 2%-5%) for each additional BMI unit when BMI was considered a continuous variable. When BMI was considered a categorical variable, the HRs for "normal" BMI vs "overweight" BMI and "obese" BMI were estimated to be as high as 0.91 (95% CI: 0.79-1.04) and 0.78 (95% CI: 0.60-1.01), respectively. The HR for the comparison of the "normal" BMI vs "underweight" BMI was estimated to be as high as 1.94 (95% CI: 1.42-2.65).ConclusionsBMI is significantly and inversely associated with ALS survival.},
author = {Dardiotis, Efthimios and Siokas, Vasileios and Sokratous, Maria and Tsouris, Zisis and Aloizou, Athina Maria and Florou, Desponia and Dastamani, Metaxia and Mentis, Alexios Fotios A. and Brotis, Alexandros G.},
doi = {10.1212/CPJ.0000000000000521},
file = {:Users/jferreira-admin/Downloads/Bodymassindexandsurvivalfromamyotrophiclateralsclerosisametaanalysis.pdf:pdf},
isbn = {0000000000000},
issn = {21630933},
journal = {Neurology: Clinical Practice},
number = {5},
pages = {437--444},
title = {{Body mass index and survival from amyotrophic lateral sclerosis: A meta-analysis}},
volume = {8},
year = {2018}
}
@article{Chio2010,
abstract = {We evaluated the clinical characteristics and outcome of tracheostomy in amyotrophic lateral sclerosis (ALS) using data from the Piemonte and Valle d'Aosta Register for ALS, a prospective epidemiological register collecting all ALS incident cases in two Italian regions. Among the 1260 patients incident in the period 1995-2004, 134 (10.6%) underwent tracheostomy. Young male patients were more likely to be tracheostomised. Site of onset (bulbar vs spinal) and period of diagnosis (1995-1999 vs 2000-2004) did not influence the likelihood of being tracheostomised. The mean duration of hospital stay was 52.0 days (SD 60.5). Overall, 27 patients died while still in hospital (20.1%). Sixty-five patients (48.5%) were discharged to home, whereas 42 (31.3%) were admitted to long-term care facilities. The median survival time after tracheostomy was 253 days. In the Cox multivariable model, the factors independently related to a longer survival were enteral nutrition, age, marital status and ALS centre follow-up. In conclusion, in an epidemiological setting, ALS survival after tracheostomy was <1 year. Sociocultural factors influence the probability of choice to be tracheostomised, even in a highly socialised health system as Italian one.},
author = {Chi{\`{o}}, Adriano and Calvo, A. and Ghiglione, P. and Mazzini, L. and Mutani, R. and Mora, G.},
doi = {10.1136/jnnp.2009.175984},
file = {:Users/jferreira-admin/Downloads/jnnp.2009.175984(1).pdf:pdf},
issn = {00223050},
journal = {Journal of Neurology, Neurosurgery and Psychiatry},
number = {10},
pages = {1141--1143},
pmid = {20660920},
title = {{Tracheostomy in amyotrophic lateral sclerosis: A 10-year population-based study in Italy}},
volume = {81},
year = {2010}
}
@article{Marin2016,
abstract = {The natural history of amyotrophic lateral sclerosis (ALS) and patient risk stratification are areas of considerable research interest. We aimed (1) to describe the survival of a representative cohort of French ALS patients, and (2) to identify covariates associated with various patterns of survival using a risk classification analysis. ALS patients recruited in the FRALim register (2000–2013) were included. Time-to-death analyses were performed using Kaplan–Meier method and Cox model. A recursive partitioning and amalgamation (RECPAM) algorithm analysis identified subgroups of patients with different patterns of survival. Among 322 patients, median survival times were 26.2 and 15.6 months from time of onset and of diagnosis, respectively. Four groups of patients were identified, depending on their baseline characteristics and survival (1) ALSFRS-R slope >0.46/month and definite or probable ALS (median survival time (MST) 10.6 months); (2) ALSFRS-R slope >0.46/month and possible or probable laboratory-supported ALS (MST: 18.1 months); (3) ALSFRS-R slope ≤0.46/month and definite or probable ALS (MST: 22.5 months), and (4) ALSFRS-R slope ≤0.46/month and possible or probable laboratory-supported ALS (MST: 37.6 months). Median survival time is among the shortest ever reported by a worldwide population-based study. This is probably related to the age structure of the patients (the oldest identified to date), driven by the underlying population (30 % of subjects older than 60 years). Further research in the field of risk stratification could help physicians better anticipate prognosis of ALS patients, and help improve the design of randomized controlled trials.},
author = {Marin, Beno{\^{i}}t and Couratier, Philippe and Arcuti, Simona and Copetti, Massimiliano and Fontana, Andrea and Nicol, Marie and Raymondeau, Marie and Logroscino, Giancarlo and Preux, Pierre Marie},
doi = {10.1007/s00415-015-7940-z},
file = {:Users/jferreira-admin/Downloads/s00415-015-7940-z(1).pdf:pdf},
issn = {14321459},
journal = {Journal of Neurology},
keywords = {Amyotrophic lateral sclerosis,Epidemiology,Mortality,Prognosis,Survival},
number = {1},
pages = {100--111},
pmid = {26518423},
title = {{Stratification of ALS patients' survival: a population-based study}},
volume = {263},
year = {2016}
}
@article{Hirose2018a,
abstract = {Introduction: We evaluated post–noninvasive ventilation survival and factors for the transition to tracheostomy in amyotrophic lateral sclerosis (ALS). Methods: We analyzed 197 patients using a prospectively collected database with 114 patients since 2000. Results: Among 114 patients, 59 patients underwent noninvasive ventilation (NIV), which prolonged the total median survival time to 43 months compared with 32 months without treatment. The best post-NIV survival was associated with a lack of bulbar symptoms, higher measured pulmonary function, and a slower rate of progression at diagnosis. The transition rate from NIV to tracheostomy gradually decreased over the years. Patients using NIV for more than 6 months were more likely to refuse tracheostomy and to be women. Discussion: This study confirmed a positive survival effect with NIV, which was less effective in patients with bulbar dysfunction. Additional studies are required to determine the best timing for using NIV with ALS in patients with bulbar dysfunction. Muscle Nerve 58:770–776 2018.},
author = {Hirose, Takahiko and Kimura, Fumiharu and Tani, Hiroki and Ota, Shin and Tsukahara, Akihiro and Sano, Eri and Shigekiyo, Taro and Nakamura, Yoshitsugu and Kakiuchi, Kensuke and Motoki, Mikiko and Unoda, Kiichi and Ishida, Simon and Nakajima, Hideto and Arawaka, Shigeki},
doi = {10.1002/mus.26149},
file = {:Users/jferreira-admin/Downloads/mus.26149.pdf:pdf},
issn = {10974598},
journal = {Muscle and Nerve},
keywords = {ALS,NIV,motor neuron disease,respiratory failure,survival prediction,tracheostomy},
number = {6},
pages = {770--776},
pmid = {29679377},
title = {{Clinical characteristics of long-term survival with noninvasive ventilation and factors affecting the transition to invasive ventilation in amyotrophic lateral sclerosis}},
volume = {58},
year = {2018}
}
@article{Sancho2011,
abstract = {Background: Home tracheotomy mechanical ventilation (HTMV) can prolong survival in patients with amyotrophic lateral sclerosis (ALS) when non-invasive ventilation (NIV) fails, but knowledge about HTMV is scarce. The aim of this study was to determine the causes of tracheotomy and the main issues of 1-year HTMV in a cohort of patients with ALS. Methods: A prospective study of all patients needing HTMV was performed in a referral respiratory care unit (RCU) from April 2001 to January 2010. Patients' informed decisions about HTMV were fully respected. Caregivers were trained and could telephone the RCU. Hospital staff made home visits. Results: All patients (n=116) agreed to participate and a tracheotomy was needed for 76, mainly due to bulbar dysfunction. Of the 38 who had a tracheotomy, in 21 it was performed in an acute setting and in 17 as a nonemergency procedure. In 19 patients the tracheotomy was related to the inadequacy of mechanically assisted coughing (MAC) to maintain normal oxygen saturation. During HTMV, 19 patients required hospitalisation, 12 with respiratory problems. The 1-year survival rate was 78.9%, with a mean survival of 10.39 months (95% CI 9.36 to 11.43). Sudden death was the main cause of death (n=9) and only one patient died from respiratory causes. No predictive factors for survival were found. Conclusions: Besides NIV inadequacy, the ineffectiveness of mechanically assisted coughing appears to be a relevant cause of tracheotomy for patients with ALS with severe bulbar dysfunction. Patients choosing HTMV provided by a referral RCU could have a good 1-year survival rate, respiratory problems being the main cause of hospitalisation but not of death.},
author = {Sancho, Jes{\'{u}}s and Servera, Emilio and D{\'{i}}az, Jos{\'{e}} Luis and Ba{\~{n}}uls, Pilar and Mar{\'{i}}n, Julio},
doi = {10.1136/thx.2011.160481},
file = {:Users/jferreira-admin/Downloads/948.full.pdf:pdf},
issn = {14683296},
journal = {Thorax},
number = {11},
pages = {948--952},
title = {{Home tracheotomy mechanical ventilation in patients with amyotrophic lateral sclerosis: Causes, complications and 1-year survival}},
volume = {66},
year = {2011}
}
@article{Cui2018a,
abstract = {Percutaneous endoscopic gastrostomy (PEG) is a method widely used for patients with amyotrophic lateral sclerosis (ALS); nevertheless, its effect on survival remains unclear. The purpose of this meta-analysis study was to determine the effects of PEG on survival in ALS patients. Relevant studies were retrieved from PubMed, EmBase, and the Cochrane Library databases, from inception to June 2017. Studies comparing PEG with other procedures in ALS patients were included. Odds ratios (ORs) in a random-effects model were used to assess the survival at different follow-up periods. Briefly, ten studies involving a total of 996 ALS patients were included. Summary ORs indicated that PEG administration was not associated with 30-day (OR = 1.59; 95%CI 0.93-2.71; P = 0.092), 10-month (OR = 1.25; 95%CI 0.72-2.17; P = 0.436), and 30-month (OR = 1.28; 95% CI 0.77-2.11; P = 0.338) survival rates, while they showed a beneficial effect in 20-month survival rate (OR = 1.97; 95% CI 1.21-3.21; P = 0.007). The survival rate was significantly prominent in reports published before 2005, and in studies with a retrospective design, sample size <100, mean age <60.0 years, and percentage male !50.0%. To sum up, these findings suggested that ALS patients administered with PEG had an increased 20-month survival rates, while there was no significant effect in 30-day, 10-month, and 30-month survival rates.},
author = {Cui, Fang and Sun, Liuqing and Xiong, Jianmei and Li, Jianyong and Zhao, Yangang and Huang, Xusheng},
doi = {10.1371/journal.pone.0192243},
file = {::},
isbn = {1111111111},
title = {{Therapeutic effects of percutaneous endoscopic gastrostomy on survival in patients with amyotrophic lateral sclerosis: A meta-analysis}},
url = {https://doi.org/10.1371/journal.pone.0192243},
year = {2018}
}
@article{Cui2018b,
abstract = {Percutaneous endoscopic gastrostomy (PEG) is a method widely used for patients with amyotrophic lateral sclerosis (ALS); nevertheless, its effect on survival remains unclear. The purpose of this meta-analysis study was to determine the effects of PEG on survival in ALS patients. Relevant studies were retrieved from PubMed, EmBase, and the Cochrane Library databases, from inception to June 2017. Studies comparing PEG with other procedures in ALS patients were included. Odds ratios (ORs) in a random-effects model were used to assess the survival at different follow-up periods. Briefly, ten studies involving a total of 996 ALS patients were included. Summary ORs indicated that PEG administration was not associated with 30-day (OR = 1.59; 95%CI 0.93–2.71; P = 0.092), 10-month (OR = 1.25; 95%CI 0.72–2.17; P = 0.436), and 30-month (OR = 1.28; 95% CI 0.77–2.11; P = 0.338) survival rates, while they showed a beneficial effect in 20-month survival rate (OR = 1.97; 95% CI 1.21–3.21; P = 0.007). The survival rate was significantly prominent in reports published before 2005, and in studies with a retrospective design, sample size <100, mean age <60.0 years, and percentage male 50.0%. To sum up, these findings suggested that ALS patients administered with PEG had an increased 20-month survival rates, while there was no significant effect in 30-day, 10-month, and 30-month survival rates.},
author = {Cui, Fang and Sun, Liuqing and Xiong, Jianmei and Li, Jianyong and Zhao, Yangang and Huang, Xusheng},
doi = {10.1371/JOURNAL.PONE.0192243},
file = {:Users/jferreira-admin/Downloads/journal.pone.0192243.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
month = {feb},
number = {2},
pmid = {29408898},
publisher = {Public Library of Science},
title = {{Therapeutic effects of percutaneous endoscopic gastrostomy on survival in patients with amyotrophic lateral sclerosis: A meta-analysis}},
volume = {13},
year = {2018}
}
@article{Chio1999,
abstract = {Percutaneous endoscopic gastrostomy (PEG) has been proposed as symptomatic treatment of dysphagia in patients with ALS. Safety and factors related to survival after PEG were analyzed in 50 consecutive ALS patients. No major acute or long-term complications were observed. Stabilization or increase in weight were observed after PEG. Median survival after PEG was 185 days, with a worse outcome in patients with weight loss ≥10% healthy body weight and forced vital capacity <65%. PEG may be a useful option in the symptomatic treatment of dysphagia in ALS.},
author = {Chi{\`{o}}, Adriano and Finocchiaro, E. and Meineri, P. and Bottacchi, E. and Schiffer, D.},
doi = {10.1212/WNL.53.5.1123},
issn = {0028-3878},
journal = {Neurology},
keywords = {80 and over,A Chi{\`{o}},Adult,Aged,Amyotrophic Lateral Sclerosis / mortality,Amyotrophic Lateral Sclerosis / surgery*,D Schiffer,Deglutition Disorders / surgery*,E Finocchiaro,Endoscopy / adverse effects*,Gastrostomy / adverse effects*,Humans,MEDLINE,Middle Aged,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,PubMed Abstract,Survival Analysis,doi:10.1212/wnl.53.5.1123,pmid:10496278},
month = {sep},
number = {5},
pages = {1123--1125},
pmid = {10496278},
publisher = {Neurology},
title = {{Safety and factors related to survival after percutaneous endoscopic gastrostomy in ALS. ALS Percutaneous Endoscopic Gastrostomy Study Group}},
url = {https://pubmed.ncbi.nlm.nih.gov/10496278/},
volume = {53},
year = {1999}
}
@article{Khairoalsindi2018,
abstract = {Amyotrophic lateral sclerosis is a neurodegenerative disease that leads to loss of the upper and lower motor neurons. Almost 90% of all cases occur in the sporadic form, with the rest occurring in the familial form. The disease has a poor prognosis, with only two disease-modifying drugs approved by the United States Food and Drug Administration (FDA). The approved drugs for the disease have very limited survival benefits. Edaravone is a new FDA-approved medication that may slow the disease progression by 33% in a selected subgroup of ALS patients. This paper covers the various interventions that may provide survival benefits, such as early diagnosis, medications, gene therapy, stem cell therapy, diet, nutritional supplements, multidisciplinary clinics, and mechanical invasive and noninvasive ventilation. The recent data on masitinib, the role of enteral feeding, gene therapy, and stem cell therapy is discussed.},
author = {Khairoalsindi, Osama A. and Abuzinadah, Ahmad R.},
doi = {10.1155/2018/6534150},
file = {::},
issn = {20901860},
journal = {Neurology Research International},
publisher = {Hindawi Limited},
title = {{Maximizing the Survival of Amyotrophic Lateral Sclerosis Patients: Current Perspectives}},
volume = {2018},
year = {2018}
}
@article{Vianello2011,
abstract = {Background: Acute respiratory failure (ARF) is a common event in the advanced stage of amyotrophic lateral sclerosis (ALS) and may be rarely a presenting symptom. Frequently, such patients require intubation and mechanical ventilation (MV) and, in a large proportion, receive tracheostomy, as a consequence of weaning failure. In our study, we investigated postdischarge survival and quality of life (QoL) after tracheostomy for ARF in patients with ALS. Methods: Design: This study is a retrospective chart review combined with prospective evaluation of QoL and degree of depression. Setting: The study was conducted in an adult, respiratory intensive care unit in a university hospital. Patients: Amyotrophic lateral sclerosis patients with tracheostomy for ARF between January 1, 1995 and April 30, 2008 were investigated. Intervention and measurements: (a) A retrospective chart review was used and (b) prospective administration of the 11-item short-form Life Satisfaction Index (LSI-11) and Beck Depression Inventory (BDI) questionnaires to survivors, at least 1 month after discharge from hospital, was performed. Results: Sixty patients were studied retrospectively. None of the patients died in the hospital after tracheostomy. Forty-two patients (70%) were discharged completely MV dependent, and 17 patients (28.3%) were partially MV dependent. One patient (1.6%) was liberated from MV.The median survival after tracheostomy was 21 months (range, 0-155 months). The survival rate was 65% by 1 year and 45% by 2 years after tracheostomy. Survival was significantly shorter in patients older than 60 years at tracheostomy, with a hazard ratio of dying of 2.1 (95% confidence interval, 1.1-3.9). All 13 survivors completed the LSI-11 and BDI. The mean (SD) cumulative score on the LSI-11 was 9.3 (3.6; range, 0-22; higher values indicating better QoL), similar to that obtained from a control group consisting of individuals with ALS who had not received tracheostomy (9.3 ± 4.3) and to that reported for persons in the general population. Only 15% of the tracheostomized patients (2/13) were severely depressed, according to BDI; 11 of 13 patients reported a positive view of tracheostomy and said that they would want to undergo this procedure if they could make the decision again. Conclusions: Patients with ALS have a high chance of long-term survival after tracheostomy for ARF. Although administered at the time of a respiratory crisis without being discussed in advance, tracheostomy shows good acceptance and results in acceptable QoL. {\textcopyright} 2011 Elsevier Inc.},
author = {Vianello, Andrea and Arcaro, Giovanna and Palmieri, Arianna and Ermani, Mario and Braccioni, Fausto and Gallan, Federico and Soraru', Gianni and Pegoraro, Elena},
doi = {10.1016/J.JCRC.2010.06.003},
issn = {1557-8615},
journal = {Journal of critical care},
keywords = {Acute Disease,Aged,Amyotrophic Lateral Sclerosis / complications*,Amyotrophic Lateral Sclerosis / mortality,Amyotrophic Lateral Sclerosis / surgery,Andrea Vianello,Depression / epidemiology,Elena Pegoraro,Female,Giovanna Arcaro,Humans,MEDLINE,Male,Middle Aged,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Patient Satisfaction / statistics & numerical data,Prospective Studies,PubMed Abstract,Quality of Life / psychology*,Respiratory Insufficiency / etiology,Respiratory Insufficiency / mortality*,Respiratory Insufficiency / surgery*,Retrospective Studies,Survival Rate,Tracheostomy*,Treatment Outcome,doi:10.1016/j.jcrc.2010.06.003,pmid:20655697},
number = {3},
pages = {329.e7--329.e14},
pmid = {20655697},
publisher = {J Crit Care},
title = {{Survival and quality of life after tracheostomy for acute respiratory failure in patients with amyotrophic lateral sclerosis}},
url = {https://pubmed.ncbi.nlm.nih.gov/20655697/},
volume = {26},
year = {2011}
}
@article{Siirala2013,
abstract = {Background: Hypoventilation due to respiratory muscle atrophy is the most common cause of death as a result of amyotrophic lateral sclerosis (ALS). Patients aged over 65 years and presenting bulbar symptoms are likely to have a poorer prognosis. The aim of the study was to assess the possible impact of age and treatment with non-invasive ventilation (NIV) on survival in ALS. Based on evidence from earlier studies, it was hypothesized that NIV increases rates of survival regardless of age. Methods. Eighty-four patients diagnosed with ALS were followed up on from January 2001 to June 2012. These patients were retrospectively divided into two groups according to their age at the time of diagnosis: Group 1 comprised patients aged ≤ 65 years while Group 2 comprised those aged > 65 years. Each group included 42 patients. NIV was tolerated by 23 patients in Group 1 and 18 patients in Group 2. Survival was measured in months from the date of diagnosis. Results: The median age in Group 1 was 59 years (range 49 - 65) and 76 years in Group 2 (range 66 - 85). Among patients in Group 1 there was no difference in probability of survival between the NIV users and non-users (Hazard Ratio = 0.88, 95% CI 0.44 - 1.77, p = 0.7). NIV users in Group 2 survived longer than those following conventional treatment (Hazard Ratio = 0.25, CI 95% 0.11 - 0.55, p <0.001). ALS patients in Group 2 who did not use NIV had a 4-fold higher risk for death compared with NIV users. Conclusions: This retrospective study found that NIV use was associated with improved survival outcomes in ALS patients older than 65 years. Further studies in larger patient populations are warranted to determine which factors modify survival outcomes in ALS. {\textcopyright} 2013 Siirala et al.; licensee BioMed Central Ltd.},
author = {Siirala, Waltteri and Aantaa, Riku and Olkkola, Klaus T. and Saaresranta, Tarja and Vuori, Arno},
doi = {10.1186/1472-684X-12-23/FIGURES/2},
file = {::},
issn = {1472684X},
journal = {BMC Palliative Care},
keywords = {Amyotrophic lateral sclerosis,Non-invasive ventilation,Survival},
month = {may},
number = {1},
pages = {1--6},
publisher = {BioMed Central},
title = {{Is the effect of non-invasive ventilation on survival in amyotrophic lateral sclerosis age-dependent?}},
url = {https://bmcpalliatcare.biomedcentral.com/articles/10.1186/1472-684X-12-23},
volume = {12},
year = {2013}
}
@article{Ackrivo2021,
abstract = {Rationale: Noninvasive ventilation (NIV) is standard of care in amyotrophic lateral sclerosis (ALS), yet few data exist regarding its benefits. Objectives: We sought to identify whether the use of NIV was associated with survival in ALS. Methods: This was a single-center retrospective cohort study of 452 patients with ALS seen between 2006 and 2015. We matched one or more NIV subjects (prescribed NIV) to non-NIV subjects (never prescribed NIV) without replacement. The outcome was time from NIV prescription date (NIV subjects) or matched date (non-NIV subjects) until death. We performed a multivariable Cox proportional hazards model with NIV hourly usage as a time-varying covariate and stratified by matched groups. Results: After creating 180 matched groups and adjusting for age, body mass index, ALS Functional Rating Scale Revised dyspnea score, and hourly NIV use, NIV was associated with a 26% reduction in the rate of death compared with non-NIV subjects (hazard ratio [HR], 0.74; 95% confidence interval [CI], 0.57–0.98; P = 0.04). Among those with limb-onset ALS, NIV subjects had a 37% lower rate of death compared with non-NIV subjects (HR, 0.63; 95% CI, 0.45–0.87; P = 0.006). Among NIV subjects, we found that NIV use for an average of >4 h/d was associated with improved survival. Conclusions: NIV use was associated with significantly better survival in ALS after matching and adjusting for confounders. Increasing duration of daily NIV use was associated with longer survival. Randomized clinical trials should be performed to identify ideal thresholds for improving survival and optimizing adherence in ALS.},
author = {Ackrivo, Jason and Hsu, Jesse Y. and Hansen-Flaschen, John and Elman, Lauren and Kawut, Steven M.},
doi = {10.1513/ANNALSATS.202002-169OC/SUPPL_FILE/DISCLOSURES.PDF},
file = {::},
issn = {23256621},
journal = {Annals of the American Thoracic Society},
keywords = {Amyotrophic lateral sclerosis,Matching,Noninvasive ventilation,Respiratory failure,Survival analysis},
month = {mar},
number = {3},
pages = {486--494},
pmid = {32946280},
publisher = {American Thoracic Society},
title = {{Noninvasive Ventilation Use Is Associated with Better Survival in Amyotrophic Lateral Sclerosis}},
url = {www.atsjournals.org.},
volume = {18},
year = {2021}
}
@article{GomezRebollo2021,
abstract = {Abstract In ALS with established respiratory failure, patients may benefit from the use of tracheostomy. There are few studies regarding the survival of these patients and related prognostic factors. Aims: To evaluate survival and factors related to the presence of tracheostomy in ALS patients. To analyze prognostic factors for survival. Methods: Observational and retrospective study(January 2005-February 2021)in ALS patients under follow-up by Pneumology. The following variables were recorded:age, sex,time since diagnosis, onset of symptoms, type of ALS, use of NIV, use of cough assistant, percutaneous gastrostomy, hospitalizations,days of stay and tracheostomy. Data were analyzed by Chi-square, U-Mann-Whitney, Kaplan-Meier, Spearman's Rho and Cox regression. Results: 218 patients were included, mean age 63.4+/-12.9years, 53.1%male, 81.9%sporadic ALS and 58.4%spinal ALS.Some 5.1%had tracheostomy, associated with the use of life support ventilation[OR=14.4(95%CI 1.8-116.2)],gastrostomy[OR=15.7(95%CI 1.9-125.0)],younger age at diagnosis(47.4±14.9years),higher number of admissions(2.6±1.7)and hospital stay(85.2±118.7days)p>0.005.There were no significant differences in tracheostomy survival(32.7±11.8months)and overall cohort survival(40.8±7.8months).In the univariate and subsequent multivariate analysis, the overall survival of patients is related to age at diagnosis and bulbar onset(HR=1.05 p<0.001 and HR=1.6 p=0.028). Conclusions: In our study, the use of tracheostomy does not improve survival over the remaining patients who refuse it. Tracheostomy is related with patients who presented continuous use of NIV, need for gastrostomy and higher number of admissions and hospital stay. Age at diagnosis and bulbar form are considered prognostic factors for survival.},
author = {{Gomez Rebollo}, Cristina and {Melgar Herrero}, Mar{\'{i}}a and {Mira Padilla}, Estefan{\'{i}}a and {Mu{\~{n}}oz Corroto}, Cristina and {Garc{\'{i}}a Lovera}, Pablo and {Pascual Mart{\'{i}}nez}, Natalia},
journal = {European Respiratory Journal},
number = {suppl 65},
title = {{Tracheostomy in ALS.Survival and related factors}},
volume = {58},
year = {2021}
}
@article{Bianchi2021,
abstract = {Abstract Objective: To assess survival of ALS patients in general and in selected demographic and clinical subgroups comparing two periods (1998–2000 vs. 2008–2010). Methods: Newly diagnosed adults resident of Lombardy, Northern Italy from a population-based registry were included. Data were collected on age at diagnosis, sex, site of onset, diagnostic delay, and El-Escorial diagnostic category. Patients were followed until death or last observation. Survival was evaluated using Kaplan–Meier curves and Cox's proportional hazards models. Results: In 2008–2010 (267 patients), median survival was 2.4 years and 1-year, 2-year, 3-year and 5-year survival rates were 79%, 56%, 41% and 24%. Longer survival was associated with male sex, younger age, spinal onset, and longer diagnostic delay. Multivariable analysis confirmed higher death in 65–69yr (HR 2.8; 95% CI 1.4-5.6), 70–74yr (HR 3.2; 95% CI 1.6-6.3) and 75 + yr (HR 6.9; 95% CI 3.5-13.8) categories, compared to ≤49yr, in females (HR 1.4; 95% CI 1.02-1.8), compared to males, and in patients diagnosed after 6–12 months (HR 1.9; 95% CI 1.4-2.7), compared with longer diagnostic delay. In 1998–2000 (235 patients), median survival was 2.2 years. The 1-year, 2-year, 3-year and 5-year survival rates were 77%, 53%, 38% and 20%. When adjusting for demographic and clinical variables, the HR for death in 2008–2010 versus 1998–2000 was 0.80 (95% CI 0.66–0.98). A significant increase of survival in 2008–2010 was found only in patients aged 50–59yr and 70–74yr at diagnosis. Conclusions: Survival of ALS has increased over time in the last decades, especially in middle aged and elderly patients. The benefits of comprehensive care in selected age groups might explain our findings.},
author = {Bianchi, Elisa and Pupillo, Elisabetta and {De Feudis}, Antonio and Enia, Gabriele and Vitelli, Eugenio and Beghi, Ettore},
doi = {10.1080/21678421.2021.2004167},
issn = {21679223},
journal = {https://doi.org/10.1080/21678421.2021.2004167},
keywords = {Survival,amyotrophic lateral sclerosis,epidemiology,population-based,registry},
publisher = {Taylor & Francis},
title = {{Trends in survival of ALS from a population-based registry}},
url = {https://www.tandfonline.com/doi/abs/10.1080/21678421.2021.2004167},
year = {2021}
}
@article{Cai2022,
abstract = {To investigate the clinical and genetic factors influencing the survival of amyotrophic lateral sclerosis (ALS) patients in China.Patients were enrolled in the study between December 2013 and Decem...},
author = {Cai, Zhengyi and Liu, Qing and Liu, Mingsheng and Yang, Xunzhe and Shen, Dongchao and Sun, Xiaohan and He, Di and Zhang, Kang and Shang, Liang and Zhang, Xue and Cui, Liying},
doi = {10.1080/01616412.2022.2029292},
issn = {0161-6412},
journal = {https://doi.org/10.1080/01616412.2022.2029292},
keywords = {Amyotrophic lateral sclerosis,clinical factor,genetic factor,prognosis,survival analysis},
month = {feb},
pages = {1--8},
publisher = {Taylor & Francis},
title = {{Survival analysis of clinical and genetic factors in an amyotrophic lateral sclerosis cohort from China}},
url = {https://www.tandfonline.com/doi/abs/10.1080/01616412.2022.2029292},
year = {2022}
}
@article{Riviere1998,
abstract = {<h3>Background</h3><p>In an attempt to better understand and define the progression of amyotrophic lateral sclerosis (ALS), we developed a classification of 5 discrete health states that reflect patients' activities of daily living. These health states were used to determine whether patients with ALS who are treated with riluzole differed from those treated with placebo.</p><h3>Setting</h3><p>Clinics for patients with ALS.</p><h3>Design</h3><p>Placebo-controlled trial of riluzole treatment in 959 patients with ALS.</p><h3>Interventions</h3><p>Treatment with riluzole or placebo.</p><h3>Main Dependent Measures</h3><p>A Cox model was used to assess whether, from the initial randomization to the end of an 18-month follow-up, there was a difference in the times of transition into subsequent health states between patients treated with riluzole and those treated with placebo.</p><h3>Results</h3><p>Our analysis showed a significant difference in the time to transit between the riluzole and the placebo groups in less severely affected cases, ie, state 2 and state A (the milder states) of ALS.</p><h3>Conclusion</h3><p>Patients receiving riluzole remained in the milder health states longer (<i>P</i><.05).</p>},
author = {Riviere, Marc and Meininger, Vincent and Zeisser, Phillipe and Munsat, Theodore},
doi = {10.1001/ARCHNEUR.55.4.526},
file = {::},
isbn = {1993;235:283289},
issn = {0003-9942},
journal = {Archives of Neurology},
keywords = {amyotrophic lateral sclerosis,follow-up,riluzole},
month = {apr},
number = {4},
pages = {526--528},
pmid = {9561981},
publisher = {American Medical Association},
title = {{An Analysis of Extended Survival in Patients With Amyotrophic Lateral Sclerosis Treated With Riluzole}},
url = {https://jamanetwork.com/journals/jamaneurology/fullarticle/773695},
volume = {55},
year = {1998}
}
@article{Knibb2016a,
abstract = {Background Amyotrophic lateral sclerosis (ALS) is a progressive and usually fatal neurodegenerative disease. Survival from diagnosis varies considerably. Several prognostic factors are known, including site of onset (bulbar or limb), age at symptom onset, delay from onset to diagnosis and the use of riluzole and non-invasive ventilation (NIV). Clinicians and patients would benefit from a practical way of using these factors to provide an individualised prognosis.

Methods 575 consecutive patients with incident ALS from a population-based registry in South-East England register for ALS (SEALS) were studied. Their survival was modelled as a two-step process: the time from diagnosis to respiratory muscle involvement, followed by the time from respiratory involvement to death. The effects of predictor variables were assessed separately for each time interval.

Findings Younger age at symptom onset, longer delay from onset to diagnosis and riluzole use were associated with slower progression to respiratory involvement, and NIV use was associated with lower mortality after respiratory involvement, each with a clinically significant effect size. Riluzole may have a greater effect in younger patients and those with longer delay to diagnosis. A patient's survival time has a roughly 50% chance of falling between half and twice the predicted median.

Interpretation A simple and clinically applicable graphical method of predicting an individual patient's survival from diagnosis is presented. The model should be validated in an independent cohort, and extended to include other important prognostic factors.},
author = {Knibb, Jonathan A. and Keren, Noa and Kulka, Anna and Leigh, P. Nigel and Martin, Sarah and Shaw, Christopher E. and Tsuda, Miho and Al-Chalabi, Ammar},
doi = {10.1136/JNNP-2015-312908},
file = {::},
isbn = {3900051070},
issn = {0022-3050},
journal = {Journal of Neurology, Neurosurgery & Psychiatry},
month = {dec},
number = {12},
pages = {1361--1367},
pmid = {27378085},
publisher = {BMJ Publishing Group Ltd},
title = {{A clinical tool for predicting survival in ALS}},
url = {https://jnnp.bmj.com/content/87/12/1361 https://jnnp.bmj.com/content/87/12/1361.abstract},
volume = {87},
year = {2016}
}
@article{Ekman2017,
abstract = {In a regression setting with a number of measured covariates not all may be relevant to the response. By reducing the numbers of covariates included in the final model we could improve its prediction accurarcy as well as making it easier to interpret. In survival analysis, the study of time-to-event data, the most common form of regression is the semi-parametric Cox proportional hazard (PH) model. In this thesis we have compared three different ways to perform variable selection in the Cox PH model, stepwise regression, lasso and bootstrap. By simulating survival data we could control which covari- ates that were significant for the response. Fitting the Cox PH model to these data using the three different variable selection methods we could evaluate how well each method performs in finding the correct model. We found that while bootstrap in some cases could improve the stepwise approach its performance is strongly effected by the choice of inclusion frequency. Lasso performed equivalent or slightly better than the stepwise method for data with weak effects. However, when the data instead consists of strong effects, the performance of stepwise is considerably better than the performance of lasso.},
author = {Ekman, Anna},
file = {:Users/jferreira-admin/Downloads/FULLTEXT01.pdf:pdf},
pages = {50},
title = {{Variable selection for the Cox proportional hazards model: A simulation study comparing the stepwise, lasso and bootstrap approach}},
year = {2017}
}
@misc{survminer,
title = {{Survminer package}},
url = {https://shariq-mohammed.github.io/files/cbsa2019/1-intro-to-survival.html}
}
@article{Pupillo2014,
abstract = {Objective To determine the long-term survival in amyotrophic lateral sclerosis (ALS) and identify predictors of prolonged survival in a population-based cohort of newly diagnosed patients. Methods An incident cohort from a population-based registry during the years 1998 through 2002 in Lombardy, Italy was followed until death or to February 28, 2013. Age, sex, date of onset of symptoms, site of onset, date of diagnosis, and El Escorial diagnostic category were collected. Survival was assessed using Kaplan-Meier curves. Cox proportional hazards function was used to identify independent prognostic predictors. Standardized mortality ratios (SMRs) were used to assess the 5-year and 10-year excess mortality of ALS patients. Results Included were 280 men and 203 women aged 18 to 93 years. Spinal onset ALS was present in 312 cases (64.6%). Definite ALS was diagnosed in 213 cases (44.1%), probable ALS in 130 (26.9%), possible ALS in 93 (19.3%), and suspected ALS in 47 (9.7%). The cumulative time-dependent survival at 1, 5, and 10 years from diagnosis was 76.2%, 23.4%, and 11.8%, respectively. Independent predictors included younger age, the diagnosis of possible/suspected ALS, spinal onset, and symptoms having started >12 months previously at diagnosis. SMR was 9.4 at 5 years and 5.4 at 10 years. SMR at 10 years was higher until age 75 year, predominating in women, and became nonsignificant for males thereafter. Interpretation The outcome in ALS varies with phenotype. Longer survival is predicted by younger age, spinal onset, male gender, and suspected ALS. After age 75 years, 10-year survival in men with ALS is similar to the general population. {\textcopyright} 2014 American Neurological Association.},
author = {Pupillo, Elisabetta and Messina, Paolo and Logroscino, Giancarlo and Beghi, Ettore and Micheli, A. and Rosettani, P. and Baldini, D. and Bianchi, G. and Rigamonti, A. and Bonito, V. and Chiveri, L. and Guidotti, M. and Rezzonico, M. and Vidale, S. and Corbo, M. and Lunetta, C. and Maestri, E. and Cotelli, Maria Sofia and Filosto, M. and Filippini, G. and Lauria, G. and Mora, G. and Papetti, L. and Morelli, C. and Perini, M. and Tavernelli, F. and Perrone, P. and Guaita, M. C. and Testa, D. and Sasanelli, F. and Galbussera, A. and Tremolizzo, L. and Ferrarese, C. and Galli, A. and Vitelli, E. and Prelle, A. and Riva, N. and Ceroni, M. and Delodovici, L. and Clerici, M. and Bono, G. and Buzzi, P. and Previdi, P. and Guarneri, G. and Abruzzi, L. and Riccardi, T. and Lorusso, L. and Mazzini, L.},
doi = {10.1002/ANA.24096},
file = {:Users/jferreira-admin/Downloads/pupillo2014.pdf:pdf},
issn = {1531-8249},
journal = {Annals of neurology},
keywords = {80 and over,Adolescent,Adult,Age Factors,Aged,Amyotrophic Lateral Sclerosis / epidemiology*,Amyotrophic Lateral Sclerosis / mortality*,Author(firstnames='A',Author(firstnames='C',Author(firstnames='D',Author(firstnames='E',Author(firstnames='F',Author(firstnames='G',Author(firstnames='L',Author(firstnames='M C',Author(firstnames='M S',Author(firstnames='M',Author(firstnames='N',Author(firstnames='P',Author(firstnames='S',Author(firstnames='T',Author(firstnames='V',Cohort Studies,CollabAuthor(name='SLALOM Group',Community Health Planning,Elisabetta Pupillo,Female,Humans,Italy,MEDLINE,Male,Middle Aged,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Non-U.S. Gov't,Paolo Messina,Predictive Value of Tests,PubMed Abstract,Research Support,Sex Factors,Survival Analysis,Survivors / statistics & numerical data*,Young Adult,affs=[],affs=[]),doi:10.1002/ana.24096,equal_contrib=False),equal_contrib=False)],initials='A',initials='C',initials='D',initials='E',initials='F',initials='G',initials='L',initials='M',initials='MC',initials='MS',initials='N',initials='P',initials='S',initials='T',initials='V',investigators=[Author(firstnames='A',is_editor=False,lastname='Abruzzi',lastname='Baldini',lastname='Bianchi',lastname='Bonito',lastname='Bono',lastname='Buzzi',lastname='Ceroni',lastname='Chiveri',lastname='Clerici',lastname='Corbo',lastname='Cotelli',lastname='Delodovici',lastname='Ferrarese',lastname='Filippini',lastname='Filosto',lastname='Galbussera',lastname='Galli',lastname='Guaita',lastname='Guarneri',lastname='Guidotti',lastname='Lauria',lastname='Lorusso',lastname='Lunetta',lastname='Maestri',lastname='Mazzini',lastname='Micheli',lastname='Mora',lastname='Morelli',lastname='Papetti',lastname='Perini',lastname='Perrone',lastname='Prelle',lastname='Previdi',lastname='Rezzonico',lastname='Riccardi',lastname='Rigamonti',lastname='Riva',lastname='Rosettani',lastname='Sasanelli',lastname='Tavernelli',lastname='Testa',lastname='Tremolizzo',lastname='Vidale',lastname='Vitelli',pmid:24382602,suffix=None},
month = {feb},
number = {2},
pages = {287--297},
pmid = {24382602},
publisher = {Ann Neurol},
title = {{Long-term survival in amyotrophic lateral sclerosis: a population-based study}},
url = {https://pubmed.ncbi.nlm.nih.gov/24382602/},
volume = {75},
year = {2014}
}
@article{Olive1995,
abstract = {Finding ways for people and wildlife to coexist requires affording both parties access to critical resources and space, but also a behavioural change by both to avoid conflict. We investigated pathway use in a population of free-ranging African elephants Loxodonta africana in the Okavango Panhandle, Botswana that share their range with humans in a multi-use, heterogeneous landscape. We used detailed ground surveys to identify and map elephant movement pathways, and mixed-effect models to explore factors influencing elephant numbers and movement behaviour on and around these pathways. We found deviation in pathway use among the elephant population, suggesting behavioural adaptations to avoid human-associated risk: avoiding pathways near settlements, particularly near larger settlements; avoiding pathways close to cultivated land; and adopting a safety-in-numbers strategy when moving through areas of human use. Our findings suggest there is opportunity to capitalize on risk avoidance by elephant populations, to minimize resource-use overlap and reduce conflict between humans and elephants. We discuss a strategy that involves ensuring appropriate protection of elephant pathways in land-use planning, using development-free buffer zones, combined with mitigation techniques along the interface with agricultural lands to increase risk levels and reinforce human-elephant interface boundaries. We recommend further examination of the use of landscape-level mitigation techniques that encourage elephants to use pathways away from human activity and help define spatial boundaries for management of human-elephant conflict in multi-use landscapes. {\textcopyright} Fauna & Flora International 2015.},
author = {Olive, By and Dunn, Jean and Clark, Virginia A},
file = {:Users/jferreira-admin/Downloads/CancerEpi-12.pdf:pdf},
issn = {00306053},
journal = {Survival},
pages = {263--276},
title = {{Introduction To Survival}},
year = {1995}
}
@article{Pinto2019,
abstract = {Introduction: Respiratory function is a critical predictor of survival in amyotrophic lateral sclerosis (ALS). We aimed to determine if slow vital capacity (SVC) is a predictor of functional loss in ALS as compared to forced vital capacity (FVC). Methods: Consecutive ALS patients in whom respiratory tests were performed at baseline and 6 months later were included. All patients were evaluated with revised ALS functional rating scale (ALSFRS-R) and the respiratory tests, SVC, and FVC. Significant independent variables of functional decay were assessed by univariate Kaplan-Meier log-rank test and multivariate Cox proportional hazards model. A monthly decay not exceeding 0.92 in ALSFRS was considered as the time event. Results: We included 232 patients (134 men; mean onset-age 59.1 ± 11.23 years; mean disease duration from first symptoms to first visit: 14.5 ± 12.9 months; 166 spinal and 66 bulbar onset). All variables studied declined significantly between the two evaluations (p < 0.001). FVC and SVC were strongly correlated at study entry (r2 = 0.98, p < 0.001) and FVC and SVC decays between first evaluation and 6 months after were the only significant prognostic variables of functional decay (p < 0.001). Conclusion: FVC and SVC decay are inter-changeable in predicting functional decay in ALS. Pharmacological interventions reducing the decline rate of FVC and SVC can have a positive impact on the global functional impairment, with relevant implications for clinical trials' design and interpretation.},
author = {Pinto, Susana and {De Carvalho}, Mamede},
doi = {10.3389/fneur.2019.00109},
file = {:Users/jferreira-admin/Downloads/fneur-10-00109.pdf:pdf},
issn = {16642295},
journal = {Frontiers in Neurology},
keywords = {Amyotrophic lateral sclerosis,Functional outcome,Predictor,Rate of progression,Slow vital capacity},
number = {FEB},
pages = {1--5},
title = {{SVC is a marker of respiratory decline function, similar to FVC, in patients with ALS}},
volume = {10},
year = {2019}
}
@article{Model,
abstract = {Purpose: To establish PET as a tool for in-vivo quantification and monitoring of intramyocardially transplanted stem cells after labelling with FDG in mice with induced myocardial infarction. Methods: After inducing myocardial infarction in C57BL/6 mice, murine embryonic stem cells were labelled with FDG and transplanted into the border zone of the infarction. Dynamic PET scans were acquired from 25 to 120 min after transplantation, followed by a scan with 20 MBq FDG administered intravenously for anatomical landmarking. All images were reconstructed using the OSEM 3D and MAP reconstruction algorithms. FDG data were corrected for cellular tracer efflux and used as marker for cellular retention. FACS analysis of transplanted cells expressing enhanced green fluorescent protein was performed to validate the PET data. Results: We observed a rapid loss of cells from the site of transplantation, followed by stable retention over 120 min. Amounts of retention were 5.3 ± 1.1 % at 25 min, 5.0 ± 0.9 % at 60 min and 5.7 ± 1.2 % at 120 min. FACS analysis showed a high correlation without significant differences between the groups (P > 0.05). FDG labelling did not have any adverse effects on cell proliferation or differentiation. Conclusion: Up-to-date imaging is a powerful method for tracking and quantifying intramyocardially transplanted stem cells in vivo in the mouse model. This revealed a massive cell loss within minutes, and thereafter a relatively stable amount of about 5 % remaining cells was observed. Our method may become crucial for further optimization of cardiac cell therapy in the widely used mouse model of infarction. {\textcopyright} 2013 Springer-Verlag Berlin Heidelberg.},
author = {Model, The Additive},
file = {:Users/jferreira-admin/Downloads/Encyclopedia of Biostatistics2ndEd2005.pdf:pdf},
isbn = {9780470011812},
journal = {Survival},
number = {1},
pages = {1--6},
title = {{Aalen's Additive Regression Model}}
}
@article{Vach2020,
abstract = {The user of the Cox proportional hazard model may be assured that if the model is correct, then the the coefficient estimates will converge to the true values, and the confidence bands will accurately reflect sampling fluctuations around these true values. If the model is not correct, however, then the coefficients' estimates may not be correct, and the confidence bands may give a misleading picture. If the covariates are independent, and if one or more covariates is omitted, then the coefficients of the other covariates will be underestimated in absolute value. If there is dependence between missing and included covariates, then nothing can be said about the direction of the error. How can we check whether the model is correct? This remains the difficult point. We can test whether covariates are significantly different from zero. However, when covariates pass this test, it does not mean that they will be estimated correctly.},
author = {Vach, Werner},
doi = {10.1201/b12925-16},
file = {:Users/jferreira-admin/Downloads/Cooke_CPH_encycl.pdf:pdf},
journal = {Regression Models as a Tool in Medical Research},
keywords = {and phrases,censoring,competing risk,cox proportional hazard,model adequacy,omitted covariates,relative risk},
number = {2},
pages = {107--118},
title = {{The Cox Proportional Hazards Model}},
volume = {34},
year = {2020}
}
@article{Diez2013a,
abstract = {1. knowledgable about the basics of survival analysis, 2. familiar with vectors, matrices, data frames, lists, plotting, and linear models in R, and 3. interested in applying survival analysis in R. This guide emphasizes the survival package 1 in R 2 . Following very brief introductions to material, functions are introduced to apply the methods. A few short supplemental functions have been written and are available in the OIsurv package 3 , and data sets from the KMsurv package 4 are also used. This guide may be a particularly helpful supplement for Klein and Moeschberger's book, with which KMsurv is associated. Ideally, this survival analysis document would be printed front-to-back and bound like a book. No topics run over two pages. Those that are two pages start on an even page, preventing the need to flip between pages for a single topic. All sample code may be run provided the OIsurv package is loaded, which automatically loads the survival and KMsurv packages. Details about installing and loading the OIsurv package are described in the first section, which discusses R packages.},
author = {Diez, David M},
file = {:Users/jferreira-admin/Downloads/survival_analysis_in_R.pdf:pdf},
journal = {None},
number = {June},
pages = {1--16},
title = {{Survival Analysis in R}},
year = {2013}
}
@article{Respiratory2010,
abstract = {The proceedings contain 3712 papers. The topics discussed include: cardiopulmonary exercise test (CPET) may show initial right cardiac dysfunction in patients with moderate-severe chronic obstructive pulmonary disease (COPD); benefits of pulmonary rehabilitation on exercise desaturation in COPD patients; does muscle wasting always mean muscle weakness? a prevalence study in COPD; cross sectional analysis of COPD care by specialists and general practitioners - national survey; correlations between the German versions of the clinical COPD questionnaire (CCQ), COPD assessment test (CAT) and the St George respiratory questionnaire (SGRQ); does using the largest measured vital capacity in PFT intepretation help categorize patients out of the nonspecific pattern?; nonlinear dynamics of heart rate variability in patients with chronic obstructive pulmonary disease and changes after 4-week rehabilitation; and clinical and functional differences, exercise capacity and physical activity among frequent and not frequent exacerbators in COPD patients.},
author = {Respiratory, European and Annual, Society},
file = {:Users/jferreira-admin/Downloads/P3842.full.pdf:pdf},
journal = {Analysis},
keywords = {1,air pollution keyword 2,environment,inflammation keyword 3},
pages = {10--11},
title = {{Can NIV parameters settings and changes overtime predict functional and survival outcome in ALS patients?}},
volume = {3},
year = {2010}
}
@article{Turner2003,
author = {Turner, M R and Parton, M J and Shaw, C E and Leigh, N and Al-, A},
doi = {10.1136/jnnp.74.7.995},
file = {::},
journal = {J Neurol Neurosurg Psychiatry},
pages = {995--997},
title = {{Prolonged survival in motor neuron disease: a descriptive study of the King's database 1990-2002}},
url = {www.jnnp.com},
volume = {74},
year = {2003}
}
@article{Westeneng2018,
abstract = {Background: Amyotrophic lateral sclerosis (ALS) is a relentlessly progressive, fatal motor neuron disease with a variable natural history. There are no accurate models that predict the disease course and outcomes, which complicates risk assessment and counselling for individual patients, stratification of patients for trials, and timing of interventions. We therefore aimed to develop and validate a model for predicting a composite survival endpoint for individual patients with ALS. Methods: We obtained data for patients from 14 specialised ALS centres (each one designated as a cohort) in Belgium, France, the Netherlands, Germany, Ireland, Italy, Portugal, Switzerland, and the UK. All patients were diagnosed in the centres after excluding other diagnoses and classified according to revised El Escorial criteria. We assessed 16 patient characteristics as potential predictors of a composite survival outcome (time between onset of symptoms and non-invasive ventilation for more than 23 h per day, tracheostomy, or death) and applied backward elimination with bootstrapping in the largest population-based dataset for predictor selection. Data were gathered on the day of diagnosis or as soon as possible thereafter. Predictors that were selected in more than 70% of the bootstrap resamples were used to develop a multivariable Royston-Parmar model for predicting the composite survival outcome in individual patients. We assessed the generalisability of the model by estimating heterogeneity of predictive accuracy across external populations (ie, populations not used to develop the model) using internal–external cross-validation, and quantified the discrimination using the concordance (c) statistic (area under the receiver operator characteristic curve) and calibration using a calibration slope. Findings: Data were collected between Jan 1, 1992, and Sept 22, 2016 (the largest data-set included data from 1936 patients). The median follow-up time was 97{\textperiodcentered}5 months (IQR 52{\textperiodcentered}9–168{\textperiodcentered}5). Eight candidate predictors entered the prediction model: bulbar versus non-bulbar onset (univariable hazard ratio [HR] 1{\textperiodcentered}71, 95% CI 1{\textperiodcentered}63–1{\textperiodcentered}79), age at onset (1{\textperiodcentered}03, 1{\textperiodcentered}03–1{\textperiodcentered}03), definite versus probable or possible ALS (1{\textperiodcentered}47, 1{\textperiodcentered}39–1{\textperiodcentered}55), diagnostic delay (0{\textperiodcentered}52, 0{\textperiodcentered}51–0{\textperiodcentered}53), forced vital capacity (HR 0{\textperiodcentered}99, 0{\textperiodcentered}99–0{\textperiodcentered}99), progression rate (6{\textperiodcentered}33, 5{\textperiodcentered}92–6{\textperiodcentered}76), frontotemporal dementia (1{\textperiodcentered}34, 1{\textperiodcentered}20–1{\textperiodcentered}50), and presence of a C9orf72 repeat expansion (1{\textperiodcentered}45, 1{\textperiodcentered}31–1{\textperiodcentered}61), all p<0{\textperiodcentered}0001. The c statistic for external predictive accuracy of the model was 0{\textperiodcentered}78 (95% CI 0{\textperiodcentered}77–0{\textperiodcentered}80; 95% prediction interval [PI] 0{\textperiodcentered}74–0{\textperiodcentered}82) and the calibration slope was 1{\textperiodcentered}01 (95% CI 0{\textperiodcentered}95–1{\textperiodcentered}07; 95% PI 0{\textperiodcentered}83–1{\textperiodcentered}18). The model was used to define five groups with distinct median predicted (SE) and observed (SE) times in months from symptom onset to the composite survival outcome: very short 17{\textperiodcentered}7 (0{\textperiodcentered}20), 16{\textperiodcentered}5 (0{\textperiodcentered}23); short 25{\textperiodcentered}3 (0{\textperiodcentered}06), 25{\textperiodcentered}2 (0{\textperiodcentered}35); intermediate 32{\textperiodcentered}2 (0{\textperiodcentered}09), 32{\textperiodcentered}8 (0{\textperiodcentered}46); long 43{\textperiodcentered}7 (0{\textperiodcentered}21), 44{\textperiodcentered}6 (0{\textperiodcentered}74); and very long 91{\textperiodcentered}0 (1{\textperiodcentered}84), 85{\textperiodcentered}6 (1{\textperiodcentered}96). Interpretation: We have developed an externally validated model to predict survival without tracheostomy and non-invasive ventilation for more than 23 h per day in European patients with ALS. This model could be applied to individualised patient management, counselling, and future trial design, but to maximise the benefit and prevent harm it is intended to be used by medical doctors only. Funding: Netherlands ALS Foundation.},
author = {Westeneng, Henk Jan and Debray, Thomas P.A. and Visser, Anne E. and van Eijk, Ruben P.A. and Rooney, James P.K. and Calvo, Andrea and Martin, Sarah and McDermott, Christopher J. and Thompson, Alexander G. and Pinto, Susana and Kobeleva, Xenia and Rosenbohm, Angela and Stubendorff, Beatrice and Sommer, Helma and Middelkoop, Bas M. and Dekker, Annelot M. and van Vugt, Joke J.F.A. and van Rheenen, Wouter and Vajda, Alice and Heverin, Mark and Kazoka, Mbombe and Hollinger, Hannah and Gromicho, Marta and K{\"{o}}rner, Sonja and Ringer, Thomas M. and R{\"{o}}diger, Annekathrin and Gunkel, Anne and Shaw, Christopher E. and Bredenoord, Annelien L. and van Es, Michael A. and Corcia, Philippe and Couratier, Philippe and Weber, Markus and Grosskreutz, Julian and Ludolph, Albert C. and Petri, Susanne and de Carvalho, Mamede and {Van Damme}, Philip and Talbot, Kevin and Turner, Martin R. and Shaw, Pamela J. and Al-Chalabi, Ammar and Chi{\`{o}}, Adriano and Hardiman, Orla and Moons, Karel G.M. and Veldink, Jan H. and van den Berg, Leonard H.},
doi = {10.1016/S1474-4422(18)30089-9},
file = {:Users/jferreira-admin/Downloads/Prognosis for patients with amyotrophic lateral sclerosis - personalised prediction model (Lancet).pdf:pdf},
issn = {14744465},
journal = {The Lancet Neurology},
number = {5},
pages = {423--433},
pmid = {29598923},
title = {{Prognosis for patients with amyotrophic lateral sclerosis: development and validation of a personalised prediction model}},
volume = {17},
year = {2018}
}
@article{Table2015,
abstract = {This appendix formed part of the original submission and has been peer reviewed. We post it as supplied by the authors. Supplement to: Galsworthy MJ, Hristovski D, Lusa L, et al. Academic output of 9 years of EU investment into health research. Lancet 2012; 380: 971.},
author = {Table, Supp and Lowering, Pressure and Comparing, Trials and Intervention, Active and Cad, M I},
file = {:Users/jferreira-admin/Downloads/Supplementary Material.pdf:pdf},
number = {15},
pages = {1--24},
title = {{Supplementary appendix Supplementary Appendix :}},
volume = {6736},
year = {2015}
}
@article{Su2021,
abstract = {Background: The survival time of amyotrophic lateral sclerosis (ALS) is greatly variable and protective or risk effects of the potential survival predictors are controversial. Thus, we aim to undertake a comprehensive meta-analysis of studies investigating non-genetic prognostic and survival factors in patients with ALS. Methods: A search of relevant literature from PubMed, Embase, Cochrane library and other citations from 1st January 1966 to 1st December 020 was conducted. Random-effects models were conducted to pool the multivariable or adjusted hazard ratios (HR) by Stata MP 16.0. PROSPERO registration number: CRD42021256923. Findings: A total of 5717 reports were identified, with 115 studies meeting pre-designed inclusion criteria involving 55,169 ALS patients. Five dimensions, including demographic, environmental or lifestyle, clinical manifestations, biochemical index, therapeutic factors or comorbidities were investigated. Twenty-five prediction factors, including twenty non-intervenable and five intervenable factors, were associated with ALS survival. Among them, NFL (HR:3.70, 6.80, in serum and CSF, respectively), FTD (HR:2.98), ALSFRS-R change (HR:2.37), respiratory subtype (HR:2.20), executive dysfunction (HR:2.10) and age of onset (HR:1.03) were superior predictors for poor prognosis, but pLMN or pUMN (HR:0.32), baseline ALSFRS-R score (HR:0.95), duration (HR:0.96), diagnostic delay (HR:0.97) were superior predictors for a good prognosis. Our results did not support the involvement of gender, education level, diabetes, hypertension, NIV, gastrostomy, and statins in ALS survival. Interpretation: Our study provided a comprehensive and quantitative index for assessing the prognosis for ALS patients, and the identified non-intervenable or intervenable factors will facilitate the development of treatment strategies for ALS. Funding: This study was supported by the National Natural Science Fund of China (Grant No. 81971188), the 1.3.5 project for disciplines of excellence, West China Hospital, Sichuan University (Grant No. 2019HXFH046), and the Science and Technology Bureau Fund of Sichuan Province (No. 2019YFS0216).},
author = {Su, Wei Ming and Cheng, Yang Fan and Jiang, Zheng and Duan, Qing Qing and Yang, Tian Mi and Shang, Hui Fang and Chen, Yong Ping},
doi = {10.1016/j.ebiom.2021.103732},
file = {:Users/jferreira-admin/Downloads/PIIS2352396421005260.pdf:pdf},
issn = {23523964},
journal = {EBioMedicine},
keywords = {Amyotrophic lateral sclerosis,Hazard ratios,Outcome,Predictors,Survival},
pages = {103732},
publisher = {Elsevier B.V.},
title = {{Predictors of survival in patients with amyotrophic lateral sclerosis: A large meta-analysis}},
url = {https://doi.org/10.1016/j.ebiom.2021.103732},
volume = {74},
year = {2021}
}
@article{Hochreiter1997a,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
file = {:Users/jferreira-admin/Downloads/lstm.pdf:pdf},
issn = {08997667},
journal = {Neural Computation},
number = {8},
pages = {1735--1780},
pmid = {9377276},
title = {{Long Short-Term Memory}},
volume = {9},
year = {1997}
}
@article{Jr2013,
author = {Jr, DW Hosmer and Lemeshow, S and Sturdivant, RX},
file = {::},
title = {{Applied logistic regression}},
url = {https://books.google.com/books?hl=pt-PT&lr=&id=64JYAwAAQBAJ&oi=fnd&pg=PR13&ots=DtfL9WcrfK&sig=n-HNj2pJHVTOmGFokhuyXlYVIoo},
year = {2013}
}
@article{Goutman,
author = {Goutman, Stephen A},
file = {::},
title = {{Diagnosis and Clinical Management of Amyotrophic Lateral Sclerosis and Other Motor Neuron Disorders}}
}
@article{Goutmana,
author = {Goutman, Stephen A},
file = {:Users/jferreira-admin/Downloads/goutman2017(1).pdf:pdf;::},
title = {{Diagnosis and Clinical Management of Amyotrophic Lateral Sclerosis and Other Motor Neuron Disorders}}
}
@article{Li2017,
abstract = {file:///C:/Users/Marika/Desktop/universit{\`{a}} di pavia/articoli/strategie per l'infertilit{\`{a}}/nihms964087.pdf},
author = {Li, Kan and Chan, Wenyaw and Doody, Rachelle S. and Quinn, Joseph and Luo, Sheng and {Alzheimer's Disease Nueroimaging Initiative}},
doi = {10.3233/JAD-161201.Prediction},
file = {:Users/jferreira-admin/Downloads/nihms865707.pdf:pdf},
journal = {Physiology & behavior},
keywords = {endothelium,estrogen,estrogen receptors,vascular smooth muscle},
number = {3},
pages = {139--148},
title = {{Prediction of conversion to Alzheimer's disease with longitudinal measures and time-to-event data}},
volume = {176},
year = {2017}
}
@article{Donohue2014,
abstract = {Motivation-Diseases that progress slowly are often studied by observing cohorts at different stages of disease for short periods of time. The Alzheimer's Disease Neuroimaging Initiative (ADNI) follows elders with various degrees of cognitive impairment, from normal to impaired. The study includes a rich panel of novel cognitive tests, biomarkers, and brain images collected every six months for up to six years. The relative timing of the observations with respect to disease pathology is unknown. We propose a general semi-parametric model and iterative estimation procedure to simultaneously estimate pathologic timing and long-term growth curves. The resulting estimates of long-term progression are fine-tuned using cognitive trajectories derived from the long-term "Personnes Ag{\'{e}}es QUID" (PAQUID) study. Results-We demonstrate with simulations that the method can recover long-term disease trends from short-term observations. The method also estimates temporal ordering of individuals with respect to disease pathology, providing subject-specific prognostic estimates of the time until onset of symptoms. When the method is applied to ADNI data, the estimated growth curves are in general agreement with prevailing theories of the Alzheimer's disease cascade. Other datasets with common outcome measures can be combined using the proposed algorithm. Availability-Software to fit the model and reproduce results with the statistical software R is available as the grace package},
author = {Donohue, Michael C and Jacqmin-Gadda, Helene and {Le Goff}, M{\'{e}}lanie and Thomas, Ronald G and Raman, Rema and Gamst, Anthony C and Beckett, Laurel A and Jack, Clifford R and Weiner, Michael W and Dartigues, Jean-Francois and Aisen, Paul S},
doi = {10.1016/j.jalz.2013.10.003},
file = {::},
journal = {Alzheimers Dement},
keywords = {multiple outcomes,self modeling regression,semiparametric regression},
number = {0},
pages = {400--410},
title = {{Estimating long-term multivariate progression from short-term data}},
url = {http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ http://mdonohue.bitbucket.org/grace/ http://loni.usc.edu},
volume = {10},
year = {2014}
}
@article{Iddi2018,
abstract = {Parkinson's disease is the second most common neurological disease and affects about 1% of persons over the age of 60 years. Due to the lack of approved surrogate markers, confirmation of the disease still requires postmortem examination. Identifying and validating biomarkers are essential steps toward improving clinical diagnosis and accelerating the search for therapeutic drugs to ameliorate disease symptoms. Until recently, statistical analysis of multicohort longitudinal studies of neurodegenerative diseases has usually been restricted to a single analysis per outcome with simple comparisons between diagnostic groups. However, an important methodological consideration is to allow the modeling framework to handle multiple outcomes simultaneously and consider the transitions between diagnostic groups. This enables researchers to monitor multiple trajectories, correctly account for the correlation among biomarkers, and assess how these associations may jointly change over the long-term course of disease. In this study, we apply a latent time joint mixed-effects model to study biomarker progression and disease dynamics in the Parkinson's Progression Markers Initiative (PPMI) and examine which markers might be most informative in the earliest phases of disease. The results reveal that, even though diagnostic category was not included in the model, it seems to accurately reflect the temporal ordering of the disease state consistent with diagnosis categorization at baseline. In addition, results indicated that the specific binding ratio on striatum and the total Unified Parkinson's Disease Rating Scale (UPDRS) show high discriminability between disease stages. An extended latent time joint mixed-effects model with heterogeneous latent time variance also showed improvement in model fit in a simulation study and when applied to real data.},
author = {Iddi, Samuel and Li, Dan and Aisen, Paul S. and Rafii, Michael S. and Litvan, Irene and Thompson, Wesley K. and Donohue, Michael C.},
doi = {10.1159/000488780},
file = {::},
issn = {16602862},
journal = {Neuro-degenerative diseases},
keywords = {Biomarkers,Clinical diagnosis,Disease trajectories,Joint mixed-effects models,Latent time shift,Multicohort longitudinal data,Multilevel Bayesian models,Parkinson's disease},
month = {oct},
number = {4},
pages = {173},
pmid = {30089306},
publisher = {NIH Public Access},
title = {{Estimating the evolution of disease in the Parkinson's Progression Markers Initiative}},
url = {/pmc/articles/PMC6314496/ /pmc/articles/PMC6314496/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6314496/},
volume = {18},
year = {2018}
}
@article{Hatami2020,
author = {Hatami, Farhad and Perrakis, Konstantinos and Cooper-knock, Johnathan},
doi = {10.1093/biostatistics/output},
file = {:Users/jferreira-admin/Downloads/2020.11.10.20229302v1.full.pdf:pdf},
pages = {1--31},
title = {{Penalized longitudinal mixed models with latent group structure , with an application in neurodegenerative diseases}},
year = {2020}
}
@article{Bilgel2016,
abstract = {It is important to characterize the temporal trajectories of disease-related biomarkers in order to monitor progression and identify potential points of intervention. These are especially important for neurodegenerative diseases, as therapeutic intervention is most likely to be effective in the preclinical disease stages prior to significant neuronal damage. Neuroimaging allows for the measurement of structural, functional, and metabolic integrity of the brain at the level of voxels, whose volumes are on the order of mm3. These voxelwise measurements provide a rich collection of disease indicators. Longitudinal neuroimaging studies enable the analysis of changes in these voxelwise measures. However, commonly used longitudinal analysis approaches, such as linear mixed effects models, do not account for the fact that individuals enter a study at various disease stages and progress at different rates, and generally consider each voxelwise measure independently. We propose a multivariate nonlinear mixed effects model for estimating the trajectories of voxelwise neuroimaging biomarkers from longitudinal data that accounts for such differences across individuals. The method involves the prediction of a progression score for each visit based on a collective analysis of voxelwise biomarker data within an expectation-maximization framework that efficiently handles large amounts of measurements and variable number of visits per individual, and accounts for spatial correlations among voxels. This score allows individuals with similar progressions to be aligned and analyzed together, which enables the construction of a trajectory of brain changes as a function of an underlying progression or disease stage. We apply our method to studying cortical $\beta$-amyloid deposition, a hallmark of preclinical Alzheimer's disease, as measured using positron emission tomography. Results on 104 individuals with a total of 300 visits suggest that precuneus is the earliest cortical region to accumulate amyloid, closely followed by the cingulate and frontal cortices, then by the lateral parietal cortex. The extracted progression scores reveal a pattern similar to mean cortical distribution volume ratio (DVR), an index of global brain amyloid levels. The proposed method can be applied to other types of longitudinal imaging data, including metabolism, blood flow, tau, and structural imaging-derived measures, to extract individualized summary scores indicating disease progression and to provide voxelwise trajectories that can be compared between brain regions.},
archivePrefix = {arXiv},
arxivId = {arXiv:1604.00912v1},
author = {Bilgel, Murat and Prince, Jerry L. and Wong, Dean F. and Resnick, Susan M. and Jedynak, Bruno M.},
doi = {10.1016/j.neuroimage.2016.04.001},
eprint = {arXiv:1604.00912v1},
file = {:Users/jferreira-admin/Downloads/1604.00912.pdf:pdf},
issn = {10959572},
journal = {NeuroImage},
keywords = {Amyloid imaging,Longitudinal image analysis,Progression score},
pages = {658--670},
pmid = {27095307},
title = {{A multivariate nonlinear mixed effects model for longitudinal image analysis: Application to amyloid imaging}},
volume = {134},
year = {2016}
}
@article{Poeran2017,
abstract = {Background and Aims—Cardiovascular disease (CVD) is among the leading causes of morbidity and mortality worldwide. Traditional risk factors predict 75-80% of an individual's risk of incident CVD. However, the role of early life experiences in future disease risk is gaining attention. The Barker hypothesis proposes fetal origins of adult disease, with consistent evidence demonstrating the deleterious consequences of birth weight outside the normal range. In this study, we investigate the role of birth weight in CVD risk prediction. Methods and Results—The Women's Health Initiative (WHI) represents a large national cohort of post-menopausal women with 63 815 participants included in this analysis. Univariable proportional hazards regression analyses evaluated the association of 4 self-reported birth weight categories against 3 CVD outcome definitions, which included indicators of coronary heart disease, ischemic stroke, coronary revascularization, carotid artery disease and peripheral arterial disease. The role of birth weight was also evaluated for prediction of CVD events in the presence of traditional risk factors using 3 existing CVD risk prediction equations: one body mass index (BMI)-based and two laboratory-based models. Low birth weight (LBW) (< 6 lbs.) was significantly associated with all CVD outcome definitions in univariable analyses (HR=1.086, p=0.009). LBW was a significant covariate in the BMI-based model (HR=1.128, p<0.0001) but not in the lipid-based models. Conclusion—LBW (<6 lbs.) is independently associated with CVD outcomes in the WHI cohort. This finding supports the role of the prenatal and postnatal environment in contributing to the development of adult chronic disease.},
author = {Poeran},
doi = {10.1159/000488780.Estimating},
file = {:Users/jferreira-admin/Downloads/nihms-976353.pdf:pdf},
isbn = {0000000000000},
journal = {Physiology & behavior},
keywords = {adolescent,child,gender dysphoria,gender identity,transgender},
number = {12},
pages = {139--148},
title = {{Estimating the evolution of disease in the Parkinson's Progression Markers Initiative}},
volume = {176},
year = {2017}
}
@misc{cc,
annote = {continuous time recurrent NN and tensor factorization for MS disease progression

class mixed models


- have a "disability score"

-random forest with static features
-RNN},
file = {:Users/jferreira-admin/PhD/articles/Longitudinal modeling of MS patient trajectories improves predictions of disability progression _ Enhanced Reader.pdf:pdf},
title = {{Longitudinal modeling of MS patient trajectories improves predictions of disability progression _ Enhanced Reader.pdf}}
}
@article{Ramamoorthy2021,
author = {Ramamoorthy, Divya and Severson, Kristen and Ghosh, Soumya and Sachs, Karen and Als, Answer},
file = {:Users/jferreira-admin/Downloads/2021.05.13.21254848v1.full.pdf:pdf},
journal = {medArxiv},
keywords = {progression},
mendeley-tags = {progression},
title = {{Identifying Patterns of ALS Progression from Sparse Longitudinal Data}},
year = {2021}
}
@article{Tavazzi2020,
abstract = {Background: Clinical registers constitute an invaluable resource in the medical data-driven decision making context. Accurate machine learning and data mining approaches on these data can lead to faster diagnosis, definition of tailored interventions, and improved outcome prediction. A typical issue when implementing such approaches is the almost unavoidable presence of missing values in the collected data. In this work, we propose an imputation algorithm based on a mutual information-weighted k-nearest neighbours approach, able to handle the simultaneous presence of missing information in different types of variables. We developed and validated the method on a clinical register, constituted by the information collected over subsequent screening visits of a cohort of patients affected by amyotrophic lateral sclerosis. Methods: For each subject with missing data to be imputed, we create a feature vector constituted by the information collected over his/her first three months of visits. This vector is used as sample in a k-nearest neighbours procedure, in order to select, among the other patients, the ones with the most similar temporal evolution of the disease over time. An ad hoc similarity metric was implemented for the sample comparison, capable of handling the different nature of the data, the presence of multiple missing values and include the cross-information among features captured by the mutual information statistic. Results: We validated the proposed imputation method on an independent test set, comparing its performance with those of three state-of-the-art competitors, resulting in better performance. We further assessed the validity of our algorithm by comparing the performance of a survival classifier built on the data imputed with our method versus the one built on the data imputed with the best-performing competitor. Conclusions: Imputation of missing data is a crucial -and often mandatory- step when working with real-world datasets. The algorithm proposed in this work could effectively impute an amyotrophic lateral sclerosis clinical dataset, by handling the temporal and the mixed-type nature of the data and by exploiting the cross-information among features. We also showed how the imputation quality can affect a machine learning task.},
annote = {Our imputation method is based on the assumption that sub- jects with a similar disease progression over a short period of time share similar feature values and can therefore be cross-exploited to impute missing values.},
author = {Tavazzi, Erica and Daberdaku, Sebastian and Vasta, Rosario and Calvo, Andrea and Chi{\`{o}}, Adriano and {Di Camillo}, Barbara},
doi = {10.1186/s12911-020-01166-2},
file = {:Users/jferreira-admin/Downloads/s12911-020-01166-2.pdf:pdf},
isbn = {1291102001166},
issn = {14726947},
journal = {BMC Medical Informatics and Decision Making},
keywords = {Amyotrophic lateral sclerosis,Clinical datasets,Imputation,K-nearest neighbours,Missing data,Mutual information,Na{\"{i}}ve Bayes},
number = {Suppl 5},
pages = {1--23},
pmid = {32819346},
publisher = {BMC Medical Informatics and Decision Making},
title = {{Exploiting mutual information for the imputation of static and dynamic mixed-type clinical data with an adaptive k-nearest neighbours approach}},
url = {http://dx.doi.org/10.1186/s12911-020-01166-2},
volume = {20},
year = {2020}
}
@article{Carreiro2015,
abstract = {Amyotrophic Lateral Sclerosis (ALS) is a devastating disease and the most common neurodegenerative disorder of young adults. ALS patients present a rapidly progressive motor weakness. This usually leads to death in a few years by respiratory failure. The correct prediction of respiratory insufficiency is thus key for patient management. In this context, we propose an innovative approach for prognostic prediction based on patient snapshots and time windows. We first cluster temporally-related tests to obtain snapshots of the patient's condition at a given time (patient snapshots). Then we use the snapshots to predict the probability of an ALS patient to require assisted ventilation after k days from the time of clinical evaluation (time window). This probability is based on the patient's current condition, evaluated using clinical features, including functional impairment assessments and a complete set of respiratory tests. The prognostic models include three temporal windows allowing to perform short, medium and long term prognosis regarding progression to assisted ventilation. Experimental results show an area under the receiver operating characteristics curve (AUC) in the test set of approximately 79% for time windows of 90, 180 and 365. days. Creating patient snapshots using hierarchical clustering with constraints outperforms the state of the art, and the proposed prognostic model becomes the first non population-based approach for prognostic prediction in ALS. The results are promising and should enhance the current clinical practice, largely supported by non-standardized tests and clinicians' experience.},
author = {Carreiro, Andr{\'{e}} V. and Amaral, Pedro M.T. and Pinto, Susana and Tom{\'{a}}s, Pedro and de Carvalho, Mamede and Madeira, Sara C.},
doi = {10.1016/j.jbi.2015.09.021},
file = {::},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
keywords = {Amyotrophic Lateral Sclerosis,Disease progression,Patient snapshots,Prognostic model,Time windows},
month = {dec},
pages = {133--144},
pmid = {26455265},
publisher = {Academic Press Inc.},
title = {{Prognostic models based on patient snapshots and time windows: Predicting disease progression to assisted ventilation in Amyotrophic Lateral Sclerosis}},
url = {https://pubmed.ncbi.nlm.nih.gov/26455265/},
volume = {58},
year = {2015}
}
@article{VanderBurgh2017,
abstract = {Amyotrophic lateral sclerosis (ALS) is a progressive neuromuscular disease, with large variation in survival between patients. Currently, it remains rather difficult to predict survival based on clinical parameters alone. Here, we set out to use clinical characteristics in combination with MRI data to predict survival of ALS patients using deep learning, a machine learning technique highly effective in a broad range of big-data analyses. A group of 135 ALS patients was included from whom high-resolution diffusion-weighted and T1-weighted images were acquired at the first visit to the outpatient clinic. Next, each of the patients was monitored carefully and survival time to death was recorded. Patients were labeled as short, medium or long survivors, based on their recorded time to death as measured from the time of disease onset. In the deep learning procedure, the total group of 135 patients was split into a training set for deep learning (n = 83 patients), a validation set (n = 20) and an independent evaluation set (n = 32) to evaluate the performance of the obtained deep learning networks. Deep learning based on clinical characteristics predicted survival category correctly in 68.8% of the cases. Deep learning based on MRI predicted 62.5% correctly using structural connectivity and 62.5% using brain morphology data. Notably, when we combined the three sources of information, deep learning prediction accuracy increased to 84.4%. Taken together, our findings show the added value of MRI with respect to predicting survival in ALS, demonstrating the advantage of deep learning in disease prognostication.},
author = {van der Burgh, Hannelore K. and Schmidt, Ruben and Westeneng, Henk Jan and de Reus, Marcel A. and van den Berg, Leonard H. and van den Heuvel, Martijn P.},
doi = {10.1016/j.nicl.2016.10.008},
file = {::},
issn = {22131582},
journal = {NeuroImage: Clinical},
keywords = {Amyotrophic lateral sclerosis,Deep learning,Neural network,Prediction,Survival,White matter connectivity},
pages = {361--369},
pmid = {28070484},
publisher = {Elsevier Inc.},
title = {{Deep learning predictions of survival based on MRI in amyotrophic lateral sclerosis}},
url = {https://pubmed.ncbi.nlm.nih.gov/28070484/},
volume = {13},
year = {2017}
}
@article{Bourke2006,
abstract = {Background: Few patients with amyotrophic lateral sclerosis currently receive non-invasive ventilation (NIV), reflecting clinical uncertainty about the role of this intervention. We aimed to assess the effect of NIV on quality of life and survival in amyotrophic lateral sclerosis in a randomised controlled trial. Methods: 92 of 102 eligible patients participated. They were assessed every 2 months and randomly assigned to NIV (n=22) or standard care (n=19) when they developed either orthopnoea with maximum inspiratory pressure less than 60% of that predicted or symptomatic hypercapnia. Primary validated quality-of-life outcome measures were the short form 36 mental component summary (MCS) and the sleep apnoea quality-of-life index symptoms domain (sym). Both time maintained above 75% of baseline (TiMCS and Tisym) and mean improvement ($\mu$MCS and $\mu$sym) were measured. Findings: NIV improved T iMCS, Tisym, $\mu$MCS, $\mu$sym, and survival in all patients and in the subgroup with better bulbar function (n=20). This subgroup showed improvement in several measures of quality of life and a median survival benefit of 205 days (p=0{\textperiodcentered}006) with maintained quality of life for most of this period. NIV improved some quality-of-life indices in those with poor bulbar function, including $\mu$sym (p=0{\textperiodcentered}018), but conferred no survival benefit. Interpretation: In patients with amyotrophic lateral sclerosis without severe bulbar dysfunction, NIV improves survival with maintenance of, and improvement in, quality of life. The survival benefit from NIV in this group is much greater than that from currently available neuroprotective therapy. In patients with severe bulbar impairment, NIV improves sleep-related symptoms, but is unlikely to confer a large survival advantage.},
author = {Bourke, Stephen C. and Tomlinson, Mark and Williams, Tim L. and Bullock, Robert E. and Shaw, Pamela J. and Gibson, G. John},
doi = {10.1016/S1474-4422(05)70326-4},
issn = {14744422},
journal = {Lancet Neurology},
keywords = {Aged,Amyotrophic Lateral Sclerosis / therapy*,Female,G John Gibson,Humans,MEDLINE,Male,Mark Tomlinson,Middle Aged,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Non-U.S. Gov't,Positive-Pressure Respiration*,PubMed Abstract,Quality of Life,Randomized Controlled Trial,Research Support,Respiratory Insufficiency / etiology,Respiratory Insufficiency / therapy,Sleep Apnea Syndromes / etiology,Sleep Apnea Syndromes / therapy,Stephen C Bourke,Survival,Treatment Outcome,doi:10.1016/S1474-4422(05)70326-4,pmid:16426990},
month = {feb},
number = {2},
pages = {140--147},
pmid = {16426990},
publisher = {Lancet Neurol},
title = {{Effects of non-invasive ventilation on survival and quality of life in patients with amyotrophic lateral sclerosis: A randomised controlled trial}},
url = {https://pubmed.ncbi.nlm.nih.gov/16426990/},
volume = {5},
year = {2006}
}
@article{Conde2019,
abstract = {Background: Amyotrophic lateral sclerosis (ALS) is a rare and progressive neurodegenerative disease involving the upper and lower motor neurons. It is also the most common and the one with the worst prognosis among the motor neuron diseases (MND). ALS invariably progresses to respiratory failure, which is an essential factor affecting the prognosis of this disease. Its prevalence in the world is heterogeneous and, in many countries, is even unknown, since national registries are not mandatory or comprehensive enough. Worldwide, the ALS/MND prevalence is estimated between 4 and 8 cases per 100,000 inhabitants, but in Portugal the prevalence was never studied. Because ALS and MND are rare diseases, population-based studies are very difficult to perform. In Portugal, there are no systematic patient registries. Objective: We aimed to obtain the best available indirect estimates of ALS/MND prevalence, using a pharmaco-epidemiological approach. Method: We developed a Bayesian multiparameter evidence synthesis model based on nationwide data of riluzole consumption, a drug highly specific for ALS/MND, combined with data from a nationwide hospital administrative database, data from the national institute of statistics, and data from other scientific articles focused on ALS/MND epidemiology, to estimate ALS/MND prevalence in Portugal. Results: We found an estimated ALS/MND prevalence in Portugal steadily increasing from 6.74 per 100,000 inhabitants (Bayesian 95% Credible Interval [95% CI] 5.39-9.37) in 2009 to 10.32 (95% CI 8.27-14.27) in 2016. In 2016, the estimated ALS/MND prevalence was higher in men, 12.08 per 100,000 (9.66-17.15), than in women, 8.56 (6.84-12.32). Regarding age groups, the estimated prevalence per 100,000 inhabitants were, in 2016 for women, 1.19 (0.78-1.85) for the <50 years' group, 8.48 (6.00-12.76) for the 51-60 group, 23.47 (18.05-33.88) for the 61-70 group, 28.77 (22.02-41.31) for the 71-80 group, and 14.45 (9.97-21.63) for the >80 group. For men, the prevalence estimates were 1.90 (1.32-2.84), 12.89 (9.44-19.16), 32.18 (24.91-45.74), 48.85 (38.72-71.40), and 31.27 (21.73-46.41), respectively, for each age group. We also observed a relevant variability across the country, with prevalence estimates, in 2016, of 9.31 cases per 100,000 inhabitants (7.45-12.86) in the Northern region of Portugal, 11.15 (8.9-15.34) in the Centre region, 10.74 (8.6-14.82) in Lisbon and Alentejo regions, and 5.55 (4.35-7.83) in the Algarve region. Conclusion: Overall, and even though we must account for the limitations of the indirect methods and models used for prevalence estimation, we probably have a very high ALS/MND prevalence in Portugal. It would be important to create registries, particularly in rare diseases, for better organization and distribution of healthcare services and resources, particularly at the level of ventilatory support.},
author = {Conde, Bebiana and Winck, Jo{\~{a}}o Carlos and Azevedo, Lu{\'{i}}s Filipe},
doi = {10.1159/000499485},
issn = {14230208},
journal = {Neuroepidemiology},
keywords = {Death certificates,Epidemiology,Incidence,Mortality,Parkinson's disease},
month = {aug},
number = {1-2},
pages = {73--83},
pmid = {31117082},
publisher = {S. Karger AG},
title = {{Estimating Amyotrophic Lateral Sclerosis and Motor Neuron Disease Prevalence in Portugal Using a Pharmaco-Epidemiological Approach and a Bayesian Multiparameter Evidence Synthesis Model}},
url = {https://pubmed.ncbi.nlm.nih.gov/31117082/},
volume = {53},
year = {2019}
}
@misc{Heffernan2006,
abstract = {This systematic review comprises an objective appraisal of the evidence in regard to the management of respiration in patients with motor neuron disease (MND/ALS). Studies were identified through computerised searches of 32 databases. Internet searches of websites of drug companies and MND/ ALS research web sites, 'snow balling' and hand searches were also employed to locate any unpublished study or other 'grey literature' on respiration and MND/ALS. Since management of MND/ALS involves a number of health professionals and care workers, searches were made across multiple disciplines. No time frame was imposed on the search in order to increase the probability of identifying all relevant studies, although there was a final limit of March 2005. Recommendations for patient and carer-based guidelines for the clinical management of respiration for MND/ALS patients are suggested on the basis of qualitative analyses of the available evidence. However, these recommendations are based on current evidence of best practice, which largely comprises observational research and clinical opinion. There is a clear need for further evidence, in particular randomised and non-randomised controlled trials on the effects of non-invasive ventilation and additional larger scale cohort studies on the issues of initial assessment of respiratory symptoms, and management and timing of interventions. {\textcopyright} 2006 Taylor & Francis.},
author = {Heffernan, Catherine and Jenkinson, Crispin and Holmes, Tricia and Macleod, Heidi and Kinnear, William and Oliver, David and Leigh, Nigel and Ampong, Mary Ann},
booktitle = {Amyotrophic Lateral Sclerosis},
doi = {10.1080/14660820510043235},
issn = {17482968},
keywords = {Management,Respiration,Respiratory insufficiency,Systematic review},
month = {mar},
number = {1},
pages = {5--15},
pmid = {16546753},
publisher = {Amyotroph Lateral Scler},
title = {{Management of respiration in MND/ALS patients: An evidence based review}},
url = {https://pubmed.ncbi.nlm.nih.gov/16546753/},
volume = {7},
year = {2006}
}
@article{Xu2021,
abstract = {Background: Increasing prognostic models for amyotrophic lateral sclerosis (ALS) have been developed. However, no comprehensive evaluation of these models has been done. The purpose of this study was to map the prognostic models for ALS to assess their potential contribution and suggest future improvements on modeling strategy. Methods: Databases including Medline, Embase, Web of Science, and Cochrane library were searched from inception to 20 February 2021. All studies developing and/or validating prognostic models for ALS were selected. Information regarding modelling method and methodological quality was extracted. Results: A total of 28 studies describing the development of 34 models and the external validation of 19 models were included. The outcomes concerned were ALS progression (n = 12; 35%), change in weight (n = 1; 3%), respiratory insufficiency (n = 2; 6%), and survival (n = 19; 56%). Among the models predicting ALS progression or survival, the most frequently used predictors were age, ALS Functional Rating Scale/ALS Functional Rating Scale-Revised, site of onset, and disease duration. The modelling method adopted most was machine learning (n = 16; 47%). Most of the models (n = 25; 74%) were not presented. Discrimination and calibration were assessed in 12 (35%) and 2 (6%) models, respectively. Only one model by Westeneng et al. (Lancet Neurol 17:423–433, 2018) was assessed with overall low risk of bias and it performed well in both discrimination and calibration, suggesting a relatively reliable model for practice. Conclusions: This study systematically reviewed the prognostic models for ALS. Their usefulness is questionable due to several methodological pitfalls and the lack of external validation done by fully independent researchers. Future research should pay more attention to the addition of novel promising predictors, external validation, and head-to-head comparisons of existing models.},
author = {Xu, Lu and He, Bingjie and Zhang, Yunjing and Chen, Lu and Fan, Dongsheng and Zhan, Siyan and Wang, Shengfeng},
doi = {10.1007/s00415-021-10508-7},
file = {:Users/jferreira-admin/PhD/articles/Xu2021_Article_PrognosticModelsForAmyotrophic.pdf:pdf},
isbn = {0123456789},
issn = {14321459},
journal = {Journal of Neurology},
keywords = {Amyotrophic lateral sclerosis,Motor neuron disease,Neurodegenerative diseases,Prognostic model,Systematic review},
number = {0123456789},
title = {{Prognostic models for amyotrophic lateral sclerosis: a systematic review}},
year = {2021}
}
@article{Zandona2017,
abstract = {Background. Amyotrophic lateral sclerosis (ALS) is a progressive neurodegenerative disease primarily affecting upper and lower motor neurons in the brain and spinal cord. The heterogeneity in the course of ALS clinical progression and ultimately survival, coupled with the rarity of this disease, make predicting disease outcome at the level of the individual patient very challenging. Besides, stratification of ALS patients has been known for years as a question of great importance to clinical practice, research and drug development. Methods. In this work, we present a Dynamic Bayesian Network (DBN) model of ALS progression to detect probabilistic relationships among variables included in the Pooled Resource Open-Access ALS Clinical Trials Database (PRO-ACT), which provides records of over 10,700 patients from different clinical trials, and with over 2,869,973 longitudinally collected data measurements. Results. Our model unravels new dependencies among clinical variables in relation to ALS progression, such as the influence of basophil count and creatine kinase on patients' clinical status and the respiratory functional state, respectively. Furthermore, it provided an indication of ALS temporal evolution, in terms of the most probable disease trajectories across time at the level of both patient population and individual patient. Conclusions. The risk factors identified by out DBN model could allow patients' stratification based on velocity of disease progression and a sensitivity analysis on this latter in response to changes in input variables, i.e. variables measured at diagnosis.},
author = {Zandon{\`{a}}, Alessandro and Francescon, Matilde and Bronfeld, Maya and Calvo, Andrea and Chi{\`{o}}, Adriano and di Camillo, Barbara},
doi = {10.7287/peerj.preprints.3262v1},
file = {:Users/jferreira-admin/PhD/articles/s12859-019-2692-x.pdf:pdf},
issn = {21678359},
journal = {PeerJ},
keywords = {amyotrophic lateral sclerosis,dynamic bayesian network,mitos,prediction,simulation,survival},
number = {Suppl 4},
pages = {1--11},
title = {{A dynamic Bayesian network model for simulation of disease progression in amyotrophic lateral sclerosis patients}},
volume = {5},
year = {2017}
}
@article{Zhang2019,
abstract = {Sporadic amyotrophic lateral sclerosis (SALS) is a devastating
neurodegenerative disorder. However, the understanding of SALS is still
poor. This research aimed to excavate attractor modules for SALS by
integrating the systemic module inference and attract method. To achieve
this, gene expression data and protein-protein data were recruited and
preprocessed. Then, based on the Spearman's correlation coefficient
(SCC) of the interactions under these two conditions, two PPI networks
separately with 870 nodes (979 interactions) in normal control group and
601 nodes (777 interactions) in SALS group were built. Systemic module
inference method was performed to identify the modules, and attract
method was used to identify attractor modules. Finally, pathway
enrichment analysis was performed to disclose the functional enrichment
of these attractor modules. In total 44 and 118 modules were identified
for normal control and SALS groups, respectively. Among them, 6 modules
were with similar gene composition between the two groups, and all 6
modules were considered as the attractor module via attract method.
These attractor modules might be potential biomarkers for early
diagnosis and therapy of SALS, which could provide insight into the
disease biology and suggest possible directions for drug screening
programs.},
author = {Zhang, Fang and Liu, Mei and Li, Qun and Song, Fei-Xue},
doi = {10.3892/etm.2019.7264},
file = {:Users/jferreira-admin/PhD/articles/b626f88b36fe1d9f48dd72bd2b3a20017fe9.pdf:pdf},
issn = {1792-0981},
journal = {Experimental and Therapeutic Medicine},
pages = {2575--2580},
title = {{Exploration of attractor modules for sporadic amyotrophic lateral sclerosis via systemic module inference and attract method}},
year = {2019}
}
@article{Ribeiro2019,
abstract = {Populational studies of human ageing often generate longitudinal datasets with high dimensionality. In order to discover knowledge in such datasets, the traditional knowledge discovery in database task needs to be adapted. In this article, we present a full knowledge discovery process that was performed on a longitudinal dataset, mentioning the singularities of this process. We investigated the English Longitudinal Study of Ageing's (ELSA's) database, employing both semi-supervised and supervised learning techniques to determine and describe the profiles of individuals annotated with the class labels “short-lived” and “long-lived” who participated in the study. We report on the data preprocessing, the clustering task of finding the best sets of representatives of the profiles of each class, and the use of supervised learning to describe these profiles and perform a longitudinal classification on the dataset to investigate how consistently the unlabelled records would fit into the classes. The results show that several aspects are used to discriminate the individuals between the longevity profiles. Those aspects include economic, social and health-related attributes. The findings have pointed towards a need to further investigate the relationships between the different aspects, especially those related to physical health and wellbeing, and how they affect the lifespan of an individual. Furthermore, our methodology and the adopted procedures can be applied to any other data mining applications for longitudinal studies of ageing.},
author = {Ribeiro, Caio Eduardo and Z{\'{a}}rate, Luis Enrique},
doi = {10.1016/j.eswa.2018.09.035},
file = {:Users/jferreira-admin/PhD/articles/Daneshyari.com_11021184.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Ageing studies,Cluster analysis,Longitudinal data,Machine learning},
pages = {75--89},
title = {{Classifying longevity profiles through longitudinal data mining}},
volume = {117},
year = {2019}
}
@article{Area2010,
abstract = {In this project we propose to study the chronic effects of cocaine-opioid combinations, a form of poly-abuse that, according to recent data from the European Monitoring Center on Drugs and Drug Addiction (1), is becoming very common among drug addicts and has severe implications on their chances of rehabilitation, thus having a significant effect on economy and public health. As opposed to what might be expected by drug addicts, cocaine-heroin combinations do not result in mutual neutralization of toxic effects, but in the exacerbation of a state of behavioral excitement and the diminishing of insight (2). In a previous successfully evaluated Project managed by the present PI and involving several members of this team (POCI/SAU-FCF/58330 /2004), cocaine and morphine were shown to interact at the molecular level, giving rise to a new chemical entity, a cocaine- morphine adduct (3). Preliminary results show that cocaine-heroin combinations (1:1) are more neurotoxic than either drug alone, and the mixture in which adducts are formed induces necrotic neuronal death, as opposed to the sequential drug combination, which does not lead to adduct formation (see attached preliminary data, Fig. 4). However, data on the neuronal effects of cocaine-opioid combinations are still scarce, not allowing determining unequivocally the role of cocaine-morphine adducts in the activity of these combinations. Therefore, in the present project, we propose to further characterize cocaine-morphine interactions (task 1) and we hypothesize that the neuronal effects of chronic cocaine-morphine combinations are mediated by cell-surface proteins (receptors and/or transporters) and/or dependent on drug(s) internalization by the neurons. These hypotheses will be tested both in vitro (task 2) and in vivo (task 3). We also hypothesize that chronic effects of cocaine-morphine combinations comprise unique in vivo consequences, namely changes in the dynamics of neurotransmitters in the cortex or in the striatum (task 4), as well as alterations in learning and memory associated with an inhibition of adult neurogenesis in the dentate gyrus of the hippocampus (task 5). These hypotheses will be tested in vivo, in Wistar rats chronically exposed to cocaine, morphine or their combinations, via subcutaneously implanted minipumps, which provide a model for chronic drug exposure and allow performing tests in animals for long periods of time, avoiding repeated stress-inducing injections and withdrawal periods that could affect the results. We will use morphine instead of heroin because it is its main active principle (4, for review), the strongest interaction previously detected in cocaine-heroin mixtures occurred between cocaine and morphine(3), and because heroin has a very short half-life in solution, as opposed to morphine (5), thus being relevant as a simulation of chronic heroin exposure. It is also hypothesized that the drug combination containing adducts has unique chronic effects, compared either to the isolated drugs or to drug combinations without adduct formation. The elements of the team have a recognized expertise in the fields of vibrational spectroscopy (e.g. Raman(3)) coupled to quantum mechanical theoretical methods, in vitro evaluation of acute cocaine(6) and heroin(7) neurotoxicity, in vitro studies of chronic exposure to cocaine(8) and heroin(9) in dopaminergic cell cultures, in vivo studies of psychostimulant(10) and opioid- psychostimulant combinations(11) using microdialysis, isolation and functional assessments in rat brain synaptosomes(12) and in the investigation of hippocampal neurogenesis(13). This constitutes a warranty of success for the presently proposed research work, providing the scientific and experimental skills needed to fully achieve the objectives envisaged. The results yielded by this project will allow attaining a better understanding of the changes occurring during combined opioid and cocaine exposure, which is an emergent phenomenon that greatly contributes to the lack of success of drug addiction treatments. The role of cocaine-opioid adducts in the neuronal effects of cocaine-morphine combinations will be defined and new data on chronic morphine or cocaine exposures is foreseen, since the free drugs will be used as controls in all experimental procedures. Finally, the knowledge obtained in this project may hopefully contribute to establish improved treatment strategies addressed to a wide range of drug addicts.},
author = {Area, Main},
file = {:Users/jferreira-admin/PhD/articles/PTDCCCI-CIF46132020 (Lacrado a 27-04-2020 a{\`{I}}s 20-59).pdf:pdf},
isbn = {6478025755900},
journal = {Review Literature And Arts Of The Americas},
keywords = {aqu{\'{a}}ticos},
pages = {1--17},
title = {{Concursos de Projectos de I & D Calls for R & D Projects Vis{\~{a}}o global da candidatura}},
volume = {2},
year = {2010}
}
@article{Silva2020,
abstract = {Objectives: To assess the prognostic value of phrenic nerve conduction (PNC) in amyotrophic lateral sclerosis (ALS). Methods: We conducted a systematic review to identify studies reporting on PNC, and mortality and/or forced vital capacity (FVC) in patients with ALS. We searched Medline, EMBASE, and Web of Science. Two independent authors selected studies and extracted data. Risk of bias was assessed using the QUIPS tool. Hazard-ratios and correlation coefficients were pooled using a random effects generic inverse-variance model. Evidence quality was evaluated with GRADE. Results: In the pooled analysis, patients with CMAP-amplitude equal or below 0.4 mV are 2.021 more likely to die over the studied period (95%CI 1.161–3.522; I2 = 55.9%; 338 participants). CMAP-amplitude showed a moderate positive correlation with FVC (r = 0.400, 95%CI = 0.226–0.550; I2 = 69.77%; 381 participants). However, there was a weak negative correlation between CMAP-latency and FVC (r = −0.235; 95%CI = −0.447 to −0.024; I2 = 15.92%; 112 participants). Conclusions: There is moderate-quality evidence that CMAP-amplitude of the PNC is correlated with FVC. Results favour a predictive value for mortality, but the risk of bias is high. Significance: PNC is a simple test that should be considered to assess respiratory function in ALS, especially in patients with bulbar involvement or cognitive impairment.},
annote = {There is moderate-quality evidence that CMAP-amplitude of the PNC is correlated with FVC. Results favour a predictive value for mortality, but the risk of bias is high.},
author = {Silva, Cl{\'{a}}udia S. and Rodrigues, Filipe B. and Duarte, Gon{\c{c}}alo S. and Costa, Jo{\~{a}}o and de Carvalho, Mamede},
doi = {10.1016/j.clinph.2019.10.016},
file = {:Users/jferreira-admin/PhD/articles/1-s2.0-S1388245719312787-main.pdf:pdf},
issn = {18728952},
journal = {Clinical Neurophysiology},
keywords = {Amyotrophic lateral sclerosis,Meta-analysis,Phrenic nerve motor response,Survival,Systematic review},
number = {1},
pages = {106--113},
pmid = {31760209},
title = {{Prognostic value of phrenic nerve conduction study in amyotrophic lateral sclerosis: Systematic review and meta-analysis}},
volume = {131},
year = {2020}
}
@article{Aidos2017,
abstract = {Alzheimer's Disease (AD) is a neurological disorder that leads to a loss of cognitive functioning, affecting older people as well as their families. Although a few treatments are available to slow down the progress of the disease, they are limited in effectiveness and should start at an early stage of the disease. Since an early diagnosis of AD is crucial, to maximize treatment effectiveness and prepare the families for the worsening of symptoms, researchers are studying biomarkers and Computer-aided diagnosis (CAD) systems. Hence, this manuscript proposes a new methodology to obtain an efficient CAD system by relying on [18F]-Fluorodeoxyglucose Positron Emission Tomography (FDG-PET) scans, while taking into account the longitudinal information of a subject. The CAD system tries to identify regions of interest by simultaneously segmenting all the FDG-PET scans acquired over time for each subject and combining the segmentation result to find the most coherent information for all the subjects. Experimental results show that the proposed CAD system outperforms a state-of-the-art approach, either when only relying on baseline scans or in the follow-up classification, achieving, for instance, more than 82.0% accuracy in the discrimination between AD and Mild Cognitive Impairment (MCI). Finally, in a multi-class classification task, the proposed CAD system attains 59.0% accuracy at baseline and goes up to 69.4% in the follow-up.},
author = {Aidos, Helena and Fred, Ana},
doi = {10.1007/s10618-017-0502-5},
file = {:Users/jferreira-admin/PhD/articles/Aidos-Fred2017_Article_DiscriminationOfAlzheimerSDise.pdf:pdf},
isbn = {1061801705},
issn = {1573756X},
journal = {Data Mining and Knowledge Discovery},
keywords = {Alzheimer's disease,CAD system,FDG-PET scans,Regions of interest,Segmentation of temporal images},
number = {4},
pages = {1006--1030},
publisher = {Springer US},
title = {{Discrimination of Alzheimer's Disease using longitudinal information}},
volume = {31},
year = {2017}
}
@article{Henriques2018,
abstract = {Three-dimensional data are increasingly prevalent across biomedical and social domains. Notable examples are gene-sample-time, individual-feature-time, or node-node-time data, generally referred to as observationattribute- context data. The unsupervised analysis of three-dimensional data can be pursued to discover putative biological modules, disease progression profiles, and communities of individuals with coherent behavior, among other patterns of interest. It is thus key to enhance the understanding of complex biological, individual, and societal systems. In this context, although clustering can be applied to group observations, its relevance is limited since observations in three-dimensional data domains are typically only meaningfully correlated on subspaces of the overall space. Biclustering tackles this challenge but disregards the third dimension. In this scenario, triclustering-the discovery of coherent subspaces within three-dimensional data-has been largely researched to tackle these problems. Despite the diversity of contributions in this field, there still lacks a structured view on the major requirements of triclustering, desirable forms of homogeneity (including coherency, structure, quality, locality, and orthonormality criteria), and algorithmic approaches. This work formalizes the triclustering task and its scope, introduces a taxonomy to categorize the contributions in the field, provides a comprehensive comparison of state-of-the-art triclustering algorithms according to their behavior and output, and lists relevant real-world applications. Finally, it highlights challenges and opportunities to advance the field of triclustering and its applicability to complex three-dimensional data analysis.},
author = {Henriques, Rui and Madeira, Sara C.},
doi = {10.1145/3195833},
file = {:Users/jferreira-admin/PhD/articles/3195833.pdf:pdf},
issn = {15577341},
journal = {ACM Computing Surveys},
keywords = {Multivariate time series analysis multidimensional,Subspace clustering,Three-dimensional data analysis,Triclustering},
number = {5},
title = {{Triclustering algorithms for three-dimensional data analysis: A comprehensive survey}},
volume = {51},
year = {2018}
}
@article{Carreiro2015a,
abstract = {Amyotrophic Lateral Sclerosis (ALS) is a devastating disease and the most common neurodegenerative disorder of young adults. ALS patients present a rapidly progressive motor weakness. This usually leads to death in a few years by respiratory failure. The correct prediction of respiratory insufficiency is thus key for patient management. In this context, we propose an innovative approach for prognostic prediction based on patient snapshots and time windows. We first cluster temporally-related tests to obtain snapshots of the patient's condition at a given time (patient snapshots). Then we use the snapshots to predict the probability of an ALS patient to require assisted ventilation after k days from the time of clinical evaluation (time window). This probability is based on the patient's current condition, evaluated using clinical features, including functional impairment assessments and a complete set of respiratory tests. The prognostic models include three temporal windows allowing to perform short, medium and long term prognosis regarding progression to assisted ventilation. Experimental results show an area under the receiver operating characteristics curve (AUC) in the test set of approximately 79% for time windows of 90, 180 and 365. days. Creating patient snapshots using hierarchical clustering with constraints outperforms the state of the art, and the proposed prognostic model becomes the first non population-based approach for prognostic prediction in ALS. The results are promising and should enhance the current clinical practice, largely supported by non-standardized tests and clinicians' experience.},
author = {Carreiro, Andr{\'{e}} V. and Amaral, Pedro M.T. and Pinto, Susana and Tom{\'{a}}s, Pedro and de Carvalho, Mamede and Madeira, Sara C.},
doi = {10.1016/j.jbi.2015.09.021},
file = {:Users/jferreira-admin/PhD/articles/1-s2.0-S153204641500221X-main.pdf:pdf},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
keywords = {Amyotrophic Lateral Sclerosis,Disease progression,Patient snapshots,Prognostic model,Time windows},
pages = {133--144},
pmid = {26455265},
title = {{Prognostic models based on patient snapshots and time windows: Predicting disease progression to assisted ventilation in Amyotrophic Lateral Sclerosis}},
volume = {58},
year = {2015}
}
@misc{Garcia-Gancedo2019,
abstract = {Background: Objective symptom monitoring of patients with Amyotrophic Lateral Sclerosis (ALS) has the potential to provide an important source of information to evaluate the impact of the disease on aspects of real-world functional capacity and activities of daily living in the home setting, providing useful objective outcome measures for clinical trials. Objective: This study aimed to investigate the feasibility of a novel digital platform for remote data collection of multiple symptoms-physical activity, heart rate variability (HRV), and digital speech characteristics-in 25 patients with ALS in an observational clinical trial setting to explore the impact of the devices on patients' everyday life and to record tolerability related to the devices and study procedures over 48 weeks. Methods: In this exploratory, noncontrolled, nondrug study, patients attended a clinical site visit every 3 months to perform activity reference tasks while wearing a sensor, to conduct digital speech tests and for conventional ALS monitoring. In addition, patients wore the sensor in their daily life for approximately 3 days every month for the duration of the study. Results: The amount and quality of digital speech data captured at the clinical sites were as intended, and there were no significant issues. All the home monitoring sensor data available were propagated through the system and were received as expected. However, the amount and quality of physical activity home monitoring data were lower than anticipated. A total of 3 or more days (or partial days) of data were recorded for 65% of protocol time points, with no data collected for 24% of time points. At baseline, 24 of 25 patients provided data, reduced to 13 of 18 patients at Week 48. Lower-than-expected quality HRV data were obtained, likely because of poor contact between the sensor and the skin. In total, 6 of 25 patients had mild or moderate adverse events (AEs) in the skin and subcutaneous tissue disorders category because of skin irritation caused by the electrode patch. There were no reports of serious AEs or deaths. Most patients found the sensor comfortable, with no or minimal impact on daily activities. Conclusions: The platform can measure physical activity in patients with ALS in their home environment; patients used the equipment successfully, and it was generally well tolerated. The quantity of home monitoring physical activity data was lower than expected, although it was sufficient to allow investigation of novel physical activity end points. Good-quality in-clinic speech data were successfully captured for analysis. Future studies using objective patient monitoring approaches, combined with the most current technological advances, may be useful to elucidate novel digital biomarkers of disease progression.},
author = {Garcia-Gancedo, Luis and Kelly, Madeline L. and Lavrov, Arseniy and Parr, Jim and Hart, Rob and Marsden, Rachael and Turner, Martin R. and Talbot, Kevin and Chiwera, Theresa and Shaw, Christopher E. and Al-Chalabi, Ammar},
booktitle = {JMIR mHealth and uHealth},
doi = {10.2196/13433},
file = {:Users/jferreira-admin/PhD/articles/Objectively Monitoring Amyotrophic Lateral Sclerosis Patient Symptoms During Clinical Trials With Sensors_ Observational Study.htm:htm},
issn = {22915222},
keywords = {Amyotrophic lateral sclerosis,Clinical trial, physical activity,Heart rate,Objective symptom monitoring,Speech},
number = {12},
pmid = {31859676},
title = {{Objectively monitoring amyotrophic lateral sclerosis patient symptoms during clinical trials with sensors: Observational study}},
volume = {7},
year = {2019}
}
@article{Ashleibta2020,
abstract = {The non-contact continuous monitoring of biomarkers comprising breathing detection and heart rate are essential vital signs to evaluate the general physical health of a patient. As compared to existing methods that need dedicated equipment (such as wearable sensors), the radio frequency (RF) signals can be synthesised to continuously monitor breathing rate in a contact-less setting. In this paper, we proposed the contact less breathing rate detection using universal software radio peripheral (USRP) platform without any wearable sensor. Our system leverage on the channel state information (CSI) to record the minute movement caused by breathing over orthogonal frequency division multiplexing (OFDM) in multiple sub-carriers. We presented a comparison of our breathing rate detection with wearable sensor (ground truth) results for single human subject. In this paper, we used wireless data to train, validate and test different machine learning (ML) algorithms to classify USRP data into normal, shallow and elevated breathing depending on the breathing rate. Although different ML models were developed using the K-Nearest Neighbor (KNN), Discriminant Analysis (DA), Naive Bayes (NB) and Decision Tree (DT) algorithms, however results showed KNN based model provided the highest accuracy for our data ( 91%) each time the trial was made. DT (17.131%), DA (59.72%) and NB (48.99%). Results presented in this paper showed that USRP based breathing rate is comparable to the wearable sensor demonstrating the potential application of our method to accurately monitor breathing rate of patients in primary or acute setting.},
author = {Ashleibta, Aboajeila Milad and Abbasi, Qammer H. and Shah, Syed Aziz and Khalid, Arslan and AbuAli, Najah Abed and Imran, Muhammad Ali},
doi = {10.1109/JSEN.2020.3035960},
file = {:Users/jferreira-admin/PhD/articles/IEEESensor_AboMilad3.pdf:pdf},
issn = {15581748},
journal = {IEEE Sensors Journal},
keywords = {Frequency modulation,Monitoring,OFDM,Vital Signs,Wearable sensors,Wireless communication,Wireless fidelity,Wireless sensor networks,channel state information,healthcare application,software defined radios,university software radio peripherals USRPs},
number = {October},
title = {{Non-Invasive RF Sensing for Detecting Breathing Abnormalities using Software Defined Radios}},
year = {2020}
}
@misc{ccc,
file = {:Users/jferreira-admin/PhD/articles/6942190.epub:epub},
title = {6942190}
}
@article{Westergaard2019,
abstract = {Sex-stratified medicine is a fundamentally important, yet understudied, facet of modern medical care. A data-driven model for how to systematically analyze population-wide, longitudinal differences in hospital admissions between men and women is needed. Here, we demonstrate a systematic analysis of all diseases and disease co-occurrences in the complete Danish population using the ICD-10 and Global Burden of Disease terminologies. Incidence rates of single diagnoses are different for men and women in most cases. The age at first diagnosis is typically lower for men, compared to women. Men and women share many disease co-occurrences. However, many sex-associated incongruities not linked directly to anatomical or genomic differences are also found. Analysis of multi-step trajectories uncover differences in longitudinal patterns, for example concerning injuries and substance abuse, cancer, and osteoporosis. The results point towards the need for an increased focus on sex-stratified medicine to elucidate the origins of the socio-economic and ethological differences.},
author = {Westergaard, David and Moseley, Pope and S{\o}rup, Freja Karuna Hemmingsen and Baldi, Pierre and Brunak, S{\o}ren},
doi = {10.1038/s41467-019-08475-9},
file = {:Users/jferreira-admin/PhD/articles/41467_2019_Article_8475.pdf:pdf},
issn = {20411723},
journal = {Nature Communications},
number = {1},
pages = {1--14},
pmid = {30737381},
publisher = {Springer US},
title = {{Population-wide analysis of differences in disease progression patterns in men and women}},
url = {http://dx.doi.org/10.1038/s41467-019-08475-9},
volume = {10},
year = {2019}
}
@article{Zhao2019,
abstract = {Current approaches to predicting a cardiovascular disease (CVD) event rely on conventional risk factors and cross-sectional data. In this study, we applied machine learning and deep learning models to 10-year CVD event prediction by using longitudinal electronic health record (EHR) and genetic data. Our study cohort included 109, 490 individuals. In the first experiment, we extracted aggregated and longitudinal features from EHR. We applied logistic regression, random forests, gradient boosting trees, convolutional neural networks (CNN) and recurrent neural networks with long short-term memory (LSTM) units. In the second experiment, we applied a late-fusion approach to incorporate genetic features. We compared the performance with approaches currently utilized in routine clinical practice – American College of Cardiology and the American Heart Association (ACC/AHA) Pooled Cohort Risk Equation. Our results indicated that incorporating longitudinal feature lead to better event prediction. Combining genetic features through a late-fusion approach can further improve CVD prediction, underscoring the importance of integrating relevant genetic data whenever available.},
author = {Zhao, Juan and Feng, Qi Ping and Wu, Patrick and Lupu, Roxana A. and Wilke, Russell A. and Wells, Quinn S. and Denny, Joshua C. and Wei, Wei Qi},
doi = {10.1038/s41598-018-36745-x},
file = {:Users/jferreira-admin/PhD/articles/41598_2018_Article_36745.pdf:pdf},
issn = {20452322},
journal = {Scientific Reports},
number = {1},
pages = {1--10},
pmid = {30679510},
publisher = {Springer US},
title = {{Learning from Longitudinal Data in Electronic Health Record and Genetic Data to Improve Cardiovascular Event Prediction}},
url = {http://dx.doi.org/10.1038/s41598-018-36745-x},
volume = {9},
year = {2019}
}
@article{Pereira2018,
abstract = {Background: Predicting progression from Mild Cognitive Impairment (MCI) to Alzheimer's Disease (AD) is an utmost open issue in AD-related research. Neuropsychological assessment has proven to be useful in identifying MCI patients who are likely to convert to dementia. However, the large battery of neuropsychological tests (NPTs) performed in clinical practice and the limited number of training examples are challenge to machine learning when learning prognostic models. In this context, it is paramount to pursue approaches that effectively seek for reduced sets of relevant features. Subsets of NPTs from which prognostic models can be learnt should not only be good predictors, but also stable, promoting generalizable and explainable models. Methods: We propose a feature selection (FS) ensemble combining stability and predictability to choose the most relevant NPTs for prognostic prediction in AD. First, we combine the outcome of multiple (filter and embedded) FS methods. Then, we use a wrapper-based approach optimizing both stability and predictability to compute the number of selected features. We use two large prospective studies (ADNI and the Portuguese Cognitive Complaints Cohort, CCC) to evaluate the approach and assess the predictive value of a large number of NPTs. Results: The best subsets of features include approximately 30 and 20 (from the original 79 and 40) features, for ADNI and CCC data, respectively, yielding stability above 0.89 and 0.95, and AUC above 0.87 and 0.82. Most NPTs learnt using the proposed feature selection ensemble have been identified in the literature as strong predictors of conversion from MCI to AD. Conclusions: The FS ensemble approach was able to 1) identify subsets of stable and relevant predictors from a consensus of multiple FS methods using baseline NPTs and 2) learn reliable prognostic models of conversion from MCI to AD using these subsets of features. The machine learning models learnt from these features outperformed the models trained without FS and achieved competitive results when compared to commonly used FS algorithms. Furthermore, the selected features are derived from a consensus of methods thus being more robust, while releasing users from choosing the most appropriate FS method to be used in their classification task.},
author = {Pereira, Telma and Ferreira, Francisco L. and Cardoso, Sandra and Silva, Dina and {De Mendon{\c{c}}a}, Alexandre and Guerreiro, Manuela and Madeira, Sara C.},
doi = {10.1186/s12911-018-0710-y},
file = {:Users/jferreira-admin/PhD/articles/s12911-018-0710-y.pdf:pdf},
issn = {14726947},
journal = {BMC Medical Informatics and Decision Making},
keywords = {Alzheimer's disease,Ensemble learning,Feature selection,Mild cognitive impairment,Neuropsychological data,Prognostic prediction,Time windows},
number = {1},
pages = {1--20},
pmid = {30567554},
publisher = {BMC Medical Informatics and Decision Making},
title = {{Neuropsychological predictors of conversion from mild cognitive impairment to Alzheimer's disease: A feature selection ensemble combining stability and predictability}},
volume = {18},
year = {2018}
}
@article{Zou2016,
abstract = {Precision medicine is an innovative approach that uses emerging biomedical technologies to deliver optimally targeted and timed interventions, customized to the molecular drivers of an individual's disease. This approach is only just beginning to be considered for treating amyotrophic lateral sclerosis (ALS). The clinical and biological complexities of ALS have hindered development of effective therapeutic strategies. In this review we consider applying the key elements of precision medicine to ALS: phenotypic classification, comprehensive risk assessment, presymptomatic period detection, potential molecular pathways, disease model development, biomarker discovery and molecularly tailored interventions. Together, these would embody a precision medicine approach, which may provide strategies for optimal targeting and timing of efforts to prevent, stop or slow progression of ALS.},
author = {Zou, Zhang Yu and Liu, Chang Yun and Che, Chun Hui and Huang, Hua Pin},
doi = {10.3978/j.issn.2305-5839.2016.01.16},
file = {:Users/jferreira-admin/PhD/articles/Towardprecisionmedicineinamyotrophiclateralsclerosis.pdf:pdf},
issn = {23055847},
journal = {Annals of Translational Medicine},
keywords = {Amyotrophic lateral sclerosis (ALS),Customized therapies,Precision medicine},
number = {2},
title = {{Toward precision medicine in amyotrophic lateral sclerosis}},
volume = {4},
year = {2016}
}
@article{Pfohl2018,
abstract = {Objective: The heterogeneity of amyotrophic lateral sclerosis (ALS) survival duration, which varies from <1 year to >10 years, challenges clinical decisions and trials. Utilizing data from 801 deceased ALS patients, we: (1) assess the underlying complex relationships among common clinical ALS metrics; (2) identify which clinical ALS metrics are the “best” survival predictors and how their predictive ability changes as a function of disease progression. Methods: Analyses included examination of relationships within the raw data as well as the construction of interactive survival regression and classification models (generalized linear model and random forests model). Dimensionality reduction and feature clustering enabled decomposition of clinical variable contributions. Thirty-eight metrics were utilized, including Medical Research Council (MRC) muscle scores; respiratory function, including forced vital capacity (FVC) and FVC % predicted, oxygen saturation, negative inspiratory force (NIF); the Revised ALS Functional Rating Scale (ALSFRS-R) and its activities of daily living (ADL) and respiratory sub-scores; body weight; onset type, onset age, gender, and height. Prognostic random forest models confirm the dominance of patient age-related parameters decline in classifying survival at thresholds of 30, 60, 90, and 180 days and 1, 2, 3, 4, and 5 years. Results: Collective prognostic insight derived from the overall investigation includes: multi-dimensionality of ALSFRS-R scores suggests cautious usage for survival forecasting; upper and lower extremities independently degenerate and are autonomous from respiratory decline, with the latter associating with nearer-to-death classifications; height and weight-based metrics are auxiliary predictors for farther-from-death classifications; sex and onset site (limb, bulbar) are not independent survival predictors due to age co-correlation. Conclusion: The dimensionality and fluctuating predictors of ALS survival must be considered when developing predictive models for clinical trial development or in-clinic usage. Additional independent metrics and possible revisions to current metrics, like the ALSFRS-R, are needed to capture the underlying complexity needed for population and personalized forecasting of survival.},
author = {Pfohl, Stephen R. and Kim, Renaid B. and Coan, Grant S. and Mitchell, Cassie S.},
doi = {10.3389/fninf.2018.00036},
file = {:Users/jferreira-admin/PhD/articles/fninf-12-00036.pdf:pdf},
issn = {16625196},
journal = {Frontiers in Neuroinformatics},
keywords = {Data complexity,Health informatics,Neuromuscular disease,Predictive medicine,Survival analysis},
number = {June},
pages = {1--13},
title = {{Unraveling the complexity of amyotrophic lateral sclerosis survival prediction}},
volume = {12},
year = {2018}
}
@article{Kueffner2019,
abstract = {Amyotrophic lateral sclerosis (ALS) is a fatal neurodegenerative disease where substantial heterogeneity in clinical presentation urgently requires a better stratification of patients for the development of drug trials and clinical care. In this study we explored stratification through a crowdsourcing approach, the DREAM Prize4Life ALS Stratification Challenge. Using data from >10,000 patients from ALS clinical trials and 1479 patients from community-based patient registers, more than 30 teams developed new approaches for machine learning and clustering, outperforming the best current predictions of disease outcome. We propose a new method to integrate and analyze patient clusters across methods, showing a clear pattern of consistent and clinically relevant sub-groups of patients that also enabled the reliable classification of new patients. Our analyses reveal novel insights in ALS and describe for the first time the potential of a crowdsourcing to uncover hidden patient sub-populations, and to accelerate disease understanding and therapeutic development.},
author = {Kueffner, Robert and Zach, Neta and Bronfeld, Maya and Norel, Raquel and Atassi, Nazem and Balagurusamy, Venkat and {Di Camillo}, Barbara and Chio, Adriano and Cudkowicz, Merit and Dillenberger, Donna and Garcia-Garcia, Javier and Hardiman, Orla and Hoff, Bruce and Knight, Joshua and Leitner, Melanie L. and Li, Guang and Mangravite, Lara and Norman, Thea and Wang, Liuxia and Xiao, Jinfeng and Fang, Wen Chieh and Peng, Jian and Yang, Chen and Chang, Huan Jui and Stolovitzky, Gustavo and Alkallas, Rached and Anghel, Catalina and Avril, Jeanne and Bacardit, Jaume and Balser, Barbara and Balser, John and Bar-Sinai, Yoav and Ben-David, Noa and Ben-Zion, Eyal and Bliss, Robin and Cai, Jialu and Chernyshev, Anatoly and Chiang, Jung Hsien and Chicco, Davide and Corriveau, Bhavna Ahuja Nicole and Dai, Junqiang and Deshpande, Yash and Desplats, Eve and Durgin, Joseph S. and Espiritu, Shadrielle Melijah G. and Fan, Fan and Fevrier, Philippe and Fridley, Brooke L. and Godzik, Adam and Goli{\'{n}}ska, Agnieszka and Gordon, Jonathan and Graw, Stefan and Guo, Yuelong and Herpelinck, Tim and Hopkins, Julia and Huang, Barbara and Jacobsen, Jeremy and Jahandideh, Samad and Jeon, Jouhyun and Ji, Wenkai and Jung, Kenneth and Karanevich, Alex and Koestler, Devin C. and Kozak, Michael and Kurz, Christoph and Lalansingh, Christopher and Larrieu, Thomas and Lazzarini, Nicola and Lerner, Boaz and Lesinski, Wojciech and Liang, Xiaotao and Lin, Xihui and Lowe, Jarrett and Mackey, Lester and Meier, Richard and Min, Wenwen and Mnich, Krzysztof and Nahmias, Violette and Noel-Macdonnell, Janelle and O'donnell, Adrienne and Paadre, Susan and Park, Ji and Polewko-Klim, Aneta and Raghavan, Rama and Rudnicki, Witold and Saghapour, Ehsan and Salomond, Jean Bernard and Sankaran, Kris and Sendorek, Dorota and Sharan, Vatsal and Shiah, Yu Jia and Sirois, Jean Karl and Sumanaweera, Dinithi N. and Usset, Joseph and Vang, Yeeleng S. and Vens, Celine and Wadden, Dave and Wang, David and Wong, Wing Chung and Xie, Xiaohui and Xu, Zhiqing and Yang, Hsih Te and Yu, Xiang and Zhang, Haichen and Zhang, Li and Zhang, Shihua and Zhu, Shanfeng},
doi = {10.1038/s41598-018-36873-4},
file = {:Users/jferreira-admin/PhD/articles/41598_2018_Article_36873.pdf:pdf},
issn = {20452322},
journal = {Scientific Reports},
number = {1},
pages = {1--14},
pmid = {30679616},
title = {{Stratification of amyotrophic lateral sclerosis patients: A crowdsourcing approach}},
volume = {9},
year = {2019}
}
@article{Martin2017,
abstract = {Amyotrophic lateral sclerosis is a neurodegenerative disease predominantly affecting upper and lower motor neurons, resulting in progressive paralysis and death from respiratory failure within 2 to 3 years. The peak age of onset is 55 to 70 years, with a male predominance. The causes of amyotrophic lateral sclerosis are only partly known, but they include some environmental risk factors as well as several genes that have been identified as harbouring disease-associated variation. Here we review the nature, epidemiology, genetic associations, and environmental exposures associated with amyotrophic lateral sclerosis.},
author = {Martin, Sarah and {Al Khleifat}, Ahmad and Al-Chalabi, Ammar},
doi = {10.12688/f1000research.10476.1},
file = {:Users/jferreira-admin/PhD/articles/f1000research-6-11289.pdf:pdf},
issn = {1759796X},
journal = {F1000Research},
keywords = {Amyotrophic lateral sclerosis,Motor neuron disease,Neurodegenerative disease},
number = {0},
pages = {1--10},
pmid = {28408982},
title = {{What causes amyotrophic lateral sclerosis?}},
volume = {6},
year = {2017}
}
@article{Brown2016,
author = {Brown, Sherry Ann},
doi = {10.3389/fphys.2016.00561},
file = {:Users/jferreira-admin/PhD/articles/fphys-07-00561.pdf:pdf},
issn = {1664042X},
journal = {Frontiers in Physiology},
keywords = {Big data analytics,Clinical decision support,Computational medicine,Patient similarity analytics,patient similarity},
number = {NOV},
pages = {1--6},
title = {{Patient similarity: Emerging concepts in systems and precision medicine}},
volume = {7},
year = {2016}
}
@article{22019,
abstract = {Background: Diagnosis requires that clinicians communicate and share patient information in an efficient manner. Advances in electronic health records (EHR) and health information technologies have created both challenges and opportunities for such communication. Methods: We conducted a multi-method, focused ethnographic study of physicians on general medicine inpatient units in two teaching hospitals. Physician teams were observed during and after morning rounds to understand workflow, data sharing and communication during diagnosis. To validate findings, interviews and focus groups were conducted with physicians. Field notes and interview/focus group transcripts were reviewed and themes identified using content analysis. Results: Existing communication technologies and EHR-based data sharing processes were perceived as barriers to diagnosis. In particular, reliance on paging systems and lack of face-to- face communication among clinicians created obstacles to sustained thinking and discussion of diagnostic decision-making. Further, the EHR created data overload and data fragmentation, making integration for diagnosis difficult. To improve diagnosis, physicians recommended replacing pagers with two-way communication devices, restructuring the EHR to facilitate access to key information, and improving training on EHR systems. Conclusions: As advances in health information technology evolve, challenges in the way clinicians share information during the diagnostic process will rise. To improve diagnosis, changes to both the technology and the way in which we use it may be necessary.},
author = {2},
doi = {10.1016/j.jmb.2018.05.037.Patient},
file = {:Users/jferreira-admin/PhD/articles/nihms975565.pdf:pdf},
journal = {Physiology & behavior},
keywords = {endothelium,estrogen,estrogen receptors,vascular smooth muscle},
number = {3},
pages = {139--148},
title = {{乳鼠心肌提取 HHS Public Access}},
volume = {176},
year = {2019}
}
@article{Henriques2014,
abstract = {BACKGROUND: Biclustering, the discovery of sets of objects with a coherent pattern across a subset of conditions, is a critical task to study a wide-set of biomedical problems, where molecular units or patients are meaningfully related with a set of properties. The challenging combinatorial nature of this task led to the development of approaches with restrictions on the allowed type, number and quality of biclusters. Contrasting, recent biclustering approaches relying on pattern mining methods can exhaustively discover flexible structures of robust biclusters. However, these approaches are only prepared to discover constant biclusters and their underlying contributions remain dispersed.\n\nMETHODS: The proposed BicPAM biclustering approach integrates existing principles made available by state-of-the-art pattern-based approaches with two new contributions. First, BicPAM is the first efficient attempt to exhaustively mine non-constant types of biclusters, including additive and multiplicative coherencies in the presence or absence of symmetries. Second, BicPAM provides strategies to effectively compose different biclustering structures and to handle arbitrary levels of noise inherent to data and with discretization procedures.\n\nRESULTS: Results show BicPAM's superiority against its peers and its ability to retrieve unique types of biclusters of interest, to efficiently deliver exhaustive solutions and to successfully recover planted biclusters in datasets with varying levels of missing values and noise. Its application over gene expression data leads to unique solutions with heightened biological relevance.\n\nCONCLUSIONS: BicPAM approaches integrate existing disperse efforts towards pattern-based biclustering and provides the first critical strategies to efficiently discover exhaustive solutions of biclusters with shifting, scaling and symmetric assumptions with varying quality and underlying structures. Additionally, BicPAM dynamically adapts its behavior to mine data with different levels of missing values and noise.},
author = {Henriques, Rui and Madeira, Sara C},
doi = {10.1186/s13015-014-0027-z},
file = {:Users/jferreira-admin/PhD/articles/s13015-014-0027-z.pdf:pdf},
issn = {1748-7188},
journal = {Algorithms for Molecular Biology},
keywords = {biclustering,biomedical data analysis,pattern mining},
number = {1},
pages = {1--30},
title = {{BicPAM: Pattern-based biclustering for biomedical data analysis}},
volume = {9},
year = {2014}
}
@article{Apweiler2018,
abstract = {New technologies to generate, store and retrieve medical and research data are inducing a rapid change in clinical and translational research and health care. Systems medicine is the interdisciplinary approach wherein physicians and clinical investigators team up with experts from biology, biostatistics, informatics, mathematics and computational modeling to develop methods to use new and stored data to the benefit of the patient. We here provide a critical assessment of the opportunities and challenges arising out of systems approaches in medicine and from this provide a definition of what systems medicine entails. Based on our analysis of current developments in medicine and healthcare and associated research needs, we emphasize the role of systems medicine as a multilevel and multidisciplinary methodological framework for informed data acquisition and interdisciplinary data analysis to extract previously inaccessible knowledge for the benefit of patients.},
author = {Apweiler, Rolf and Beissbarth, Tim and Berthold, Michael R. and Bl{\"{u}}thgen, Nils and Burmeister, Yvonne and Dammann, Olaf and Deutsch, Andreas and Feuerhake, Friedrich and Franke, Andre and Hasenauer, Jan and Hoffmann, Steve and H{\"{o}}fer, Thomas and Jansen, Peter L.M. and Kaderali, Lars and Klingm{\"{u}}ller, Ursula and Koch, Ina and Kohlbacher, Oliver and Kuepfer, Lars and Lammert, Frank and Maier, DIeter and Pfeifer, Nico and Radde, Nicole and Rehm, Markus and Roeder, Ingo and Saez-Rodriguez, Julio and Sax, Ulrich and Schmeck, Bernd and Schuppert, Andreas and Seilheimer, Bernd and Theis, Fabian J. and Vera, Julio and Wolkenhauer, Olaf},
doi = {10.1038/emm.2017.290},
file = {:Users/jferreira-admin/PhD/articles/emm2017290a.pdf:pdf},
issn = {20926413},
journal = {Experimental and Molecular Medicine},
number = {3},
pages = {e453--6},
pmid = {29497170},
publisher = {Nature Publishing Group},
title = {{Whither systems medicine?}},
url = {http://dx.doi.org/10.1038/emm.2017.290},
volume = {50},
year = {2018}
}
@book{Henriques2018a,
abstract = {Statistical evaluation of biclustering solutions is essential to guarantee the absence of spurious relations and to validate the high number of scientific statements inferred from unsupervised data analysis without a proper statistical ground. Most biclustering methods rely on merit functions to discover biclusters with specific homogeneity criteria. However, strong homogeneity does not guarantee the statistical significance of biclustering solutions. Furthermore, although some biclustering methods test the statistical significance of specific types of biclusters, there are no methods to assess the significance of flexible biclustering models. This work proposes a method to evaluate the statistical significance of biclustering solutions. It integrates state-of-the-art statistical views on the significance of local patterns and extends them with new principles to assess the significance of biclusters with additive, multiplicative, symmetric, order-preserving and plaid coherencies. The proposed statistical tests provide the unprecedented possibility to minimize the number of false positive biclusters without incurring on false negatives, and to compare state-of-the-art biclustering algorithms according to the statistical significance of their outputs. Results on synthetic and real data support the soundness and relevance of the proposed contributions, and stress the need to combine significance and homogeneity criteria to guide the search for biclusters.},
author = {Henriques, Rui and Madeira, Sara C.},
booktitle = {Data Mining and Knowledge Discovery},
doi = {10.1007/s10618-017-0521-2},
file = {:Users/jferreira-admin/PhD/articles/Henriques-Madeira2018_Article_BSigEvaluatingTheStatisticalSi.pdf:pdf},
isbn = {1061801705},
issn = {1573756X},
keywords = {Biclustering,Pattern mining,Statistical significance},
number = {1},
pages = {124--161},
publisher = {Springer US},
title = {{BSig: evaluating the statistical significance of biclustering solutions}},
volume = {32},
year = {2018}
}
@article{DeCarvalho2017,
abstract = {Objective: To define an applicable dataset for ALS patient registries we weighted specific clinical items as scored by worldwide ALS experts. Methods: Sixty participants were invited based on relevant clinical work, publications and personal acquaintance. They rated 160 clinical items consensually agreed by the members of our project, incorporating specialists from five European Centres. Scoring scheme was defined as: 1–essential; 2–important; 3–not very important. A mixed effect model was applied to rank items and to find possible correlations with geographical region (Europe vs. outside Europe). Results: We received 40 responses, 20 from Europe and 20 from outside; 42/160 data were scored as essential by >50% of the respondents, including: date of birth, gender, date of disease onset, date of diagnosis, ethnicity, region of onset, predominant upper neuron (UMN) or lower motor neuron (LMN) impairment, proximal versus distal weakness, respiratory symptoms, dysarthria, weight loss, signs of LMN/UMN involvement, emotional incontinence, cognitive changes, respiratory signs, neck weakness, body mass index, ALSFRS-R at entry, ALSFRS-R subscores at entry, timing and pattern of spreading and staging, electromyography, spirometry, MRI, CK level, riluzole intake, genetic background, history of physical exercise and previous and current main occupation. Four components were scored as non-relevant, including place of birth, blood pressure and pain at onset. There was no significant difference between regions (European vs. non-European countries). Conclusions: Our study identified a consensual set of clinical data with 42 specific items that can be used as a minimal data set for patient registers and for clinical trials.},
author = {{De Carvalho}, Mamede and Ryczkowski, Adam and Andersen, Peter and Gromicho, Marta and Grosskreutz, Julian and Ku{\'{z}}ma-Kozakiewicz, Magdalena and Petri, Susanne and Piotrkiewicz, Maria and {Miltenberger Miltenyi}, Gabriel},
doi = {10.1080/21678421.2017.1349150},
file = {:Users/jferreira-admin/PhD/articles/2017SurveyofALSExpertsonlinefirst.pdf:pdf},
issn = {21679223},
journal = {Amyotrophic Lateral Sclerosis and Frontotemporal Degeneration},
keywords = {Amyotrophic lateral sclerosis,hierarchy,signals,survey,symptoms},
number = {7-8},
pages = {505--510},
pmid = {28705085},
publisher = {Informa UK Limited, trading as Taylor & Francis Group},
title = {{International Survey of ALS Experts about Critical Questions for Assessing Patients with ALS}},
url = {https://doi.org/10.1080/21678421.2017.1349150},
volume = {18},
year = {2017}
}
@book{Henriques2015,
abstract = {Repositories of health records are collections of events with varying number and sparsity of occurrences within and among patients. Although a large number of predictive models have been proposed in the last decade, they are not yet able to simultaneously capture cross-attribute and temporal dependencies associated with these repositories. Two major streams of predictive models can be found. On one hand, deterministic models rely on compact subsets of discriminative events to anticipate medical conditions. On the other hand, generative models offer a more complete and noise-tolerant view based on the likelihood of the testing arrangements of events to discriminate a particular outcome. However, despite the relevance of generative predictive models, they are not easily extensible to deal with complex grids of events. In this work, we rely on the Markov assumption to propose new predictive models able to deal with cross-attribute and temporal dependencies. Experimental results hold evidence for the utility and superior accuracy of generative models to anticipate health conditions, such as the need for surgeries. Additionally, we show that the proposed generative models are able to decode temporal patterns of interest (from the learned lattices) with acceptable completeness and precision levels, and with superior efficiency for voluminous repositories.},
author = {Henriques, Rui and Antunes, Cl{\'{a}}udia and Madeira, Sara C.},
booktitle = {Data Mining and Knowledge Discovery},
doi = {10.1007/s10618-014-0385-7},
file = {:Users/jferreira-admin/PhD/articles/Henriques2015_Article_GenerativeModelingOfRepositori.pdf:pdf},
isbn = {1061801403857},
issn = {1573756X},
keywords = {Cross-attribute dependencies,Hidden Markov models,Integrated healthcare data,Predictive models,Repositories of events,Sparse temporal data,Temporal dependencies},
number = {4},
pages = {999--1032},
publisher = {Springer US},
title = {{Generative modeling of repositories of health records for predictive tasks}},
volume = {29},
year = {2015}
}
@article{Lipton2016,
abstract = {Clinical medical data, especially in the intensive care unit (ICU), consist of multivariate time series of observations. For each patient visit (or episode), sensor data and lab test results are recorded in the patient's Electronic Health Record (EHR). While potentially containing a wealth of insights, the data is difficult to mine effectively, owing to varying length, irregular sampling and missing data. Recurrent Neural Networks (RNNs), particularly those using Long Short-Term Memory (LSTM) hidden units, are powerful and increasingly popular models for learning from sequence data. They effectively model varying length sequences and capture long range dependencies. We present the first study to empirically evaluate the ability of LSTMs to recognize patterns in multivariate time series of clinical measurements. Specifically, we consider multilabel classification of diagnoses, training a model to classify 128 diagnoses given 13 frequently but irregularly sampled clinical measurements. First, we establish the effectiveness of a simple LSTM network for modeling clinical data. Then we demonstrate a straightforward and effective training strategy in which we replicate targets at each sequence step. Trained only on raw time series, our models outperform several strong baselines, including a multilayer perceptron trained on hand-engineered features.},
archivePrefix = {arXiv},
arxivId = {1511.03677},
author = {Lipton, Zachary C. and Kale, David C. and Elkan, Charles and Wetzel, Randall},
eprint = {1511.03677},
file = {:Users/jferreira-admin/PhD/articles/1511.03677.pdf:pdf},
journal = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
pages = {1--18},
title = {{Learning to diagnose with LSTM recurrent neural networks}},
year = {2016}
}
@article{Westeneng2018a,
abstract = {Background: Amyotrophic lateral sclerosis (ALS) is a relentlessly progressive, fatal motor neuron disease with a variable natural history. There are no accurate models that predict the disease course and outcomes, which complicates risk assessment and counselling for individual patients, stratification of patients for trials, and timing of interventions. We therefore aimed to develop and validate a model for predicting a composite survival endpoint for individual patients with ALS. Methods: We obtained data for patients from 14 specialised ALS centres (each one designated as a cohort) in Belgium, France, the Netherlands, Germany, Ireland, Italy, Portugal, Switzerland, and the UK. All patients were diagnosed in the centres after excluding other diagnoses and classified according to revised El Escorial criteria. We assessed 16 patient characteristics as potential predictors of a composite survival outcome (time between onset of symptoms and non-invasive ventilation for more than 23 h per day, tracheostomy, or death) and applied backward elimination with bootstrapping in the largest population-based dataset for predictor selection. Data were gathered on the day of diagnosis or as soon as possible thereafter. Predictors that were selected in more than 70% of the bootstrap resamples were used to develop a multivariable Royston-Parmar model for predicting the composite survival outcome in individual patients. We assessed the generalisability of the model by estimating heterogeneity of predictive accuracy across external populations (ie, populations not used to develop the model) using internal–external cross-validation, and quantified the discrimination using the concordance (c) statistic (area under the receiver operator characteristic curve) and calibration using a calibration slope. Findings: Data were collected between Jan 1, 1992, and Sept 22, 2016 (the largest data-set included data from 1936 patients). The median follow-up time was 97{\textperiodcentered}5 months (IQR 52{\textperiodcentered}9–168{\textperiodcentered}5). Eight candidate predictors entered the prediction model: bulbar versus non-bulbar onset (univariable hazard ratio [HR] 1{\textperiodcentered}71, 95% CI 1{\textperiodcentered}63–1{\textperiodcentered}79), age at onset (1{\textperiodcentered}03, 1{\textperiodcentered}03–1{\textperiodcentered}03), definite versus probable or possible ALS (1{\textperiodcentered}47, 1{\textperiodcentered}39–1{\textperiodcentered}55), diagnostic delay (0{\textperiodcentered}52, 0{\textperiodcentered}51–0{\textperiodcentered}53), forced vital capacity (HR 0{\textperiodcentered}99, 0{\textperiodcentered}99–0{\textperiodcentered}99), progression rate (6{\textperiodcentered}33, 5{\textperiodcentered}92–6{\textperiodcentered}76), frontotemporal dementia (1{\textperiodcentered}34, 1{\textperiodcentered}20–1{\textperiodcentered}50), and presence of a C9orf72 repeat expansion (1{\textperiodcentered}45, 1{\textperiodcentered}31–1{\textperiodcentered}61), all p<0{\textperiodcentered}0001. The c statistic for external predictive accuracy of the model was 0{\textperiodcentered}78 (95% CI 0{\textperiodcentered}77–0{\textperiodcentered}80; 95% prediction interval [PI] 0{\textperiodcentered}74–0{\textperiodcentered}82) and the calibration slope was 1{\textperiodcentered}01 (95% CI 0{\textperiodcentered}95–1{\textperiodcentered}07; 95% PI 0{\textperiodcentered}83–1{\textperiodcentered}18). The model was used to define five groups with distinct median predicted (SE) and observed (SE) times in months from symptom onset to the composite survival outcome: very short 17{\textperiodcentered}7 (0{\textperiodcentered}20), 16{\textperiodcentered}5 (0{\textperiodcentered}23); short 25{\textperiodcentered}3 (0{\textperiodcentered}06), 25{\textperiodcentered}2 (0{\textperiodcentered}35); intermediate 32{\textperiodcentered}2 (0{\textperiodcentered}09), 32{\textperiodcentered}8 (0{\textperiodcentered}46); long 43{\textperiodcentered}7 (0{\textperiodcentered}21), 44{\textperiodcentered}6 (0{\textperiodcentered}74); and very long 91{\textperiodcentered}0 (1{\textperiodcentered}84), 85{\textperiodcentered}6 (1{\textperiodcentered}96). Interpretation: We have developed an externally validated model to predict survival without tracheostomy and non-invasive ventilation for more than 23 h per day in European patients with ALS. This model could be applied to individualised patient management, counselling, and future trial design, but to maximise the benefit and prevent harm it is intended to be used by medical doctors only. Funding: Netherlands ALS Foundation.},
author = {Westeneng, Henk Jan and Debray, Thomas P.A. and Visser, Anne E. and van Eijk, Ruben P.A. and Rooney, James P.K. and Calvo, Andrea and Martin, Sarah and McDermott, Christopher J. and Thompson, Alexander G. and Pinto, Susana and Kobeleva, Xenia and Rosenbohm, Angela and Stubendorff, Beatrice and Sommer, Helma and Middelkoop, Bas M. and Dekker, Annelot M. and van Vugt, Joke J.F.A. and van Rheenen, Wouter and Vajda, Alice and Heverin, Mark and Kazoka, Mbombe and Hollinger, Hannah and Gromicho, Marta and K{\"{o}}rner, Sonja and Ringer, Thomas M. and R{\"{o}}diger, Annekathrin and Gunkel, Anne and Shaw, Christopher E. and Bredenoord, Annelien L. and van Es, Michael A. and Corcia, Philippe and Couratier, Philippe and Weber, Markus and Grosskreutz, Julian and Ludolph, Albert C. and Petri, Susanne and de Carvalho, Mamede and {Van Damme}, Philip and Talbot, Kevin and Turner, Martin R. and Shaw, Pamela J. and Al-Chalabi, Ammar and Chi{\`{o}}, Adriano and Hardiman, Orla and Moons, Karel G.M. and Veldink, Jan H. and van den Berg, Leonard H.},
doi = {10.1016/S1474-4422(18)30089-9},
file = {:Users/jferreira-admin/PhD/articles/Lancet Neurol 2018 - Westeneng - Prognosis-for-patients-with-amyotrophic-lateral-sclerosis.pdf:pdf},
issn = {14744465},
journal = {The Lancet Neurology},
number = {5},
pages = {423--433},
pmid = {29598923},
title = {{Prognosis for patients with amyotrophic lateral sclerosis: development and validation of a personalised prediction model}},
volume = {17},
year = {2018}
}
@article{Rajkomar2019,
abstract = {A 49-year-old patient notices a painless rash on his shoulder but does not seek care. Months later, his wife asks him to see a doctor, who diagnoses a seborrheic keratosis. Later, when the patient undergoes a screening colonoscopy, a nurse notices a dark macule on his shoulder and advises him to have it evaluated. One month later, the patient sees a dermatologist, who obtains a biopsy specimen of the lesion. The findings reveal a noncancerous pigmented lesion. Still concerned, the dermatologist requests a second reading of the biopsy specimen, and invasive melanoma is diagnosed. An oncologist initiates treatment with systemic chemotherapy. A physician friend asks the patient why he is not receiving immunotherapy. W hat if every medical decision, whether made by an intensivist or a community health worker, was instantly reviewed by a team of relevant experts who provided guidance if the decision seemed amiss? Patients with newly diagnosed, uncomplicated hypertension would receive the medications that are known to be most effective rather than the one that is most familiar to the prescriber. 1,2 Inadvertent overdoses and errors in prescribing would be largely eliminated. 3,4 Patients with mysterious and rare ailments could be directed to renowned experts in fields related to the suspected diagnosis. 5 Such a system seems far-fetched. There are not enough medical experts to staff it, it would take too long for experts to read through a patient's history, and concerns related to privacy laws would stop efforts before they started. 6 Yet, this is the promise of machine learning in medicine: the wisdom contained in the decisions made by nearly all clinicians and the outcomes of billions of patients should inform the care of each patient. That is, every diagnosis, management decision, and therapy should be personalized on the basis of all known information about a patient, in real time, incorporating lessons from a collective experience. This framing emphasizes that machine learning is not just a new tool, such as a new drug or medical device. Rather, it is the fundamental technology required to meaningfully process data that exceed the capacity of the human brain to comprehend ; increasingly, this overwhelming store of information pertains to both vast clinical databases and even the data generated regarding a single patient. 7 Nearly 50 years ago, a Special Article in the Journal stated that computing would be "augmenting and, in some cases, largely replacing the intellectual functions of the physician." 8 Yet, in early 2019, surprisingly little in health care is driven by machine learning. Rather than report the myriad proof-of-concept models (of retrospective data) that have been tested, here we describe the core structural changes and paradigm shifts in the health care system that are necessary to enable the full promise of machine learning in medicine (see video). M achine Le a r ning E x pl a ined Traditionally, software engineers have distilled knowledge in the form of explicit computer code that instructs computers exactly how to process data and how to A video overview of machine learning is available at NEJM.org},
author = {Rajkomar, Alvin and Dean, Jeffrey and Kohane, Isaac},
doi = {10.1056/nejmra1814259},
file = {:Users/jferreira-admin/PhD/articles/july-31-ai-homework3.pdf:pdf},
issn = {0028-4793},
journal = {New England Journal of Medicine},
number = {14},
pages = {1347--1358},
pmid = {30943338},
title = {{Machine Learning in Medicine}},
volume = {380},
year = {2019}
}
@misc{gggg,
title = {{Grubbs : Sample Criteria for Testing Outlying Observations}},
url = {https://projecteuclid.org/euclid.aoms/1177729885},
urldate = {2020-01-20}
}
@article{Grubbs1950,
abstract = {The problem of testing outlying observations, although an old one, is of considerable importance in applied statistics. Many and various types of significance tests have been proposed by statisticians interested in this field of application. In this connection, we bring out in the Histrical Comments notable advances toward a clear formulation of the problem and important points which should be considered in attempting a complete solution. In Section 4 we state some of the situations the experimental statistician will very likely encounter in practice, these considerations being based on experience. For testing the significance of the largest observation in a sample of size n from a normal population, we propose the statistic S2nS2=∑n−1i=1(xi−x¯n)2∑ni=1(xi−x¯)2 where x1≤x2≤⋯≤xn,x¯n=1n−1∑n−1i=1xi and x¯=1n∑ni=1xi. A similar statistic, S21/S2, can be used for testing whether the smallest observation is too low. It turns out that S2nS2=1−1n−1(xn−x¯s)2=1−1n−1T2n, where s2=1n$\sigma$(xi−x¯)2, and Tn is the studentized extreme deviation already suggested by E. Pearson and C. Chandra Sekar [1] for testing the significance of the largest observation. Based on previous work by W. R. Thompson [12], Pearson and Chandra Sekar were able to obtain certain percentage points of Tn without deriving the exact distribution of Tn. The exact distribution of S2n/S2 (or Tn) is apparently derived for the first time by the present author. For testing whether the two largest observations are too large we propose the statistic S2n−1,nS2=∑n−2i=1(xi−x¯n−1,n)2∑ni=1(xi−x¯)2,x¯n−1,n=1n−2∑n−2i=1xi and a similar statistic, S21,2/S2, can be used to test the significance of the two smallest observations. The probability distributions of the above sample statistics S2=∑ni=1(xi−x¯)2wherex¯=1n∑ni=1xi S2n=∑n−1i=1(xi−x¯n)2wherex¯n=1n−1∑n−1i=1xi S21=∑ni=2(xi−x¯1)2wherex¯1=1n−1∑ni=2xi are derived for a normal parent and tables of appropriate percentage points are given in this paper (Table I and Table V). Although the efficiencies of the above tests have not been completely investigated under various models for outlying observations, it is apparent that the proposed sample criteria have considerable intuitive appeal. In deriving the distributions of the sample statistics for testing the largest (or smallest) or the two largest (or two smallest) observations, it was first necessary to derive the distribution of the difference between the extreme observation and the sample mean in terms of the population $\sigma$. This probabilityX1≤x2≤x3⋯≤xn s2=1n∑ni=1(xi−x¯)2x¯=1n∑ni=1xi distribution was apparently derived first by A. T. McKay [11] who employed the method of characteristic functions. The author was not aware of the work of McKay when the simplified derivation for the distribution of xn−x¯$\sigma$ outlined in Section 5 below was worked out by him in the spring of 1945, McKay's result being called to his attention by C. C. Craig. It has been noted also that K. R. Nair [20] worked out independently and published the same derivation of the distribution of the extreme minus the mean arrived at by the present author--see Biometrika, Vol. 35, May, 1948. We nevertheless include part of this derivation in Section 5 below as it was basic to the work in connection with the derivations given in Sections 8 and 9. Our table is considerably more extensive than Nair's table of the probability integral of the extreme deviation from the sample mean in normal samples, since Nair's table runs from n=2 to n=9, whereas our Table II is for n=2 to n=25. The present work is concluded with some examples.},
author = {Grubbs, Frank E.},
doi = {10.1214/aoms/1177729885},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
month = {mar},
number = {1},
pages = {27--58},
publisher = {Institute of Mathematical Statistics},
title = {{Sample Criteria for Testing Outlying Observations}},
volume = {21},
year = {1950}
}
@unpublished{Greenacre2013,
author = {Greenacre, Michael and Ayhan, Oztas},
title = {{(PDF) Identifying Inliers}},
url = {https://www.researchgate.net/publication/298069752_Identifying_Inliers},
year = {2013}
}
@article{Taylor2002,
author = {Taylor, Rosemary N. and McEntegart, Damian J. and Stillman, Eleanor C.},
doi = {10.1177/009286150203600115},
issn = {0092-8615},
journal = {Drug Information Journal},
month = {jan},
number = {1},
pages = {115--125},
title = {{Statistical Techniques to Detect Fraud and other Data Irregularities in Clinical Questionnaire Data}},
url = {http://journals.sagepub.com/doi/10.1177/009286150203600115},
volume = {36},
year = {2002}
}
@article{Venet2012a,
abstract = {BACKGROUND Classical monitoring approaches rely on extensive on-site visits and source data verification. These activities are associated with high cost and a limited contribution to data quality. Central statistical monitoring is of particular interest to address these shortcomings. PURPOSE This article outlines the principles of central statistical monitoring and the challenges of implementing it in actual trials. METHODS A statistical approach to central monitoring is based on a large number of statistical tests performed on all variables collected in the database, in order to identify centers that differ from the others. The tests generate a high-dimensional matrix of p-values, which can be analyzed by statistical methods and bioinformatic tools to identify extreme centers. RESULTS Results from actual trials are provided to illustrate typical findings that can be expected from a central statistical monitoring approach, which detects abnormal patterns that were not (or could not have been) detected by on-site monitoring. LIMITATIONS Central statistical monitoring can only address problems present in the data. Important aspects of trial conduct such as a lack of informed consent documentation, for instance, require other approaches. The results provided here are empirical examples from a limited number of studies. CONCLUSION Central statistical monitoring can both optimize on-site monitoring and improve data quality and as such provides a cost-effective way of meeting regulatory requirements for clinical trials.},
author = {Venet, David and Doffagne, Erik and Burzykowski, Tomasz and Beckers, Fran{\c{c}}ois and Tellier, Yves and Genevois-Marlin, Eric and Becker, Ursula and Bee, Valerie and Wilson, Veronique and Legrand, Catherine and Buyse, Marc},
doi = {10.1177/1740774512447898},
issn = {1740-7745},
journal = {Clinical Trials: Journal of the Society for Clinical Trials},
month = {dec},
number = {6},
pages = {705--713},
pmid = {22684241},
title = {{A statistical approach to central monitoring of data quality in clinical trials}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22684241 http://journals.sagepub.com/doi/10.1177/1740774512447898},
volume = {9},
year = {2012}
}
@article{Buyse1999,
abstract = {Recent cases of fraud in clinical trials have attracted considerable media attention, but relatively little reaction from the biostatistical community. In this paper we argue that biostatisticians should be involved in preventing fraud (as well as unintentional errors), detecting it, and quantifying its impact on the outcome of clinical trials. We use the term 'fraud' specifically to refer to data fabrication (making up data values) and falsification (changing data values). Reported cases of such fraud involve cheating on inclusion criteria so that ineligible patients can enter the trial, and fabricating data so that no requested data are missing. Such types of fraud are partially preventable through a simplification of the eligibility criteria and through a reduction in the amount of data requested. These two measures are feasible and desirable in a surprisingly large number of clinical trials, and neither of them in any way jeopardizes the validity of the trial results. With regards to detection of fraud, a brute force approach has traditionally been used, whereby the participating centres undergo extensive monitoring involving up to 100 per cent verification of their case records. The cost-effectiveness of this approach seems highly debatable, since one could implement quality control through random sampling schemes, as is done in fields other than clinical medicine. Moreover, there are statistical techniques available (but insufficiently used) to detect 'strange' patterns in the data including, but no limited to, techniques for studying outliers, inliers, overdispersion, underdispersion and correlations or lack thereof. These techniques all rest upon the premise that it is quite difficult to invent plausible data, particularly highly dimensional multivariate data. The multicentric nature of clinical trials also offers an opportunity to check the plausibility of the data submitted by one centre by comparing them with the data from all other centres. Finally, with fraud detected, it is essential to quantify its likely impact upon the outcome of the clinical trial. Many instances of fraud in clinical trials, although morally reprehensible, have a negligible impact on the trial's scientific conclusions.},
author = {Buyse, M and George, S L and Evans, S and Geller, N L and Ranstam, J and Scherrer, B and Lesaffre, E and Murray, G and Edler, L and Hutton, J and Colton, T and Lachenbruch, P and Verma, B L},
doi = {10.1002/(sici)1097-0258(19991230)18:24<3435::aid-sim365>3.0.co;2-o},
issn = {0277-6715},
journal = {Statistics in medicine},
month = {dec},
number = {24},
pages = {3435--51},
pmid = {10611617},
title = {{The role of biostatistics in the prevention, detection and treatment of fraud in clinical trials.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10611617},
volume = {18},
year = {1999}
}
@article{Seed2009,
author = {Seed, Mary and Juarez, Magdalena and Alnatour, Ranya},
doi = {10.1111/j.1744-6171.2009.00193.x},
file = {::},
issn = {10736077},
journal = {Journal of Child and Adolescent Psychiatric Nursing},
keywords = {Adolescent,longitudinal study,nursing,patient selection,research},
month = {aug},
number = {3},
pages = {150--153},
publisher = {John Wiley & Sons, Ltd (10.1111)},
title = {{Improving Recruitment and Retention Rates in Preventive Longitudinal Research with Adolescent Mothers}},
url = {http://doi.wiley.com/10.1111/j.1744-6171.2009.00193.x},
volume = {22},
year = {2009}
}
@article{Patel2003,
abstract = {<p>There are many potential pitfalls in the identification and enlistment of suitable candidates for psychiatric research. The challenges of recruitment are highlighted, detailing impact of study design, characteristics of participants, including demographics and personal preferences, investigator characteristics and collaboration with clinicians. Techniques used in recruitment are discussed, including financial incentives, assertive tracking and communication methods. Ethical issues, methods of data collection, and control participants are also considered. Key issues are: early consideration of the impact of study design on the recruitment process; the participant's perspective; close collaboration with colleagues; the investigator's good interpersonal, communication and organisational skills; and feedback to collaborators, associated clinicians and participants.</p>},
author = {Patel, Maxine X. and Doku, Victor and Tennakoon, Lakshika},
doi = {10.1192/apt.9.3.229},
file = {::},
issn = {1355-5146},
journal = {Advances in Psychiatric Treatment},
month = {may},
number = {3},
pages = {229--238},
publisher = {Cambridge University Press},
title = {{Challenges in recruitment of research participants}},
url = {https://www.cambridge.org/core/product/identifier/S1355514600000468/type/journal_article},
volume = {9},
year = {2003}
}
@article{Kahi2013,
abstract = {BACKGROUND
Colonoscopy quality is operator-dependent. Studies assessing the effect of interventions to decrease variation in colonoscopy quality have shown inconsistent results. Since 2009, endoscopists at our university-affiliated, Veterans Affairs medical center have received a quarterly “report card” summarizing individual colonoscopy quality indicators as part of an ongoing quality assurance program. 

OBJECTIVE
To determine the effect of the quality report card intervention on colonoscopy performance. 

DESIGN
Retrospective study. 

SETTING
Tertiary-care, academic, university-affiliated, Veterans Affairs medical center in Indianapolis, Indiana. 

PATIENTS
Data from 6 endoscopists practicing at the Roudebush Veterans Affairs Medical Center were included. Patients were average-risk, aged 50 years or older, undergoing their first screening colonoscopy. 

INTERVENTION
Quarterly report card. The study time frame was July 1, 2008 to December 31, 2008 (before-intervention) and April 1, 2009 to March 31, 2011 (intervention). 

MAIN OUTCOME MEASUREMENTS
The primary outcomes were cecal intubation and adenoma detection rates (ADR), adjusted for physician, patient age, and sex. Multivariable logistic regression was performed to determine factors associated with adenoma detection. 

RESULTS
A total of 928 patients (male 93%, white 78%) were included (before-intervention 336; intervention 592). There were no significant differences in patient age, sex, smoking status, body mass index, bowel preparation quality, colonoscope model, and proportion of colonoscopies performed with a trainee between the before-intervention and intervention phases. In the intervention phase, the adjusted adenoma detection and cecal intubation rates were significantly higher: 53.9% (95% confidence interval [CI], 49.7%-58.1%) vs 44.7% (95% CI, 39.1%-50.4%); P = .013 and 98.1% (95% CI, 96.7%-99.0%) vs 95.6% (95% CI, 92.5%-97.5%); P = .027, respectively. A higher ADR trend in the intervention phase was found for 5 of the 6 physicians. The increment in ADR was due mostly to increased detection of proximal adenomas. There were no significant changes in serrated polyp detection, advanced neoplasm detection, number of adenomas detected per colonoscopy, and mean size of adenomas after implementation of the intervention. The report card intervention remained significantly associated with higher ADRs after adjustment for patient age, sex, and physician (odds ratio 1.45; 95% CI, 1.08-1.94). 

LIMITATIONS
Single center, small number of endoscopists. 

CONCLUSION
A quarterly report card was associated with improved colonoscopy quality indicators. This intervention is practical to generate and implement and may serve as a model for quality improvement programs in different patient and physician groups.},
author = {Kahi, Charles J. and Ballard, Darren and Shah, Anand S. and Mears, Raenita and Johnson, Cynthia S.},
doi = {10.1016/J.GIE.2013.01.012},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Kahi et al. - 2013 - Impact of a quarterly report card on colonoscopy quality measures.pdf:pdf},
issn = {0016-5107},
journal = {Gastrointestinal Endoscopy},
month = {jun},
number = {6},
pages = {925--931},
publisher = {Mosby},
title = {{Impact of a quarterly report card on colonoscopy quality measures}},
url = {https://www.sciencedirect.com/science/article/pii/S001651071300028X},
volume = {77},
year = {2013}
}
@article{Gabriel2014,
author = {Gabriel, Rodney A. and Gimlich, Robert and Ehrenfeld, Jesse M. and Urman, Richard D.},
doi = {10.1007/s10916-014-0144-8},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Gabriel et al. - 2014 - Operating Room Metrics Score Card—Creating a Prototype for Individualized Feedback.pdf:pdf},
issn = {0148-5598},
journal = {Journal of Medical Systems},
month = {nov},
number = {11},
pages = {144},
publisher = {Springer US},
title = {{Operating Room Metrics Score Card—Creating a Prototype for Individualized Feedback}},
url = {http://link.springer.com/10.1007/s10916-014-0144-8},
volume = {38},
year = {2014}
}
@article{Mukamel2003,
author = {Mukamel, Dana B. and Spector, William D.},
doi = {10.1093/geront/43.suppl_2.58},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Mukamel, Spector - 2003 - Quality Report Cards and Nursing Home Quality.pdf:pdf},
issn = {1758-5341},
journal = {The Gerontologist},
month = {apr},
number = {suppl_2},
pages = {58--66},
publisher = {Narnia},
title = {{Quality Report Cards and Nursing Home Quality}},
url = {http://academic.oup.com/gerontologist/article/43/suppl_2/58/637534/Quality-Report-Cards-and-Nursing-Home-Quality},
volume = {43},
year = {2003}
}
@article{Gumbus2005,
abstract = {This experiential exercise presents the concept of the Balanced Scorecard (BSC) and applies it in a university setting. The Balanced Scorecard was developed 12 years ago and has grown in popularity...},
author = {Gumbus, Andra},
doi = {10.1177/1052562905276278},
issn = {1052-5629},
journal = {Journal of Management Education},
keywords = {balanced scorecard,organizational measurement,organizational metrics},
month = {aug},
number = {4},
pages = {617--630},
publisher = {Sage PublicationsSage CA: Thousand Oaks, CA},
title = {{Introducing the Balanced Scorecard: Creating Metrics to Measure Performance}},
url = {http://journals.sagepub.com/doi/10.1177/1052562905276278},
volume = {29},
year = {2005}
}
@article{Blumenthal1998,
author = {Blumenthal, David and Kilo, Charles M.},
doi = {10.1111/1468-0009.00108},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Blumenthal, Kilo - 1998 - A Report Card on Continuous Quality Improvement.pdf:pdf},
issn = {0887-378X},
journal = {The Milbank Quarterly},
month = {dec},
number = {4},
pages = {625--648},
publisher = {John Wiley & Sons, Ltd (10.1111)},
title = {{A Report Card on Continuous Quality Improvement}},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1468-0009.00108},
volume = {76},
year = {1998}
}
@article{Stanley2010a,
abstract = {BACKGROUND
The Pediatric Emergency Care Applied Research Network (PECARN) is a federally funded multi-center research network. To promote high quality research within the network, it is important to establish evaluation tools to measure performance of the research sites. 

PURPOSE
To describe the collaborative development of a site performance measure tool “report card” in an academic pediatric research network. To display report card template information and discuss the successes and challenges of the report cards. 

DEVELOPMENT AND IMPLEMENTATION OF THE NETWORK PERFORMANCE MEASURE TOOL
The PECARN Quality Assurance Subcommittee and the PECARN data center were responsible for the development and implementation of the report cards. Using a Balanced Scorecard format, four key metrics were identified to align with PECARN's research goals. Performance indicators were defined for each of these metrics. After two years of development, the final report cards have been implemented annually since 2005. Protocol submission time to the Institutional Review Board (IRB) improved between 2005 and 2007. Mean overall report card scores for site report cards increased during this period with less variance between highest and lowest performing sites indicating overall improvement. 

CONCLUSIONS
Report cards have helped PECARN sites and investigators focus on performance improvement and may have contributed to improved operations and efficiencies within the network.},
author = {Stanley, Rachel and Lillis, Kathleen A. and Zuspan, Sally Jo and Lichenstein, Richard and Ruddy, Richard M. and Gerardi, Michael J. and Dean, J. Michael},
doi = {10.1016/J.CCT.2010.05.007},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Stanley et al. - 2010 - Development and implementation of a performance measure tool in an academic pediatric research network.pdf:pdf},
issn = {1551-7144},
journal = {Contemporary Clinical Trials},
month = {sep},
number = {5},
pages = {429--437},
publisher = {Elsevier},
title = {{Development and implementation of a performance measure tool in an academic pediatric research network}},
url = {https://www.sciencedirect.com/science/article/pii/S1551714410000935},
volume = {31},
year = {2010}
}
@article{Peccora2014,
author = {Peccora, Christian D. and Gimlich, Robert and Cornell, Richard P. and Vacanti, Charles A. and Ehrenfeld, Jesse M. and Urman, Richard D.},
doi = {10.1007/s10916-014-0105-2},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Peccora et al. - 2014 - Anesthesia Report Card – A Customizable Tool for Performance Improvement.pdf:pdf},
issn = {0148-5598},
journal = {Journal of Medical Systems},
month = {sep},
number = {9},
pages = {105},
publisher = {Springer US},
title = {{Anesthesia Report Card – A Customizable Tool for Performance Improvement}},
url = {http://link.springer.com/10.1007/s10916-014-0105-2},
volume = {38},
year = {2014}
}
@article{Shahian2011,
abstract = {Appropriate implementation is essential to create a credible public reporting system. Ideally, data should be obtained from an audited clinical data registry, and structure, process, or outcomes metrics may be reported. Composite measures are increasingly used, as are measures of appropriateness, patient satisfaction, functional status, and health-related quality of life. Classification of provider performance should use statistical criteria appropriate to the policy objectives and to the desired balance of sensitivity and specificity. Public reports should use simplified visual or tabular presentation aids that maximize correct interpretation of numerical data. Because of sample size issues, and to emphasize that cardiac surgery requires team-based care, public reporting should generally be focused at the program rather than individual surgeon level. This may also help to mitigate risk aversion, the avoidance of high-risk patients.},
author = {Shahian, David M. and Edwards, Fred H. and Jacobs, Jeffrey P. and Prager, Richard L. and Normand, Sharon-Lise T. and Shewan, Cynthia M. and O'Brien, Sean M. and Peterson, Eric D. and Grover, Frederick L.},
doi = {10.1016/J.ATHORACSUR.2011.06.101},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Shahian et al. - 2011 - Public Reporting of Cardiac Surgery Performance Part 2—Implementation.pdf:pdf},
issn = {0003-4975},
journal = {The Annals of Thoracic Surgery},
month = {sep},
number = {3},
pages = {S12--S23},
publisher = {Elsevier},
title = {{Public Reporting of Cardiac Surgery Performance: Part 2—Implementation}},
url = {https://www.sciencedirect.com/science/article/pii/S0003497511016663},
volume = {92},
year = {2011}
}
@article{Palerm2007,
abstract = {OBJECTIVE-We propose a novel algorithm to adjust prandial insulin dose using sparse blood glucose measurements. The dose is adjusted on the basis of a performance measure for the same meal on the previous day. We determine the best performance measure and tune the algorithm to match the recommendations of experienced physicians. RESEARCH DESIGN AND METHODS-Eleven subjects with type 1 diabetes, using continuous subcutaneous insulin infusion, were recruited (seven women and four men, aged 21-65 years with A1C of 7.1 1.3%). Basal insulin infusion rates were optimized. Target carbohydrate content for the lunch meal was calculated on the basis of a weight-maintenance diet. Over a period of 2-4 days, subjects were asked to measure their blood glucose according to the algorithm's protocol. Starting with their usual insulin-to-carbohydrate ratio, the insulin bolus dose was titrated downward until postprandial glucose levels were high (180-250 mg/dl [10-14 mmol/l]). Subsequently, physicians made insulin bolus recommendations to normalize postprandial glucose concentrations. Graphical methods were then used to determine the most appropriate performance measure for the algorithm to match the physician's decisions. For the best performance measure, the gain of the controller was determined to be the best match to the dose recommendations of the physicians. RESULTS-The correlation between the clinically determined dose adjustments and those of the algorithm is R 2 0.95, P 1e 18. CONCLUSIONS-We have shown how engineering methods can be melded with medical expertise to develop and refine a dosing algorithm. This algorithm has the potential of drastically simplifying the determination of correct insulin-to-carbohydrate ratios.},
author = {Palerm, Cesar C and Zisser, Howard and Bevier, Wendy C and Jovanovi{\v{c}}, Lois and Doyle, Francis J},
doi = {10.2337/dc06},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Palerm et al. - 2007 - Prandial Insulin Dosing Using Run-to-Run Control Application of clinical data and medical expertise to define a s.pdf:pdf},
title = {{Prandial Insulin Dosing Using Run-to-Run Control Application of clinical data and medical expertise to define a suitable performance metric}},
url = {http://care.diabetesjournals.org},
year = {2007}
}
@manual{Zhu2018,
annote = {R package version 0.9.0},
author = {Zhu, Hao},
title = {{kableExtra: Construct Complex Table with 'kable' and Pipe Syntax}},
url = {https://cran.r-project.org/package=kableExtra},
year = {2018}
}
@manual{RCoreTeam2018,
address = {Vienna, Austria},
author = {{R Core Team}},
organization = {R Foundation for Statistical Computing},
title = {{R: A Language and Environment for Statistical Computing}},
url = {https://www.r-project.org/},
year = {2018}
}
@manual{Allaire2018,
annote = {R package version 1.10},
author = {Allaire, J J and Xie, Yihui and McPherson, Jonathan and Luraschi, Javier and Ushey, Kevin and Atkins, Aron and Wickham, Hadley and Cheng, Joe and Chang, Winston},
title = {{rmarkdown: Dynamic Documents for R}},
url = {https://cran.r-project.org/package=rmarkdown},
year = {2018}
}
@book{Wickham2016,
author = {Wickham, Hadley},
isbn = {978-3-319-24277-4},
publisher = {Springer-Verlag New York},
title = {{ggplot2: Elegant Graphics for Data Analysis}},
url = {http://ggplot2.org},
year = {2016}
}
@article{Demidenko2009,
abstract = {Although sample size calculations have become an important element in the design of research projects, such methods for studies involving current status data are scarce. Here, we propose a method for calculating power and sample size for studies using current status data. This method is based on a Weibull survival model for a two-group comparison. The Weibull model allows the investigator to specify a group difference in terms of a hazards ratio or a failure time ratio. We consider exponential, Weibull and uniformly distributed censoring distributions. We base our power calculations on a parametric approach with the Wald test because it is easy for medical investigators to conceptualize and specify the required input variables. As expected, studies with current status data have substantially less power than studies with the usual right-censored failure time data. Our simulation results demonstrate the merits of these proposed power calculations.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Demidenko, Eugene},
doi = {10.1002/sim},
eprint = {NIHMS150003},
file = {:Users/jferreira-admin/Downloads/Demidenko-2007-Statistics_in_Medicine.pdf:pdf},
isbn = {2007090091480},
issn = {02776715},
journal = {Statistics in Medicine},
keywords = {ces-d,conditional maximum likelihood,fixed effects,generalized linear mixed model,hausman test,linear mixed model,random effects,robust,variance},
number = {December 2006},
pages = {1487--1497},
pmid = {19455509},
title = {{Sample size determination for logistic regression revisited}},
volume = {28},
year = {2009}
}
@article{Nunez-Anton1994,
author = {Nunez-Anton, Vicente and Woodworth, George G.},
doi = {10.2307/2533387},
issn = {0006341X},
journal = {Biometrics},
month = {jun},
number = {2},
pages = {445},
title = {{Analysis of Longitudinal Data with Unequally Spaced Observations and Time- Dependent Correlated Errors}},
url = {https://www.jstor.org/stable/2533387?origin=crossref},
volume = {50},
year = {1994}
}
@article{Huang2018,
abstract = {Patient recruitment is widely recognized as a key determinant of success for clinical trials. Yet a substantial number of trials fail to reach recruitment goals—a situation that has important scientific, financial, ethical, and policy implications. Further, there are important effects on stakeholders who directly contribute to the trial including investigators, sponsors, and study participants. Despite efforts over multiple decades to identify and address barriers, recruitment challenges persist. To advance a more comprehensive approach to trial recruitment, the Clinical Trials Transformation Initiative (CTTI) convened a project team to examine the challenges and to issue actionable, evidence-based recommendations for improving recruitment planning that extend beyond common study-specific strategies. We describe our multi-stakeholder effort to develop a framework that delineates three areas essential to strategic recruitment planning efforts: (1) trial design and protocol development, (2) trial feasibility and site selection, and (3) communication. Our recommendations propose an upstream approach to recruitment planning that has the potential to produce greater impact and reduce downstream barriers. Additionally, we offer tools to help facilitate adoption of the recommendations. We hope that our framework and recommendations will serve as a guide for initial efforts in clinical trial recruitment planning irrespective of disease or intervention focus, provide a common basis for discussions in this area and generate targets for further analysis and continual improvement.},
author = {Huang, Grant D. and Bull, Jonca and {Johnston McKee}, Kelly and Mahon, Elizabeth and Harper, Beth and Roberts, Jamie N.},
doi = {10.1016/J.CCT.2018.01.003},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Huang et al. - 2018 - Clinical trials recruitment planning A proposed framework from the Clinical Trials Transformation Initiative.pdf:pdf},
issn = {1551-7144},
journal = {Contemporary Clinical Trials},
month = {mar},
pages = {74--79},
publisher = {Elsevier},
title = {{Clinical trials recruitment planning: A proposed framework from the Clinical Trials Transformation Initiative}},
url = {https://www.sciencedirect.com/science/article/pii/S155171441730753X},
volume = {66},
year = {2018}
}
@article{Goldfarb2011,
author = {Goldfarb, By Norman M},
file = {:Users/jferreira-admin/Downloads/1102_Site_Metrics.pdf:pdf},
number = {2},
title = {{“ Can You Handle the Truth ?” Driving the Performance of a Clinical Research Site with Metrics}},
volume = {7},
year = {2011}
}
@article{Shahian2001,
abstract = {Public report cards and confidential, collaborative peer education represent distinctly different approaches to cardiac surgery quality assessment and improvement. This review discusses the controversies regarding their methodology and relative effectiveness. Report cards have been the more commonly used approach, typically as a result of state legislation. They are based on the presumption that publication of outcomes effectively motivates providers, and that market forces will reward higher quality. Numerous studies have challenged the validity of these hypotheses. Furthermore, although states with report cards have reported significant decreases in risk-adjusted mortality, it is unclear whether this improvement resulted from public disclosure or, rather, from the development of internal quality programs by hospitals. An additional confounding factor is the nationwide decline in heart surgery mortality, including states without quality monitoring. Finally, report cards may engender negative behaviors such as high-risk case avoidance and "gaming" of the reporting system, especially if individual surgeon results are published. The alternative approach, continuous quality improvement, may provide an opportunity to enhance performance and reduce interprovider variability while avoiding the unintended negative consequences of report cards. This collaborative method, which uses exchange visits between programs and determination of best practice, has been highly effective in northern New England and in the Veterans Affairs Administration. However, despite their potential advantages, quality programs based solely on confidential continuous quality improvement do not address the issue of public accountability. For this reason, some states may continue to mandate report cards. In such instances, it is imperative that appropriate statistical techniques and report formats are used, and that professional organizations simultaneously implement continuous quality improvement programs. The statistical methodology underlying current report cards is flawed, and does not justify the degree of accuracy presented to the public. All existing risk-adjustment methods have substantial inherent imprecision, and this is compounded when the results of such patient-level models are aggregated and used inappropriately to assess provider performance. Specific problems include sample size differences, clustering of observations, multiple comparisons, and failure to account for the random component of interprovider variability. We advocate the use of hierarchical or multilevel statistical models to address these concerns, as well as report formats that emphasize the statistical uncertainty of the results. {\textcopyright} 2001 by The Society of Thoracic Surgeons.},
author = {Shahian, David M. and Normand, Sharon Lise and Torchiana, David F. and Lewis, Stanley M. and Pastore, John O. and Kuntz, Richard E. and Dreyer, Paul I.},
doi = {10.1016/S0003-4975(01)03222-2},
file = {:Users/jferreira-admin/Downloads/PIIS0003497501032222.pdf:pdf},
issn = {00034975},
journal = {Annals of Thoracic Surgery},
number = {6},
pages = {2155--2168},
title = {{Cardiac surgery report cards: Comprehensive review and statistical critique}},
volume = {72},
year = {2001}
}
@article{Landwehrmeyer2017,
abstract = {Background The study of complex neurodegenerative diseases is moving away from hypothesis-driven biological methods toward large scale multimodal approaches, requiring standardized collaborative efforts. Enroll-HD exemplifies such an integrated clinical research platform, designed and implemented to meet the research and clinical needs of Huntington's disease (HD). The aim of this study was to describe the unique organization of Enroll-HD and report baseline data analyses of its core study. Methods The Enroll-HD platform incorporates electronic data capture, biosampling, and a longitudinal observational study spanning four continents (ClinicalTrials.gov Identifier: NCT01574053). The primary study population includes HD gene expansion carriers (HDGECs; CAG expansion ≥36), subdivided into manifest/premanifest HD. The control population consists of genotype-negative first-degree relatives and family controls not genetically related. The study includes 10 core clinical assessments covering motor, cognitive, and behavioral domains. Results This data set comprises 1,534 participants (HDGEC = 1,071; controls = 463). Participant retention was high; 42 participants prematurely withdrew from the study. Mean ± standard deviation SD CAG repeat size was 43.5 ± 3.5 for HDGECs and 19.8 ± 3.4 for controls. Motor and behavioral assessments identified numerical differences between controls and HDGECs (manifest > premanifest > controls). Functional and independence assessments were generally similar for the premanifest and control groups with overlap in range of scores obtained. For the majority of cognitive tests, there were large differences between participants with manifest HD and all other groups. Conclusions These first data from the Enroll-HD clinical research platform demonstrate the maturity and potential of the platform in collecting high-quality, clinically relevant data. Future data sets will be substantially larger as the platform expands longitudinally and regionally.},
author = {Landwehrmeyer, Georg B. and Fitzer-Attas, Cheryl J. and Giuliano, Joseph D. and Gon{\c{c}}alves, Nilza and Anderson, Karen E. and Cardoso, Francisco and Ferreira, Joaquim J. and Mestre, Tiago A. and Stout, Julie C. and Sampaio, Cristina},
doi = {10.1002/mdc3.12388},
issn = {23301619},
journal = {Movement Disorders Clinical Practice},
month = {mar},
number = {2},
pages = {212--224},
pmid = {30363395},
title = {{Data Analytics from Enroll-HD, a Global Clinical Research Platform for Huntington's Disease}},
volume = {4},
year = {2017}
}
@manual{R-markdown,
annote = {R package version 0.8},
author = {Allaire, J J and Horner, Jeffrey and Marti, Vicent and Porte, Natacha},
title = {{markdown: 'Markdown' Rendering for R}},
url = {https://cran.r-project.org/package=markdown},
year = {2017}
}
@article{HuntingtonStudyGroupCOHORTInvestigators2012,
abstract = {BACKGROUND Careful characterization of the phenotype and genotype of Huntington disease (HD) can foster better understanding of the condition. METHODS We conducted a cohort study in the United States, Canada, and Australia of members of families affected by HD. We collected demographic and clinical data, conducted the Unified Huntington's Disease Rating Scale and Mini-Mental State Examination, and determined Huntingtin trinucleotide CAG repeat length. We report primarily on cross-sectional baseline data from this recently completed prospective, longitudinal, observational study. RESULTS As of December 31, 2009, 2,318 individuals enrolled; of these, 1,985 (85.6%) were classified into six analysis groups. Three groups had expanded CAG alleles (36 repeats or more): individuals with clinically diagnosed HD [n = 930], and clinically unaffected first-degree relatives who had previously pursued [n = 248] or not pursued [n = 112] predictive DNA testing. Three groups lacked expanded alleles: first-degree relatives who had previously pursued [n = 41] or not pursued [n = 224] genetic testing, and spouses and caregivers [n = 430]. Baseline mean performance differed across groups in all motor, behavioral, cognitive, and functional measures (p<0.001). Clinically unaffected individuals with expanded alleles weighed less (76.0 vs. 79.6 kg; p = 0.01) and had lower cognitive scores (28.5 vs. 29.1 on the Mini Mental State Examination; p = 0.008) than individuals without expanded alleles. The frequency of "high normal" repeat lengths (27 to 35) was 2.5% and repeat lengths associated with reduced penetrance (36 to 39) was 2.7%. CONCLUSION Baseline analysis of COHORT study participants revealed differences that emerge prior to clinical diagnosis. Longitudinal investigation of this cohort will further characterize the natural history of HD and genetic and biological modifiers. TRIAL REGISTRATION Clinicaltrials.gov NCT00313495.},
author = {{Huntington Study Group COHORT Investigators}, E. Ray and Dorsey, E Ray},
doi = {10.1371/journal.pone.0029522},
editor = {Lewin, Alfred},
issn = {1932-6203},
journal = {PloS one},
month = {feb},
number = {2},
pages = {e29522},
pmid = {22359536},
title = {{Characterization of a large group of individuals with huntington disease and their relatives enrolled in the COHORT study.}},
volume = {7},
year = {2012}
}
@unpublished{U.S.FoodandDrugAsministrationFDA2013,
author = {{U.S. Food and Drug Asministration (FDA)}},
issn = {0910-0733},
title = {{Guidance for industry: oversight of clinical investigations—a risk-based approach to monitoring. 2013. Available at: http://www.fda.gov/downloads/Drugs/Guidance ComplianceRegulatoryInformation/Guidances/UCM269919.pdf}},
year = {2013}
}
@misc{mmm,
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - markdown_bib.tex:tex},
title = {markdown_bib}
}
@article{Orth2011,
abstract = {BACKGROUND Huntington's disease (HD) is a rare triplet repeat (CAG) disorder. Advanced, multi-centre, multi-national research frameworks are needed to study simultaneously multiple complementary aspects of HD. This includes the natural history of HD, its management and the collection of clinical information and biosamples for research. METHODS We report on cross-sectional data of the first 1766 participants in REGISTRY, the European Huntington's Disease Network's (EHDN), multi-lingual, multi-national prospective observational study of HD in Europe. Data collection (demographics, phenotype, genotype, medication, co-morbidities, biosamples) followed a standard protocol. RESULTS Phenotype, and the HD genotype, of manifest HD participants across different European regions was similar. Motor onset was most common (48%) with a non-motor onset in more than a third of participants. Motor signs increased, and cognitive abilities and functional capacity declined as the disease burden (CAGn-35.5) X age) increased. A life-time history of behavioural symptoms was common, but the behavioural score was not related to disease burden. One fifth of participants had severe psychiatric problems, e.g. suicidal ideation and attempts, and/or irritability/aggression, with psychosis being less common. Participants on anti-dyskinetic medication had a higher motor and lower cognitive score, were older, and more prone to physical trauma. A higher motor and a lower cognitive score predicted more advanced disease. CONCLUSIONS The unparalleled collection of clinical data and biomaterials within the EHDN's REGISTRY can expedite the search for disease modifiers (genetic and environmental) of age at onset and disease progression that could be harnessed for the development of novel treatments.},
author = {Orth, Michael and Handley, Olivia J and Schwenke, Carsten and Dunnett, Stephen B. and Craufurd, David and Ho, Aileen K and Wild, Edward and Tabrizi, Sarah J and Landwehrmeyer, G. Bernhard and {Investigators of the European Huntington's Disease Network}},
doi = {10.1371/currents.RRN1184},
issn = {2157-3999},
journal = {PLoS Currents},
month = {apr},
pages = {RRN1184},
pmid = {20890398},
title = {{Observing Huntington's Disease: the European Huntington's Disease Network's REGISTRY}},
volume = {2},
year = {2011}
}
@article{Walker2018,
author = {Walker, Kate F and Turzanski, Julie and Whitham, Diane and Montgomery, Alan and Duley, Lelia},
file = {:Users/jferreira-admin/Downloads/13063_2018_Article_2941.pdf:pdf},
isbn = {1306301829418},
keywords = {Multicentre,Randomised trials,Clinical trials,Perf,clinical trials,key performance indicators,multicentre,operational metrics,performance metrics,randomised trials,site performance,trial management},
pages = {1--9},
publisher = {Trials},
title = {{Monitoring performance of sites within multicentre randomised trials : a systematic review of performance metrics}},
year = {2018}
}
@article{Berthon-jones2015,
author = {Berthon-jones, Nisha and Courtney-vega, Kymme and Donaldson, Anna and Haskelberg, Hila and Emery, Sean and Puls, Rebekah},
doi = {10.1186/s13063-015-0653-x},
file = {:Users/jferreira-admin/Downloads/13063_2015_Article_653.pdf:pdf},
issn = {???},
journal = {???},
keywords = {Site performance metrics,Altair,Multinational rand,altair,au,benchmarking performance,clinical,clinical trial data quality,clinical trial start-up,clinical trials,correspondence,edu,kirby,laboratory sample quality in,multinational randomised clinical trial,nberthon-jones,operational metrics,participant,recruitment,site performance metrics,trial performance metrics,unsw},
pages = {1--10},
publisher = {???},
title = {{Assessing site performance in the Altair study , a multinational clinical trial}},
url = {???},
year = {2015}
}
@article{Whitham2018,
author = {Whitham, Diane and Turzanski, Julie and Bradshaw, Lucy and Clarke, Mike and Culliford, Lucy and Duley, Lelia and Shaw, Lisa and Skea, Zoe and Treweek, Shaun P and Walker, Kate and Williamson, Paula R and Montgomery, Alan A},
file = {:Users/jferreira-admin/Downloads/document.pdf:pdf},
keywords = {Multicentre randomised trials,Performance metrics,,ac,alan,consensus meeting,correspondence,delphi survey,montgomery,multicentre randomised trials,nottingham,performance metrics,trial management,uk},
pages = {1--11},
publisher = {Trials},
title = {{Development of a standardised set of metrics for monitoring site performance in multicentre randomised trials : a Delphi study}},
year = {2018}
}
@article{Walker2018a,
author = {Walker, Kate F and Turzanski, Julie and Whitham, Diane and Montgomery, Alan and Duley, Lelia},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Walker et al. - 2018 - Monitoring performance of sites within multicentre randomised trials a systematic review of performance metrics.pdf:pdf},
isbn = {1306301829418},
keywords = {Multicentre,Randomised trials,Clinical trials,Perf,clinical trials,key performance indicators,multicentre,operational metrics,performance metrics,randomised trials,site performance,trial management},
pages = {1--9},
publisher = {Trials},
title = {{Monitoring performance of sites within multicentre randomised trials : a systematic review of performance metrics}},
year = {2018}
}
@article{Berthon-jones2015a,
author = {Berthon-jones, Nisha and Courtney-vega, Kymme and Donaldson, Anna and Haskelberg, Hila and Emery, Sean and Puls, Rebekah},
doi = {10.1186/s13063-015-0653-x},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Berthon-jones et al. - 2015 - Assessing site performance in the Altair study , a multinational clinical trial.pdf:pdf},
issn = {???},
journal = {???},
keywords = {Site performance metrics,Altair,Multinational rand},
number = {April},
publisher = {???},
title = {{Assessing site performance in the Altair study , a multinational clinical trial}},
url = {???},
year = {2015}
}
@article{Stanley2010,
author = {Stanley, Rachel and Lillis, Kathleen A and Jo, Sally and Lichenstein, Richard and Ruddy, Richard M and Gerardi, Michael J and Dean, J Michael and Care, Emergency and Pecarn, Network},
doi = {10.1016/j.cct.2010.05.007},
file = {:Users/jferreira-admin/Desktop/CHDI/papers/Reportcard.pdf:pdf},
issn = {1551-7144},
journal = {Contemporary Clinical Trials},
keywords = {performance measures},
number = {5},
pages = {429--437},
publisher = {Elsevier Inc.},
title = {{Development and implementation of a performance measure tool in an academic pediatric research network}},
url = {http://dx.doi.org/10.1016/j.cct.2010.05.007},
volume = {31},
year = {2010}
}
@article{Whitham2018a,
author = {Whitham, Diane and Turzanski, Julie and Bradshaw, Lucy and Clarke, Mike and Culliford, Lucy and Duley, Lelia and Shaw, Lisa and Skea, Zoe and Treweek, Shaun P and Walker, Kate and Williamson, Paula R and Montgomery, Alan A},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Whitham et al. - 2018 - Development of a standardised set of metrics for monitoring site performance in multicentre randomised trials a.pdf:pdf},
keywords = {Multicentre randomised trials,Performance metrics,,ac,alan,consensus meeting,correspondence,delphi survey,montgomery,multicentre randomised trials,nottingham,performance metrics,trial management,uk},
pages = {1--11},
publisher = {Trials},
title = {{Development of a standardised set of metrics for monitoring site performance in multicentre randomised trials : a Delphi study}},
year = {2018}
}
@article{Bedding2016,
author = {Bedding, Alun},
file = {:Users/jferreira-admin/Desktop/CHDI/papers/PCA/Centralized Statistical Monitoring to Detect Data Integrity Issues - Alun Bedding.pdf:pdf},
journal = {PhUSE},
number = {July},
title = {{Centralized Statistical Monitoring (CSM) to Detect Data Integrity Issues Alun Bedding}},
year = {2016}
}
@article{Moye2006,
author = {Moy{\'{e}}, Lemuel A.},
doi = {10.1007/0-387-27782-X},
file = {:Users/jferreira-admin/Desktop/CHDI/papers/PCA/fraud detection.pdf:pdf},
isbn = {0-387-27781-1},
keywords = {icle},
title = {{Statistical Monitoring of Clinical Trials}},
url = {http://link.springer.com/10.1007/0-387-27782-X},
volume = {1},
year = {2006}
}
@article{Weir2011,
abstract = {Fraud is surprisingly hard to conceal. If you make up or alter the data in a clinical trial you will leave a trail that a good statistician can follow. Christopher Weir and Gordon Murray tell would-be detectives where the clues will be.},
author = {Weir, Christopher and Murray, Gordon},
doi = {10.1111/j.1740-9713.2011.00521.x},
file = {:Users/jferreira-admin/Desktop/j.1740-9713.2011.00521.x.pdf:pdf},
isbn = {1740-9713},
issn = {17409705},
journal = {Significance},
number = {4},
pages = {164--168},
title = {{Fraud in clinical trials}},
url = {http://doi.wiley.com/10.1111/j.1740-9713.2011.00521.x},
volume = {8},
year = {2011}
}
@book{Belle,
author = {Belle, Gerald Van},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Belle - Unknown - Statistical Rules.pdf:pdf},
isbn = {9780470144480},
title = {{Statistical Rules}}
}
@article{Kilani2012,
author = {Kilani, Karim},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Kilani - 2012 - Keywords.pdf:pdf},
keywords = {additive random utility models,best-worst choice,gev,inclusion-exclusion principle,logit,probabilistic choice},
title = {{Keywords :}},
year = {2012}
}
@article{Timmermans2016,
abstract = {Data quality may impact the outcome of clinical trials; hence, there is a need to implement quality control strategies for the data collected. Traditional approaches to quality control have primarily used source data verification during on-site monitoring visits, but these approaches are hugely expensive as well as ineffective. There is growing interest in central statistical monitoring (CSM) as an effective way to ensure data quality and consistency in multicenter clinical trials. Methods CSM with SMART™ uses advanced statistical tools that help identify centers with atypical data patterns which might be the sign of an underlying quality issue. This approach was used to assess the quality and consistency of the data collected in the Stomach Cancer Adjuvant Multi-institutional Trial Group Trial, involving 1495 patients across 232 centers in Japan. Results In the Stomach Cancer Adjuvant Multi-institutional Trial Group Trial, very few atypical data patterns were found among the participating centers, and none of these patterns were deemed to be related to a quality issue that could significantly affect the outcome of the trial. Discussion CSM can be used to provide a check of the quality of the data from completed multicenter clinical trials before analysis, publication, and submission of the results to regulatory agencies. It can also form the basis of a risk-based monitoring strategy in ongoing multicenter trials. CSM aims at improving data quality in clinical trials while also reducing monitoring costs.},
author = {Timmermans, Catherine and Doffagne, Erik and Venet, David and Desmet, Lieven and Legrand, Catherine and Burzykowski, Tomasz and Buyse, Marc},
doi = {10.1007/s10120-015-0533-9},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Timmermans et al. - 2016 - Statistical monitoring of data quality and consistency in the Stomach Cancer Adjuvant Multi-institutional (3).pdf:pdf},
isbn = {1012001505339},
issn = {14363305},
journal = {Gastric Cancer},
keywords = {Clinical trials,Multicenter study,Quality control,Risk management,Stomach neoplasm},
number = {1},
pages = {24--30},
pmid = {26298185},
publisher = {Springer Japan},
title = {{Statistical monitoring of data quality and consistency in the Stomach Cancer Adjuvant Multi-institutional Trial Group Trial}},
volume = {19},
year = {2016}
}
@article{Baigent2008,
abstract = {Errors in the design, the conduct, the data collection process, and the analysis of a randomized trial have the potential to affect not only the safety of the patients in the trial, but also, through the introduction of bias, the safety of future patients. Trial monitoring, defined broadly to include methods of oversight which begin when the study is designed and continue until it is reported in a publication, has a role to play in eliminating such errors. On-site monitoring can be extremely inefficient for the identification of errors most likely to compromise patient safety or bias study results. However, a variety of other monitoring strategies offer alternatives to on-site monitoring. Each new trial should conduct a risk assessment to identify the optimal means of monitoring, taking into account the likely sources of error, their consequences for patients, the study's validity, and the available resources. Trial management committees should consider central statistical monitoring a key aspect of such monitoring. The systematic application of this approach would be likely to lead to tangible benefits, and resources that are currently wasted on inefficient on-site monitoring could be diverted to increasing trial sample sizes or conducting more trials.},
author = {Baigent, Colin and Harrell, Frank E. and Buyse, Marc and Emberson, Jonathan R. and Altman, Douglas G.},
doi = {10.1177/1740774507087554},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Baigent et al. - 2008 - Ensuring trial validity by data quality assurance and diversification of monitoring methods(3).pdf:pdf},
isbn = {1740774507},
issn = {17407745},
journal = {Clinical Trials},
number = {1},
pages = {49--55},
pmid = {18283080},
title = {{Ensuring trial validity by data quality assurance and diversification of monitoring methods}},
volume = {5},
year = {2008}
}
@article{Greenacre2014,
abstract = {The problem of outliers is well-known in statistics: an outlier is a value that is far from the general distribution of the other observed values, and can often perturb the results of a statistical analysis. Various procedures exist for identifying outliers, in case they need to receive special treatment, which in some cases can be exclusion from consideration. An inlier, by contrast, is an observation lying within the general distribution of other observed values, generally does not perturb the results but is nevertheless non-conforming and unusual. For single variables, an inlier is practically impossible to identify, but in the multivariate case, thanks to interrelationships between variables, values can be identified that are observed to be more central in a distribution but would be expected, based on the other information in the data matrix, to be more outlying. We propose an approach to identify inliers in a data matrix, based on the singular value decomposition. An application is presented using a table of economic indicators for the 27 member countries of the European Union in 2011, where inlying values are identified for some countries such as Estonia and Luxembourg.},
author = {Greenacre, Michael and Ayhan, H {\"{O}}zta},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Greenacre, Ayhan - 2014 - Identifying Inliers Identifying inliers(2).pdf:pdf},
keywords = {imputation,inlier,outlier,singular value decomposition},
number = {June},
pages = {1--11},
title = {{Identifying Inliers Identifying inliers}},
year = {2014}
}
@article{Kirkwood2011,
abstract = {Clinical trial sponsors are required to set up appropriate measures to monitor the conduct of the trial. The aim of monitoring is to ensure the patients' well being, compliance with the approved protocol and regulatory requirements, as well as data accuracy and completeness. Classical monitoring approaches that rely on on-site visits are useful for some of these purposes, but extensive source data verification is extremely time consuming and may have only a limited impact on data quality. It is therefore not surprising that the current practice of performing intensive on-site monitoring is coming into question and that interest focuses on more pragmatic, risk- based approaches that improve the cost- effectiveness ratio without compromising the quality and integrity of clinical trials. A recent draft guidance of the Food and Drug Administration (FDA) reflects this trend and states unequivocally: "FDA encourages greater reliance on centralized monitoring practices than has been the case historically, with correspondingly less emphasis on on-site monitoring". In this presentation, we first review the potential sources of data errors in clinical trials. We then outline the principles of central statistical monitoring and the challenges of its implementation in actual trials. Results from both terminated and on-going trials are presented to illustrate typical findings that can be expected from the monitoring approach. We conclude by a discussion of the potential role and limitations of central statistical monitoring, and we argue that it can both optimize on-site monitoring and improve the quality of clinical trial data.},
author = {Kirkwood, Amy a and Hackshaw, Allan},
doi = {10.1186/1745-6215-12-S1-A55},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Kirkwood, Hackshaw - 2011 - Central statistical monitoring in clinical trials(2).pdf:pdf},
isbn = {1745621512},
issn = {1745-6215},
journal = {Trials},
number = {Suppl 1},
pages = {A55},
publisher = {BioMed Central Ltd},
title = {{Central statistical monitoring in clinical trials}},
url = {http://www.trialsjournal.com/content/12/S1/A55},
volume = {12},
year = {2011}
}
@article{Timmermans2016a,
abstract = {Data quality may impact the outcome of clinical trials; hence, there is a need to implement quality control strategies for the data collected. Traditional approaches to quality control have primarily used source data verification during on-site monitoring visits, but these approaches are hugely expensive as well as ineffective. There is growing interest in central statistical monitoring (CSM) as an effective way to ensure data quality and consistency in multicenter clinical trials. Methods CSM with SMART™ uses advanced statistical tools that help identify centers with atypical data patterns which might be the sign of an underlying quality issue. This approach was used to assess the quality and consistency of the data collected in the Stomach Cancer Adjuvant Multi-institutional Trial Group Trial, involving 1495 patients across 232 centers in Japan. Results In the Stomach Cancer Adjuvant Multi-institutional Trial Group Trial, very few atypical data patterns were found among the participating centers, and none of these patterns were deemed to be related to a quality issue that could significantly affect the outcome of the trial. Discussion CSM can be used to provide a check of the quality of the data from completed multicenter clinical trials before analysis, publication, and submission of the results to regulatory agencies. It can also form the basis of a risk-based monitoring strategy in ongoing multicenter trials. CSM aims at improving data quality in clinical trials while also reducing monitoring costs.},
author = {Timmermans, Catherine and Doffagne, Erik and Venet, David and Desmet, Lieven and Legrand, Catherine and Burzykowski, Tomasz and Buyse, Marc},
doi = {10.1007/s10120-015-0533-9},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Timmermans et al. - 2016 - Statistical monitoring of data quality and consistency in the Stomach Cancer Adjuvant Multi-institutional (2).pdf:pdf},
isbn = {1012001505339},
issn = {14363305},
journal = {Gastric Cancer},
keywords = {Clinical trials,Multicenter study,Quality control,Risk management,Stomach neoplasm},
number = {1},
pages = {24--30},
pmid = {26298185},
publisher = {Springer Japan},
title = {{Statistical monitoring of data quality and consistency in the Stomach Cancer Adjuvant Multi-institutional Trial Group Trial}},
volume = {19},
year = {2016}
}
@article{Baigent2008a,
abstract = {Errors in the design, the conduct, the data collection process, and the analysis of a randomized trial have the potential to affect not only the safety of the patients in the trial, but also, through the introduction of bias, the safety of future patients. Trial monitoring, defined broadly to include methods of oversight which begin when the study is designed and continue until it is reported in a publication, has a role to play in eliminating such errors. On-site monitoring can be extremely inefficient for the identification of errors most likely to compromise patient safety or bias study results. However, a variety of other monitoring strategies offer alternatives to on-site monitoring. Each new trial should conduct a risk assessment to identify the optimal means of monitoring, taking into account the likely sources of error, their consequences for patients, the study's validity, and the available resources. Trial management committees should consider central statistical monitoring a key aspect of such monitoring. The systematic application of this approach would be likely to lead to tangible benefits, and resources that are currently wasted on inefficient on-site monitoring could be diverted to increasing trial sample sizes or conducting more trials.},
author = {Baigent, Colin and Harrell, Frank E. and Buyse, Marc and Emberson, Jonathan R. and Altman, Douglas G.},
doi = {10.1177/1740774507087554},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Baigent et al. - 2008 - Ensuring trial validity by data quality assurance and diversification of monitoring methods(2).pdf:pdf},
isbn = {1740774507},
issn = {17407745},
journal = {Clinical Trials},
number = {1},
pages = {49--55},
pmid = {18283080},
title = {{Ensuring trial validity by data quality assurance and diversification of monitoring methods}},
volume = {5},
year = {2008}
}
@article{Revealed,
author = {Revealed, Risk-based Monitoring and Assessment, Objective and Quality, Data},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Revealed, Assessment, Quality - Unknown - Fran{\c{c}}ois Torche(2).pdf:pdf},
title = {{Fran{\c{c}}ois Torche}}
}
@article{Carter2005,
abstract = {Adequate participant recruitment is vital to the conduct of a clinical trial. Projected recruitment rates are often over-estimated, and the time to recruit the target population (accrual period) is often under-estimated. This report illustrates three approaches to estimating the accrual period and applies the methods to a multi-center, randomized, placebo controlled trial undergoing development. Incorporating known sources of accrual variation can yield a more justified estimate of the accrual period. Simulation studies can be incorporated into a clinical trial's planning phase to provide estimates for key accrual summaries including the mean and standard deviation of the accrual period. The accrual period of a clinical trial should be carefully considered, and the allocation of sufficient time for participant recruitment is a fundamental aspect of planning a clinical trial.},
author = {Carter, Rickey E. and Sonne, Susan C. and Brady, Kathleen T.},
doi = {10.1186/1471-2288-5-11},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Carter, Sonne, Brady - 2005 - Practical considerations for estimating clinical trial accrual periods Application to a multi-center effec.pdf:pdf},
isbn = {1471-2288 (Electronic)\r1471-2288 (Linking)},
issn = {14712288},
journal = {BMC Medical Research Methodology},
pages = {1--5},
pmid = {15796782},
title = {{Practical considerations for estimating clinical trial accrual periods: Application to a multi-center effectiveness study}},
volume = {5},
year = {2005}
}
@article{George2015,
abstract = {Highly publicized cases of fabrication or falsification of data in clinical trials have occurred in recent years and it is likely that there are additional undetected or unreported cases. We review the available evidence on the incidence of data fraud in clinical trials, describe several prominent cases, present information on motivation and contributing factors and discuss cost-effective ways of early detection of data fraud as part of routine central statistical monitoring of data quality. Adoption of these clinical trial monitoring procedures can identify potential data fraud not detected by conventional on-site monitoring and can improve overall data quality.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {George, Stephen L and Buyse, Marc},
doi = {10.4155/CLI.14.116},
eprint = {arXiv:1011.1669v3},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/George, Buyse - 2015 - Data fraud in clinical trials.pdf:pdf},
isbn = {9788578110796},
issn = {2041-6792},
journal = {Clin. Invest. (Lond.)},
keywords = {central statistical monitoring,data fabrication and falsification,data fraud},
number = {2},
pages = {161--173},
pmid = {25729561},
title = {{Data fraud in clinical trials}},
volume = {5},
year = {2015}
}
@article{Diez2012,
abstract = {1. knowledgable about the basics of survival analysis, 2. familiar with vectors, matrices, data frames, lists, plotting, and linear models in R, and 3. interested in applying survival analysis in R. This guide emphasizes the survival package 1 in R 2 . Following very brief introductions to material, functions are introduced to apply the methods. A few short supplemental functions have been written and are available in the OIsurv package 3 , and data sets from the KMsurv package 4 are also used. This guide may be a particularly helpful supplement for Klein and Moeschberger's book, with which KMsurv is associated. Ideally, this survival analysis document would be printed front-to-back and bound like a book. No topics run over two pages. Those that are two pages start on an even page, preventing the need to flip between pages for a single topic. All sample code may be run provided the OIsurv package is loaded, which automatically loads the survival and KMsurv packages. Details about installing and loading the OIsurv package are described in the first section, which discusses R packages.},
author = {Diez, David M},
doi = {10.1198/tech.2004.s740},
journal = {OpenIntro},
number = {February},
pages = {1--16},
title = {{Survival Analysis in R}},
year = {2012}
}
@article{Diez2013,
abstract = {1. knowledgable about the basics of survival analysis, 2. familiar with vectors, matrices, data frames, lists, plotting, and linear models in R, and 3. interested in applying survival analysis in R. This guide emphasizes the survival package 1 in R 2 . Following very brief introductions to material, functions are introduced to apply the methods. A few short supplemental functions have been written and are available in the OIsurv package 3 , and data sets from the KMsurv package 4 are also used. This guide may be a particularly helpful supplement for Klein and Moeschberger's book, with which KMsurv is associated. Ideally, this survival analysis document would be printed front-to-back and bound like a book. No topics run over two pages. Those that are two pages start on an even page, preventing the need to flip between pages for a single topic. All sample code may be run provided the OIsurv package is loaded, which automatically loads the survival and KMsurv packages. Details about installing and loading the OIsurv package are described in the first section, which discusses R packages.},
author = {Diez, David M},
doi = {10.1198/tech.2004.s740},
journal = {None},
number = {June},
pages = {1--16},
title = {{Survival Analysis in R}},
year = {2013}
}
@article{Nass2009,
author = {Nass, Sharyl J and Levit, Laura A and Gostin, Lawrence O and Rule, Institute of Medicine (US) Committee on Health Research and the Privacy of Health
 Information: The HIPAA Privacy},
publisher = {National Academies Press (US)},
title = {{The Value and Importance of Health Information Privacy}},
url = {https://www.ncbi.nlm.nih.gov/books/NBK9579/},
year = {2009}
}
@misc{Gregory,
author = {Gregory, Jennifer},
title = {{Patient health monitoring: Improving quality of care with real-time analytics | IBM Big Data & Analytics Hub}},
url = {http://www.ibmbigdatahub.com/blog/patient-health-monitoring-improving-quality-care-real-time-analytics},
urldate = {2018-01-14}
}
@misc{cccccc,
title = {{The Cost of Sequencing a Human Genome - National Human Genome Research Institute (NHGRI)}},
url = {https://www.genome.gov/27565109/the-cost-of-sequencing-a-human-genome/},
urldate = {2018-01-14}
}
@misc{McMullan,
author = {McMullan, Dawn},
title = {{Genome | What Is Personalized Medicine?}},
url = {http://genomemag.com/what-is-personalized-medicine/},
urldate = {2018-01-14}
}
@misc{Perkins,
author = {Perkins, Anne},
title = {{Could science fiction save NHS data and improve our health? | Anne Perkins | Society | The Guardian}},
url = {https://www.theguardian.com/society/2017/oct/24/could-science-fiction-data-improve-health},
urldate = {2018-01-14}
}
@article{Universit2009,
author = {Universit, Technischen and Templ, Matthias},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Universit, Templ - 2009 - New Developments in Statistical Disclosure Control and Imputation in Official Statistics.pdf:pdf},
title = {{New Developments in Statistical Disclosure Control and Imputation in Official Statistics}},
year = {2009}
}
@article{Iman1982,
abstract = {A method for inducing a desired rank correlation matrix on a multivariate input random variable for use in a simulation study is introduced in this paper. This method is simple to use, is distribution free, preserves the exact form of the marginal distributions on the input variables, and may be used with any type of sampling scheme for which correlation of input variables is a meaningful concept. A Monte Carlo study provides an estimate of the bias and variability associated with the method. Input variables used in a model for study of geologic disposal of radioactive waste provide an example of the usefulness of this procedure. A textbook example shows how the output may be affected by the method presented in this paper.},
author = {Iman, Ronald L. and Conover, W. J.},
doi = {10.1080/03610918208812265},
issn = {0361-0918},
journal = {Communications in Statistics - Simulation and Computation},
keywords = {computer models,dependences,distribution free,large-scale computer codes,multivariate distribution,rank correlation},
month = {jan},
number = {3},
pages = {311--334},
publisher = { Marcel Dekker, Inc. },
title = {{A distribution-free approach to inducing rank correlation among input variables}},
url = {http://www.tandfonline.com/doi/abs/10.1080/03610918208812265},
volume = {11},
year = {1982}
}
@article{cccccccc,
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - 2016 - Data Dictionary of Enroll-HD Plus Periodic Dataset Table of Contents.pdf:pdf},
title = {{Data Dictionary of Enroll-HD Plus Periodic Dataset Table of Contents}},
year = {2016}
}
@article{Appendix2008,
author = {Appendix, Multimedia},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Appendix - 2008 - Multimedia appendix 2.pdf:pdf},
number = {2007},
pages = {2--7},
title = {{Multimedia appendix 2}},
year = {2008}
}
@article{K2012,
abstract = {Background: There are many benefits to open datasets. However, privacy concerns have hampered the widespread creation of open health data. There is a dearth of documented methods and case studies for the creation of public-use health data. We describe a new methodology for creating a longitudinal public health dataset in the context of the Heritage Health Prize (HHP). The HHP is a global data mining competition to predict, by using claims data, the number of days patients will be hospitalized in a subsequent year. The winner will be the team or individual with the most accurate model past a threshold accuracy, and will receive a US $3 million cash prize. HHP began on April 4, 2011, and ends on April 3, 2013.<br />Objective: To de-identify the claims data used in the HHP competition and ensure that it meets the requirements in the US Health Insurance Portability and Accountability Act (HIPAA) Privacy Rule.<br />Methods: We defined a threshold risk consistent with the HIPAA Privacy Rule Safe Harbor standard for disclosing the competition dataset. Three plausible re-identification attacks that can be executed on these data were identified. For each attack the re-identification probability was evaluated. If it was deemed too high then a new de-identification algorithm was applied to reduce the risk to an acceptable level. We performed an actual evaluation of re-identification risk using simulated attacks and matching experiments to confirm the results of the de-identification and to test sensitivity to assumptions. The main metric used to evaluate re-identification risk was the probability that a record in the HHP data can be re-identified given an attempted attack.<br />Results: An evaluation of the de-identified dataset estimated that the probability of re-identifying an individual was .0084, below the .05 probability threshold specified for the competition. The risk was robust to violations of our initial assumptions.<br />Conclusions: It was possible to ensure that the probability of re-identification for a large longitudinal dataset was acceptably low when it was released for a global user community in support of an analytics competition. This is an example of, and methodology for, achieving open data principles for longitudinal health data. },
author = {K, El Emam and Arbuckle, L and Koru, G and Eze, B and Gaudette, L and Neri, E and Rose, S and Howard, J and Gluck, J},
doi = {10.2196/jmir.2001},
issn = {1438-8871},
keywords = {Health Insurance Portability and Accountability Ac,Medical Records Systems, Computerized,Patient Identification Systems,United States},
language = {English},
number = {1 OP  - Publisher: JMIR Publications Country of Publication: Canada NLM ID: 100959882 Publication Model: Electronic Cited Medium: Internet ISSN: 1438-8871 (Electronic) Linking ISSN: 14388871 NLM ISO Abbreviation: J. Med. Internet Res. Subsets: MEDLINE},
pages = {e33},
title = {{De-identification methods for open health data: the case of the Heritage Health Prize claims dataset.}},
url = {http://search.ebscohost.com/login.aspx?direct=true&site=eds-live&db=mnh&AN=22370452},
volume = {14},
year = {2012}
}
@article{Venet2012,
abstract = {BACKGROUND: Classical monitoring approaches rely on extensive on-site visits and source data verification. These activities are associated with high cost and a limited contribution to data quality. Central statistical monitoring is of particular interest to address these shortcomings. PURPOSE: This article outlines the principles of central statistical monitoring and the challenges of implementing it in actual trials. METHODS: A statistical approach to central monitoring is based on a large number of statistical tests performed on all variables collected in the database, in order to identify centers that differ from the others. The tests generate a high-dimensional matrix of p-values, which can be analyzed by statistical methods and bioinformatic tools to identify extreme centers. RESULTS: Results from actual trials are provided to illustrate typical findings that can be expected from a central statistical monitoring approach, which detects abnormal patterns that were not (or could not have been) detected by on-site monitoring. LIMITATIONS: Central statistical monitoring can only address problems present in the data. Important aspects of trial conduct such as a lack of informed consent documentation, for instance, require other approaches. The results provided here are empirical examples from a limited number of studies. CONCLUSION: Central statistical monitoring can both optimize on-site monitoring and improve data quality and as such provides a cost-effective way of meeting regulatory requirements for clinical trials.},
annote = {sources of data errors:
- completely unintentional data erros
- datat errors resuting from carelesseness
- Fabricated data
- Data falsified to reacha desired objective

Challenges in actual trials

- Staggered availability of data
- VOlume of data
- Cleanliness
- Systematic differences between centers},
author = {Venet, David and Doffagne, Erik and Burzykowski, Tomasz and Beckers, Fran{\c{c}}ois and Tellier, Yves and Genevois-Marlin, Eric and Becker, Ursula and Bee, Valerie and Wilson, Veronique and Legrand, Catherine and Buyse, Marc},
doi = {10.1177/1740774512447898},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Venet et al. - 2012 - A statistical approach to central monitoring of data quality in clinical trials(3).pdf:pdf},
issn = {1740-7745},
journal = {Clinical Trials},
number = {6},
pages = {705--713},
pmid = {22684241},
title = {{A statistical approach to central monitoring of data quality in clinical trials}},
url = {http://journals.sagepub.com/doi/10.1177/1740774512447898},
volume = {9},
year = {2012}
}
@article{ElEmam2015,
author = {{El Emam}, Khaled and Abdallah, Kald},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/El Emam, Abdallah - 2015 - De-identifying clinical trials data.pdf:pdf},
journal = {Applied Clinical Trials},
pages = {40--48},
title = {{De-identifying clinical trials data}},
url = {http://www.appliedclinicaltrialsonline.com/de-identifying-clinical-trials-data},
year = {2015}
}
@article{Meindl2012,
annote = {Truncation of highly identifible subjects.

Types of violations, and estimating the ris of disclosure based on the 3 violation types.},
author = {Meindl, Mag Bernhard and Templ, D I Matthias and Meindl, Bernhard and Templ, Matthias and Kowarik, Alexander},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Meindl et al. - 2012 - Guidelines for the Anonymization of Microdata Using R -package sdcMicro.pdf:pdf},
pages = {1--49},
title = {{Guidelines for the Anonymization of Microdata Using R -package sdcMicro}},
year = {2012}
}
@article{ElEmam2009,
abstract = {Background: Explicit patient consent requirements in privacy laws can have a negative impact on health research, leading to selection bias and reduced recruitment. Often legislative requirements to obtain consent are waived if the information collected or disclosed is de-identified. Objective: The authors developed and empirically evaluated a new globally optimal de-identification algorithm that satisfies the k-anonymity criterion and that is suitable for health datasets. Design: Authors compared OLA (Optimal Lattice Anonymization) empirically to three existing k-anonymity algorithms, Datafly, Samarati, and Incognito, on six public, hospital, and registry datasets for different values of k and suppression limits. Measurement: Three information loss metrics were used for the comparison: precision, discernability metric, and non-uniform entropy. Each algorithm's performance speed was also evaluated. Results: The Datafly and Samarati algorithms had higher information loss than OLA and Incognito; OLA was consistently faster than Incognito in finding the globally optimal de-identification solution. Conclusions: For the de-identification of health datasets, OLA is an improvement on existing k-anonymity algorithms in terms of information loss and performance. {\textcopyright} 2009 J Am Med Inform Assoc.},
annote = {Acommonly used de-identification criterion is k-anonymity, and many k-anonymity algorithms have been developed. This criterion stipulates that each record in a dataset is similar to at least another k-1 records on the potentially identifying variables. For example, if k ? 5 and the poten- tially identifying variables are age and gender, then a k-anonymized dataset has at least 5 records for each value combination of age and gender.

A common way to satisfy the k-anonymity criterion is to
- generalize values in the quasi-identifiers by reducing their precision
- Discretization intervals must be definable by the end-user
- Use global recoding instead of local recoding
- THe de-identification solution must be globally optimal

As more generalization is applied, the extent of suppression goes down. For example, node (d0
,g0,a1), with age generalized to 5-year intervals, has only 30% of the records suppressed.

Suppression is preferable to generalization because the former affects single records whereas generalization affects all the records in the dataset.

An information loss metric that takes into account the height of the generalization hierarchy is Precision or Prec. The

Another commonly used information loss metric is the Discernability Metric or DM.
The discernability
metric assigns a penalty to every record that is proportional to the number of records that are indistinguishable from it. Following the same reasoning, DM assigns a penalty equal to the whole dataset for every suppressed record (since suppressed records are indistinguishable from all other records)

One information loss metric that has been proposed based on entropy has recently been extended to address the non-uniform distribution problem.

OLA is an algorithm developed by the authors to choose the node. THere are others like datafly, samarati and incognito. incognito and OLA find the minimum information loss solution

Max Supression allowwed in this case is 1 or 5 or 10%

evaluating whether a node is k-anonymous consists of 3 tasks:
- generalizing quasi-identifiers
- Computing new equivalence classes on the generalized quasi-identifiers
- summing the number of records in equivalence classes that are smaller than k

A local cell suppression algorithm can be applied to the flagged records and it will only suppress specific values of the quasi- identifiers on the flagged records

Some common constraints can be accommodated with OLA by limiting the nodes that are included in the lattice.


As another example, consider a longitudinal health insur- ance claims dataset with a patient's residence postal code at the beginning of each year included as a quasi-identifier. For many patients their postal code will be the same from one year to the next. Therefore, it would be prudent to correlate the postal codes for all the years in the dataset to ensure that postal code is generalized to the same height across all years.},
author = {{El Emam}, Khaled and Dankar, Fida Kamal and Issa, Romeo and Jonker, Elizabeth and Amyot, Daniel and Cogo, Elise and Corriveau, Jean Pierre and Walker, Mark and Chowdhury, Sadrul and Vaillancourt, Regis and Roffey, Tyson and Bottomley, Jim},
doi = {10.1197/jamia.M3144},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/El Emam et al. - 2009 - A Globally Optimal k-Anonymity Method for the De-Identification of Health Data.pdf:pdf},
isbn = {1527-974X; 1067-5027},
issn = {10675027},
journal = {Journal of the American Medical Informatics Association},
number = {5},
pages = {670--682},
pmid = {19567795},
title = {{A Globally Optimal k-Anonymity Method for the De-Identification of Health Data}},
volume = {16},
year = {2009}
}
@article{Cavoukian2013,
author = {Cavoukian, Ann and Ph, D},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Cavoukian, Ph - 2013 - Looking Forward De-identification Developments – New Tools , New Challenges.pdf:pdf},
number = {May},
title = {{Looking Forward : De-identification Developments – New Tools , New Challenges}},
year = {2013}
}
@article{ElEmam2009a,
abstract = {BACKGROUND Pharmacies often provide prescription records to private research firms, on the assumption that these records are de-identified (i.e., identifying information has been removed). However, concerns have been expressed about the potential that patients can be re-identified from such records. Recently, a large private research firm requested prescription records from the Children's Hospital of Eastern Ontario (CHEO), as part of a larger effort to develop a database of hospital prescription records across Canada. OBJECTIVE To evaluate the ability to re-identify patients from CHEO'S prescription records and to determine ways to appropriately de-identify the data if the risk was too high. METHODS The risk of re-identification was assessed for 18 months' worth of prescription data. De-identification algorithms were developed to reduce the risk to an acceptable level while maintaining the quality of the data. RESULTS The probability of patients being re-identified from the original variables and data set requested by the private research firm was deemed quite high. A new de-identified record layout was developed, which had an acceptable level of re-identification risk. The new approach involved replacing the admission and discharge dates with the quarter and year of admission and the length of stay in days, reporting the patient's age in weeks, and including only the first character of the patient's postal code. Additional requirements were included in the data-sharing agreement with the private research firm (e.g., audit requirements and a protocol for notification of a breach of privacy). CONCLUSIONS Without a formal analysis of the risk of re-identification, assurances of data anonymity may not be accurate. A formal risk analysis at one hospital produced a clinically relevant data set that also protects patient privacy and allows the hospital pharmacy to explicitly manage the risks of breach of patient privacy.},
author = {{El Emam}, Khaled and Dankar, Fida K. and Vaillancourt, R{\'{e}}gis and Roffey, Tyson and Lysyk, Mary},
doi = {10.4212/cjhp.v62i4.812},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/El Emam et al. - 2009 - Evaluating the risk of re-identification of patients from hospital prescription records.pdf:pdf},
isbn = {0008-4123 (Print)\r0008-4123 (Linking)},
issn = {00084123},
journal = {Canadian Journal of Hospital Pharmacy},
keywords = {Data anonymity,De-identification,Privavy,Re-identification risk,Secondary use of data},
number = {4},
pages = {307--319},
pmid = {22478909},
title = {{Evaluating the risk of re-identification of patients from hospital prescription records}},
volume = {62},
year = {2009}
}
@article{ElEmam2011,
abstract = {Electronic health records are increasingly being linked to DNA repositories and used as a source of clinical information for genomic research. Privacy legislation in many jurisdictions, and most research ethics boards, require that either personal health information is de-identified or that patient consent or authorization is sought before the data are disclosed for secondary purposes. Here, I discuss how de-identification has been applied in current genomic research projects. Recent metrics and methods that can be used to ensure that the risk of re-identification is low and that disclosures are compliant with privacy legislation and regulations (such as the Health Insurance Portability and Accountability Act Privacy Rule) are reviewed. Although these methods can protect against the known approaches for re-identification, residual risks and specific challenges for genomic research are also discussed.},
author = {{El Emam}, Khaled},
doi = {10.1186/gm239},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/El Emam - 2011 - Methods for the de-identification of electronic health records for genomic research.pdf:pdf},
isbn = {1756-994X},
issn = {1756-994X},
journal = {Genome medicine},
keywords = {EHR,Genome,Share},
number = {4},
pages = {25},
pmid = {21542889},
title = {{Methods for the de-identification of electronic health records for genomic research.}},
url = {http://genomemedicine.com/content/3/4/25%5Cnpapers3://publication/doi/10.1186/gm239},
volume = {3},
year = {2011}
}
@article{ElEmam2010,
abstract = {This article describes a method for assessing the overall risk of re-identification for a health data set and how that risk information can be used to decide how much to de-identify the data before it's disclosed. Such an approach ensures that the amount of distortion to the data is proportionate to the risk involved in disclosing a particular data set to a particular data recipient.},
annote = {Talks about the levels of identifiability and the types of risks.

Identifiability as a continuumm. Threshold for that scale.

With level 4 data, the custodian
objectively measures identifiability and can substantiate claims that it's above or below a specified thresh- old. Level 4 data can be microdata or appear in tabular form, and only at this level can data move from being personal information to not being personal information. Level 4 data is called managed because the data custodian can manage the risk of re-identification.

3 types of risks: Prosecutor, Journalist (individual), Marketeer (population)},
author = {{El Emam}, Khaled},
doi = {10.1109/MSP.2010.103},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/El Emam - 2010 - Risk-based de-identification of health data.pdf:pdf},
isbn = {1540-7993},
issn = {15407993},
journal = {IEEE Security and Privacy},
keywords = {Data de-identification,Healthcare,Privacy,Re-identification},
number = {3},
pages = {64--67},
pmid = {23765864},
title = {{Risk-based de-identification of health data}},
volume = {8},
year = {2010}
}
@article{Skinner2009,
abstract = {Survey respondents are usually provided with an assurance that their responses will be treated confidentially. These assurances may relate to the way their responses will be handled within the agency conducting the survey, or they may relate to the nature of the statistical outputs of the survey as, for example, in the “confidentiality guarantee” in the United Kingdom (U.K.) National Statistics Code of Practice that “no statistics will be produced that are likely to identify an individual.” This chapter discusses the methods for ensuring that the latter kinds of assurances are met. Statistical disclosure control (SDC) refers to the methodology used in the design of the statistical outputs from the survey for protecting the confidentiality of respondents' answers. There are various kinds of statistical outputs from surveys. The most traditional are tables of descriptive estimates, such as totals, means, and proportions. The release of such estimates from surveys of households and individuals has not been considered to represent a major threat to confidentiality, particularly because of the protection provided by sampling. {\textcopyright} 2009 Elsevier Inc.},
annote = {Statistical disclosure control refers to the methodology used in the design of the statistical outputs from a survey for protecting the confidentiality of respondents' answers.

Measures of the probability of disclosure are called disclosure risk.

identity disclosure or identification, which would occur if the intruder linked a known individual (or other unit) to an individual microdata record or other element of the statistical output. 

SDC methods vary according to the form of the statistical output. Some simple
approaches are:
-reduction of detail
-suppression
- Stochastic perturbation of variables
- synthetic microdata
- selective perturbation
- record swapping
- Microaggregation 

The key challenge in SDC is how to deal with the trade-off between disclosure risk
and utility. In general, the more the disclosure risk is reduced by an SDC method, the lower will be the expected utility of the output. (an optimization problem)

Disclosure in Tabular Outputs (And SDC methods for tabular outputs)

Risk probability of disclosure. (record level and file level risks)},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Skinner, Chris},
doi = {10.1016/S0169-7161(08)00015-1},
eprint = {arXiv:1011.1669v3},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Skinner - 2009 - Statistical Disclosure Control for Survey Data.pdf:pdf},
isbn = {9780444521033},
issn = {01697161},
journal = {Handbook of Statistics},
number = {PA},
pages = {381--396},
pmid = {25246403},
title = {{Statistical Disclosure Control for Survey Data}},
volume = {29},
year = {2009}
}
@book{Templ2015,
annote = {This package can be used for the generation of anonymized (micro)data, i.e. for the creation of public- and scientific-use files. In addition, various risk estimation methods are included.

Statistical disclosure control (SDC), also known as statistical disclosure limitation (SDL) or disclosure avoidance, is a technique used in data-driven research to ensure no person or organization is identifiable from the results of an analysis of survey or administrative data, or in the release of microdata. The purpose of SDC is to protect the confidentiality of the respondents and subjects of the research. 
  
In rules-based SDC, a rigid set of rules is used to determine whether or not the results of data analysis can be released. The rules are applied consistently, which makes it obvious what kinds of output are acceptable. However, because the rules are inflexible, either disclosive information may still slip through, or the rules are overrestrictive and may only allow for results that are too broad for useful analysis to be published. 
  
In principles-based SDC, both the researcher and the output checker are trained in SDC. They receive a set of rules, which are rules-of-thumb rather than hard rules as in rules-based SDC. This means that in principle, any output may be approved or refused. The rules-of-thumb are a starting point for the researcher and explain from the beginning which outputs would be deemed safe and non-disclosive, and which outputs are unsafe. It is up to the researcher to prove that any 'unsafe' outputs are non-disclosive, but the checker has the final say. Since there are no hard rules, this requires specialist knowledge on disclosure risks from both the researcher and the checker. It encourages the researcher to produce safe results in the first place. However, this also means that the outcome may be inconsistent and uncertain. It requires extensive training and a high understanding of statistics and data analysis.},
author = {Templ, Author Matthias and Kowarik, Alexander and Meindl, Bernhard and Templ, Maintainer Matthias and True, Bytecompile},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Templ et al. - 2015 - Package ‘ sdcMicro '.pdf:pdf},
isbn = {9783642336263},
title = {{Package ‘ sdcMicro '}},
year = {2015}
}
@article{ElEmam2011a,
abstract = {BACKGROUND: Privacy legislation in most jurisdictions allows the disclosure of health data for secondary purposes without patient consent if it is de-identified. Some recent articles in the medical, legal, and computer science literature have argued that de-identification methods do not provide sufficient protection because they are easy to reverse. Should this be the case, it would have significant and important implications on how health information is disclosed, including: (a) potentially limiting its availability for secondary purposes such as research, and (b) resulting in more identifiable health information being disclosed. Our objectives in this systematic review were to: (a) characterize known re-identification attacks on health data and contrast that to re-identification attacks on other kinds of data, (b) compute the overall proportion of records that have been correctly re-identified in these attacks, and (c) assess whether these demonstrate weaknesses in current de-identification methods.\n\nMETHODS AND FINDINGS: Searches were conducted in IEEE Xplore, ACM Digital Library, and PubMed. After screening, fourteen eligible articles representing distinct attacks were identified. On average, approximately a quarter of the records were re-identified across all studies (0.26 with 95% CI 0.046-0.478) and 0.34 for attacks on health data (95% CI 0-0.744). There was considerable uncertainty around the proportions as evidenced by the wide confidence intervals, and the mean proportion of records re-identified was sensitive to unpublished studies. Two of fourteen attacks were performed with data that was de-identified using existing standards. Only one of these attacks was on health data, which resulted in a success rate of 0.00013.\n\nCONCLUSIONS: The current evidence shows a high re-identification rate but is dominated by small-scale studies on data that was not de-identified according to existing standards. This evidence is insufficient to draw conclusions about the efficacy of de-identification methods.},
annote = {Artigo que faz uma revis{\~{a}}o dos ataques e re-identificac{\~{a}}o a dados de saude.},
author = {{El Emam}, Khaled and Jonker, Elizabeth and Arbuckle, Luk and Malin, Bradley},
doi = {10.1371/journal.pone.0028071},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/El Emam et al. - 2011 - A systematic review of re-identification attacks on health data.pdf:pdf},
isbn = {1932-6203; 1932-6203},
issn = {19326203},
journal = {PLoS ONE},
number = {12},
pmid = {22164229},
title = {{A systematic review of re-identification attacks on health data}},
volume = {6},
year = {2011}
}
@article{Duncan2001,
abstract = {Information organizations (IOs) must provide data products that are both useful and have low risk of confidentiality disclosure. Recognizing that deidentification of data is generally inadequate to protect their confidentiality against attack by a data snooper, concerned IOs can apply disclosure limitation techniques to the original data. Desirably, the resulting restricted data have both high data utility U to users (analytically valid data) and low disclosure risk R (safe data). This article shows the promise of the R-U confidentiality map, a chart that traces the impact on R and U of changes in the parameters of a disclosure limitation procedure. Theory for the R-U confidentiality map is developed for additive noise applied to univariate data under various scenarios of data snooper attack. These scenarios are predicated on different knowledge states for the data snooper. A demonstration is provided of how to implement the theory for a real database. Through simulation methods, this leads to an empirical R-U confidentiality map. Application is made to data from a National Center for Education Statistics (NCES) survey, the},
annote = {In practice, different SDC methods can be evaluated and compared by considering the values of alternative measures of risk and utility. For given measures of each, it can sometimes be useful to construct an RU map (Duncan et al., 2001), where a measure of risk is plotted against a measure of utility for a set of candidate SDC methods.},
author = {Duncan, G.T. and Keller-McNulty, S.a. and Stokes, S.L.},
doi = {10.1080/09332480.2004.10554908},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Duncan, Keller-McNulty, Stokes - 2001 - Disclosure risk vs. data utility The RU confidentiality map.pdf:pdf},
issn = {0933-2480},
journal = {Technical Report LA-UR-01-6428, Statistical Sciences Group, Los Alamos National Laboratory},
number = {121},
pages = {1--30},
title = {{Disclosure risk vs. data utility: The RU confidentiality map}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.79.1598},
year = {2001}
}
@article{Hamon,
abstract = {Background and Purpose—The primary objective of this study was to assess the incidence of new cerebral infarcts related to cardiac catheterization in patients explored through the right transradial approach. Methods—This prospective study involved 41 consecutive patients with severe aortic valve stenosis. To assess the incidence of cerebral infarction, all patients underwent cerebral diffusion-weighted MRI before and after cardiac catheterization through the right transradial approach. Results—We detected only two patients (4.9%) with new, small, isolated acute cerebral diffusion abnormalities postcatheterization. All patients remained asymptomatic. Conclusions—New cerebral lesions on diffusion-weighted MRI are infrequent in patients explored through the right transradial approach. Randomized studies are warranted to confirm for potential advantages of transradial approach versus the femoral approach in cardiac catheterization. (Stroke. 2007;38:2176-2179.) Key Words: brain infarction Ⅲ diffusion-weighted imaging Ⅲ stroke Ⅲ transcranial Doppler T he overall risk of cerebral infarction after cardiac cathe-terization is low when estimated on the basis of clinical examination. However, advances in neuroimaging tech-niques, like diffusion-weighted MRI (DW-MRI), have re-cently revealed that silent brain infarcts may occur at an unexpectedly high rate after cardiac catheterization (up to 15% to 22%). 1–3 These silent cerebral infarcts related to microembolism during endovascular procedures are thought to be caused primarily by the catheter dislodging atherosclerotic debris from the descending or arch portions of the aorta. 4,5 Thus, it has been suggested that using a right brachial or right radial artery catheter insertion point might reduce the risk of stroke by avoiding the negotiation of the descending aorta and aortic arch that is inherent when using a femoral artery catheter insertion point. 5 We performed cerebral DW-MRI before and after cardiac catheterization in a series of consecutive patients with aortic stenosis to prospectively evaluate the incidence of new cerebral infarction when patients are explored by means of the right transradial approach (TRA).},
author = {Hamon, Michele and Gomes, Sophie and Clergeau, Marie-Rose and Fradin, Sabine and Morello, R{\'{e}}my and Hamon, Martial},
doi = {10.1161/STROKEAHA.107.482414},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Hamon et al. - 2007 - Risk of Acute Brain Injury Related to Cerebral Microembolism During Cardiac Catheterization Performed by Right Upp.pdf:pdf},
journal = {American Heart Association Journal},
title = {{Risk of Acute Brain Injury Related to Cerebral Microembolism During Cardiac Catheterization Performed by Right Upper Limb Arterial Access}},
year = {2007}
}
@article{Ratib2013a,
abstract = {BACKGROUND Neurologic complications (NCs) are a rare but potentially devastating complication that may follow percutaneous coronary intervention (PCI). In recent years, there has been an increase in use of transradial access, driven by a developing body of evidence that favors its use over femoral access. Concerns have been raised, however, that transradial access may increase the risk of NC compared with transfemoral access. We aimed to investigate the influence of access site selection on the occurrence of NCs through a period of transition during which transradial access became the dominant route for PCI procedures performed in the United Kingdom. METHODS We performed a retrospective analysis of the British Cardiovascular Intervention Society database between January 2006 and December 2010. The data were split into 2 cohorts based on access site. An NC was defined as a periprocedural ischemic stroke, hemorrhagic stroke, or transient ischemic attack occurring before hospital discharge. Binary logistic multivariate analysis was used to investigate the influence of access site utilization on NCs and adjust for measured confounding factors. RESULTS Between 2006 and 2010, the use of radial access increased from 17.2% to 50.8% of all PCI procedures. A total of 124,616 radial procedures and 223,476 femoral procedures were studied with a NC rate of 0.11% in each cohort. In univariate (odds ratio 1.01, 95% CI 0.82-1.24, P = .93) and multivariate analysis (odds ratio 0.99, 95% CI 0.79-1.23, P = .91), there was no significant association between the use of radial access and the occurrence of NCs. CONCLUSION These results suggest that radial access is not associated with an increased risk of clinically detected NCs, even during a period when there was a rapid evolution in the preferred access site for PCI in the United Kingdom. These are reassuring results, particularly for operators embarking on a change to radial access for PCI.},
author = {Ratib, Karim and Mamas, Mamas and Routledge, Helen and Ludman, Peter and Fraser, Douglas and Nolan, James},
doi = {10.1016/j.ahj.2012.10.015},
issn = {1097-6744},
journal = {American heart journal},
month = {mar},
number = {3},
pages = {317--24},
pmid = {23453099},
title = {{Influence of access site choice on incidence of neurologic complications after percutaneous coronary intervention.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23453099},
volume = {165},
year = {2013}
}
@article{Sacco1997,
author = {Sacco, Ralph and Benjamin, Emelia and Broderick, Joseph and Dyken, Mark and Easton, J. Donald and Feinberg, William M. and Goldstein, Larry B. and Gorelick, Philip B. and Howard, George and Kittner, Steven J. and Manolio, Teri A. and Whisnant, Jack P. and Wolf, Philip A.},
journal = {Stroke},
number = {7},
title = {{Risk Factors}},
volume = {28},
year = {1997}
}
@book{gray2000,
address = {New York},
author = {Gray, Henry},
edition = {20th},
publisher = {Bartebly},
title = {{Anatomy of the Human Body}},
year = {2000}
}
@article{Friedman1981,
author = {Friedman, Jerome H and Stuetzle, Werner},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Friedman, Stuetzle - 1981 - Projection Pursuit Regression.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {376},
pages = {817--823},
title = {{Projection Pursuit Regression}},
url = {http://links.jstor.org/sici?sici=0162-1459%28198112%2976%3A376%3C817%3APPR%3E2.0.CO%3B2-1},
volume = {76},
year = {1981}
}
@book{Hastie1990,
address = {London},
author = {Hastie, TJ and Tibshirani, RJ},
publisher = {Chapman and Hall},
title = {{Generalized additive models}},
url = {https://www.google.com/books?hl=pt-PT&lr=&id=qa29r1Ze1coC&oi=fnd&pg=PR13&dq=Generalized+Additive+Models+hastie+1990&ots=j32NitxYjO&sig=Gg_V-3YKpkw83GPZy7SGKW9SCL0},
year = {1990}
}
@book{Buja1989,
address = {Toronto},
author = {Buja, A and Hastie, T and Tibshirani, R},
booktitle = {The Annals of Statistics},
publisher = {University of Toronto, Dept. of Statistics},
title = {{Linear smoothers and additive models}},
url = {http://www.jstor.org/stable/2241560},
year = {1989}
}
@article{Leontief1947,
author = {Leontief, Wassily},
doi = {10.2307/1905335},
issn = {00129682},
journal = {Econometrica},
month = {oct},
number = {4},
pages = {361},
title = {{Introduction to a Theory of the Internal Structure of Functional Relationships}},
url = {http://www.jstor.org/stable/1905335?origin=crossref},
volume = {15},
year = {1947}
}
@article{Meldrum2000,
abstract = {This article discusses the history and development of randomized clinical trial methodology, the reasons for its status and authority as a method of therapeutic evaluation, and the continuing role of clinical judgement in designing, interpreting, and applying the findings of trials.},
author = {Meldrum, M L},
issn = {0889-8588},
journal = {Hematology/oncology clinics of North America},
month = {aug},
number = {4},
pages = {745--60, vii},
pmid = {10949771},
title = {{A brief history of the randomized controlled trial. From oranges and lemons to the gold standard.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10949771},
volume = {14},
year = {2000}
}
@article{Austin2007a,
abstract = {Objective: I conducted a systematic review of the use of propensity score matching in the cardiovascular surgery literature. I examined the adequacy of reporting and whether appropriate statistical methods were used. Methods: I examined 60 articles published in the Annals of Thoracic Surgery, European Journal of Cardio-thoracic Surgery, Journal of Cardiovascular Surgery, and the Journal of Thoracic and Cardiovascular Surgery between January 1, 2004, and December 31, 2006. Results: Thirty-one of the 60 studies did not provide adequate information on how the propensity score-matched pairs were formed. Eleven (18%) of studies did not report on whether matching on the propensity score balanced baseline characteristics between treated and untreated subjects in the matched sample. No studies used appropriate methods to compare baseline characteristics between treated and untreated subjects in the propensity score-matched sample. Eight (13%) of the 60 studies explicitly used statistical methods appropriate for the analysis of matched data when estimating the effect of treatment on the outcomes. Two studies used appropriate methods for some outcomes, but not for all outcomes. Thirty-nine (65%) studies explicitly used statistical methods that were inappropriate for matched-pairs data when estimating the effect of treatment on outcomes. Eleven studies did not report the statistical tests that were used to assess the statistical significance of the treatment effect. Conclusions: Analysis of propensity score-matched samples tended to be poor in the cardiovascular surgery literature. Most statistical analyses ignored the matched nature of the sample. I provide suggestions for improving the reporting and analysis of studies that use propensity score matching. {\textcopyright} 2007 The American Association for Thoracic Surgery.},
author = {Austin, Peter C.},
doi = {10.1016/j.jtcvs.2007.07.021},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Austin - 2007 - Propensity-score matching in the cardiovascular surgery literature from 2004 to 2006 A systematic review and suggestions.pdf:pdf},
isbn = {1941-7705; 1941-7713},
issn = {00225223},
journal = {Journal of Thoracic and Cardiovascular Surgery},
number = {5},
pmid = {17976439},
title = {{Propensity-score matching in the cardiovascular surgery literature from 2004 to 2006: A systematic review and suggestions for improvement}},
volume = {134},
year = {2007}
}
@misc{Flury1986,
abstract = {We propose to use the term standard distance for the quantity |x̄1 - x̄2|/s in univariate analysis and show that it can be easily generalized to the multivariate situation, where it coincides with the square root of the Mahalanobis distance between two samples. CR - Copyright &#169; 1986 American Statistical Association},
author = {Flury, Bk and Riedwyl, Hans},
booktitle = {The American Statistician},
doi = {10.2307/2684560},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Flury, Riedwyl - 1986 - Standard distance in univariate and multivariate analysis.pdf:pdf},
issn = {00031305},
keywords = {and sociology as a,chology,education,euclidean distance,is is used in,mahalanobis distance,mean difference,projection,psy-,standardized mea-,t test,the signed standard distance,xl - x2},
number = {3},
pages = {249--251},
title = {{Standard distance in univariate and multivariate analysis}},
url = {http://amstat.tandfonline.com/doi/abs/10.1080/00031305.1986.10475403},
volume = {40},
year = {1986}
}
@article{Flury1986a,
author = {Flury, Bernhard K. and Riedwyl, Hans},
doi = {10.2307/2684560},
issn = {00031305},
journal = {The American Statistician},
month = {aug},
number = {3},
pages = {249},
title = {{Standard Distance in Univariate and Multivariate Analysis}},
url = {http://www.jstor.org/stable/2684560?origin=crossref},
volume = {40},
year = {1986}
}
@article{Donnan2008a,
abstract = {Stroke is the second most common cause of death and major cause of disability worldwide. Because of the ageing population, the burden will increase greatly during the next 20 years, especially in developing countries. Advances have occurred in the prevention and treatment of stroke during the past decade. For patients with acute stroke, management in a stroke care unit, intravenous tissue plasminogen activator within 3 h or aspirin within 48 h of stroke onset, and decompressive surgery for supratentorial malignant hemispheric cerebral infarction are interventions of proven benefit; several other interventions are being assessed. Proven secondary prevention strategies are warfarin for patients with atrial fibrillation, endarterectomy for symptomatic carotid stenosis, antiplatelet agents, and cholesterol reduction. The most important intervention is the management of patients in stroke care units because these provide a framework within which further study might be undertaken. These advances have exposed a worldwide shortage of stroke health-care workers, especially in developing countries.},
author = {Donnan, Geoffrey A and Fisher, Marc and Macleod, Malcolm and Davis, Stephen M},
doi = {10.1016/S0140-6736(08)60694-7},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Donnan et al. - 2008 - Stroke(2).pdf:pdf},
issn = {01406736},
journal = {The Lancet},
number = {9624},
pages = {1612--1623},
title = {{Stroke}},
volume = {371},
year = {2008}
}
@article{Raposo2015,
abstract = {Background and aim Transradial approach (TRA) is being used increasingly as the preferential vascular access site for both diagnostic and interventional procedures. However, concerns have risen about the risk of clinically meaningful neurologic complications. We aimed to assess the association between the risk of stroke/transient ischemic attack (TIA) and the transradial (vs. transfemoral) approach. Methods and Results Data from 16,710 cases included in a single centre prospective registry between January 2006 and November 2012 was analyzed. Radial procedures were considered as those in which the radial access was used either primarily (n=4,195) or after conversion (n=36). Potential cases with neurologic events were targeted by cross-referencing patients who underwent both cardiac catheterization and cranial-computed tomography (cranial-CT) during the same admission episode (n=67). Procedure-related events were defined as a definitive non-CABG related stroke/TIA occurring within 48 hr of the procedure. TRA increased from 0.7% in 2006 to 75% in 2012. Total incidence of stroke/TIA was 0.16% and did not change over the study period (P=0.26). There was no significant difference in stroke/TIA rates between groups (0.165% vs. 0.160%; P=1.0). After correction for baseline differences and propensity score matching, TRA was not an independent predictor of stroke/TIA (OR 1.21; 95% CI 0.49-2.98 and 1.3; 95% CI 0.55-3.54, respectively). Results were consistent in pre-specified sub-groups according to age (>65 y.o. vs. younger), gender, interventional vs. diagnostic and ACS vs. stable. Conclusion Rates of documented stroke/TIA were low. Our observational study suggests that widening the use of the TRA is not associated with an increased risk of clinically relevant procedure-related neurologic complications.},
author = {Raposo, L and Madeira, S and Teles, R C and Santos, M and Gabriel, H M and Goncalves, P and Brito, J and Leal, S and Almeida, M and Mendes, M},
doi = {10.1002/ccd.25884},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Raposo et al. - 2015 - Neurologic complications after transradial or transfemoral approach for diagnostic and interventional cardiac cat.pdf:pdf},
isbn = {1972952943},
issn = {15221946},
journal = {Catheterization and Cardiovascular Interventions},
keywords = {acute coronary syndrome,adult,aged,article,cerebrovascular accident/co [Complication],computer assisted tomography,controlled study,diagnostic procedure,female,femoral artery,heart catheterization,human,interventional cardiovascular procedure,major clinical study,male,mortality,observational study,percutaneous coronary intervention,propensity score,radial artery,retrospective study,transient ischemic attack/co [Complication],vascular access},
number = {1},
pages = {61--70},
pmid = {2015147688},
title = {{Neurologic complications after transradial or transfemoral approach for diagnostic and interventional cardiac catheterization: A propensity score analysis of 16,710 cases from a single centre prospective registry}},
url = {http://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=emed13&AN=2015147688%5Cnhttp://sfx.ucl.ac.uk/sfx_local?sid=OVID:embase&id=pmid:&id=doi:10.1002/ccd.25884&issn=1522-1946&isbn=&volume=86&issue=1&spage=61&pages=61-70&date=2015&title=Cathet},
volume = {86},
year = {2015}
}
@misc{NationalHeart2016,
author = {{National Heart}, Lung and Blood Institute},
booktitle = {http://www.nhlbi.nih.gov/},
title = {{What Is Coronary Heart Disease?}},
url = {http://www.nhlbi.nih.gov/health/health-topics/topics/cad},
urldate = {2016-07-01},
year = {2016}
}
@book{Topola,
abstract = {7th edition. Ideal for cardiologists, surgeons, and referring physicians who need a clinical guide to interventional procedures, Textbook of Interventional Cardiology focuses on the latest treatment protocols for managing heart disorders at every level of complexity. In this updated edition, Dr. Topol continues to bring together experts in the field who present the current state of knowledge and clinical practice in interventional cardiology, including cutting-edge theories, trends, and applications of diagnostic and interventional cardiology, as well as peripheral vascular techniques and practices. Individualized assessment for percutaneous or surgical revascularization -- Evidence-based interventional practice -- Diabetes -- Prior evaluation : functional testing and multidetector computed tomography -- Intracoronary pressure and flow measurements -- Contrast-induced acute kidney injury and the role of chronic kidney disease in percutaneous coronary intervention -- Radiation safety during cardiovascular procedures -- Preoperative coronary intervention -- Sex and ethnicity issues in interventional cardiology -- Platelet inhibitor agents -- Anticoagulation in percutaneous coronary intervention -- Lipid lowering in coronary artery disease -- Thrombolytic intervention -- Other adjunctive drugs for coronary intervention : beta-blockers, calcium-channel blockers, and angiotensin-converting enzyme inhibitors -- The history of balloon angioplasty -- Bare-metal and drug-eluting coronary stents -- Drug-coated balloons -- Elective intervention for stable angina or silent ischemia -- Percutaneous intervention for non-ST-segment elevation acute coronary syndromes -- Percutaneous coronary intervention in acute ST-segment elevation myocardial infarction -- Interventions in cardiogenic shock -- Bifurcations and branch vessel stenting -- Percutaneous coronary intervention for unprotected left main coronary artery stenosis -- Complex and multivessel percutaneous coronary intervention -- Intervention for coronary chronic total occlusions -- Bypass graft intervention -- The thrombus-containing lesion -- Complications of percutaneous coronary intervention -- Periprocedural myocardial infarction and embolism-protection devices -- Access management and closure devices -- Transradial approach for diagnostic coronary angiography and intervention -- The role of the cardiac surgeon -- Restenosis -- Bioresorbable coronary scaffolds -- The role of adjunct devices : atherectomy, cutting balloon, and laser -- Support devices for high-risk percutaneous coronary interventions -- Regional centers of excellence for the care of patients with acute ischemic heart disease -- Post-percutaneous coronary intervention hospitalization, length of stay, and discharge planning -- Lower extremity interventions -- Upper extremities and aortic arch -- Chronic mesenteric ischemia : diagnosis and intervention -- Renal artery stenosis -- Device therapy for resistant hypertension -- Thoracic and abdominal aortic vascular interventions -- Venous intervention -- Carotid and cerebrovascular intervention -- Stroke centers and interventional cardiology -- Imaging for intracardiac interventions -- Percutaneous closure of patent foramen ovale and atrial septal defect -- Left atrial appendage closure and stroke : local device therapy for cardioembolic stroke protection -- Mitral valvuloplasty -- Percutaneous mitral valve repair -- Transcatheter aortic valve interventions : from balloon aortic valvuloplasty to transcatheter aortic valve implantation -- Pulmonary and tricuspid valve interventions -- Hypertrophic cardiomyopathy -- Pericardial interventions -- Transcatheter therapies for congenital heart disease -- Stem cell therapy for ischemic heart disease -- Qualitative and quantitative coronary angiography -- Intravascular ultrasound -- High-risk vulnerable plaques : definition, diagnosis, and treatment -- Optical coherence tomography -- Medical economics and interventional cardiology -- Quality of care in interventional cardiology -- Volume and outcome -- Interventional heart failure.},
address = {Philadelphia},
author = {Topol, Eric J. and Teirstein, Paul S.},
isbn = {9780323340380},
publisher = {Elsevier},
title = {{Textbook of interventional cardiology}},
year = {2016}
}
@article{Lakhan2009,
abstract = {Interventional medical practitioners are specialists who do minimally invasive procedures instead of surgery or other treatment. Most often, these procedures utilize various imaging and catheterization techniques in order to diagnose and treat vascular issues in the body. Interventionalist techniques, including injecting arteries with dye, visualizing these via x-ray, and opening up blockages, developed from early pioneers' bold and sometimes controversial experiments which aimed to find safer and better ways to treat coronary artery and other atherosclerotic vascular disease. Currently, the major interventional specialties are interventional (or vascular) radiology, interventional cardiology, and endovascular surgical (interventional) neuroradiology. All three are perfecting the use of stents and other procedures to keep diseased arteries open, while also evaluating the application these procedures. The rapid new development of imaging technologies, mechanical devices, and types of treatment, while certainly beneficial to the patient, can also lead to ambiguity regarding specific specialty claims on certain techniques and devices. While these practitioners can be in competition with each other, cooperation and communication are the most advantageous methods to deal with these "turf wars." All of the interventionalists are needed to deliver the best medical care to patients, now and in the future.},
author = {Lakhan, Shaheen E and Kaplan, Anna and Laird, Cyndi and Leiter, Yaacov},
doi = {10.1186/1755-7682-2-27},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Lakhan et al. - 2009 - The interventionalism of medicine interventional radiology, cardiology, and neuroradiology.pdf:pdf},
issn = {1755-7682},
journal = {International archives of medicine},
number = {1},
pages = {27},
pmid = {19740425},
publisher = {BioMed Central},
title = {{The interventionalism of medicine: interventional radiology, cardiology, and neuroradiology.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19740425 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2745361},
volume = {2},
year = {2009}
}
@article{Braitman2002a,
author = {Braitman, Leonard E and Rosenbaum, Paul R},
issn = {1539-3704},
journal = {Annals of internal medicine},
month = {oct},
number = {8},
pages = {693--5},
pmid = {12379071},
title = {{Rare outcomes, common treatments: analytic strategies using propensity scores.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12379071},
volume = {137},
year = {2002}
}
@article{Peduzzi1996,
author = {Peduzzi, Peter and Concato, John and Kemper, Elizabeth and Holford, Theodore R. and Feinstein, Alvan R.},
doi = {10.1016/S0895-4356(96)00236-3},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Peduzzi et al. - 1996 - A simulation study of the number of events per variable in logistic regression analysis.pdf:pdf},
issn = {08954356},
journal = {Journal of Clinical Epidemiology},
keywords = {Monte Carlo,bias,precision,significance testing},
month = {dec},
number = {12},
pages = {1373--1379},
publisher = {Elsevier},
title = {{A simulation study of the number of events per variable in logistic regression analysis}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0895435696002363},
volume = {49},
year = {1996}
}
@book{Arbogast2013,
address = {Rockville},
author = {Arbogast, Patrick G and VanderWeele, Tyler J},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Arbogast, VanderWeele - 2013 - Considerations for Statistical Analysis.pdf:pdf},
publisher = {Agency for Healthcare Research and Quality (US)},
title = {{Considerations for Statistical Analysis}},
year = {2013}
}
@book{Organization,
address = {Geneva},
author = {{Mendis; Shanthi} and {Puska; Pekka} and {Norrving; Bo}},
pages = {3--18},
publisher = {World Health Organization in collaboration with the World Heart Federation and the World Stroke Organization},
title = {{Global Atlas on cardiovascular disease prevention and control}},
year = {2011}
}
@article{Ratib2013,
abstract = {BACKGROUND Neurologic complications (NCs) are a rare but potentially devastating complication that may follow percutaneous coronary intervention (PCI). In recent years, there has been an increase in use of transradial access, driven by a developing body of evidence that favors its use over femoral access. Concerns have been raised, however, that transradial access may increase the risk of NC compared with transfemoral access. We aimed to investigate the influence of access site selection on the occurrence of NCs through a period of transition during which transradial access became the dominant route for PCI procedures performed in the United Kingdom. METHODS We performed a retrospective analysis of the British Cardiovascular Intervention Society database between January 2006 and December 2010. The data were split into 2 cohorts based on access site. An NC was defined as a periprocedural ischemic stroke, hemorrhagic stroke, or transient ischemic attack occurring before hospital discharge. Binary logistic multivariate analysis was used to investigate the influence of access site utilization on NCs and adjust for measured confounding factors. RESULTS Between 2006 and 2010, the use of radial access increased from 17.2% to 50.8% of all PCI procedures. A total of 124,616 radial procedures and 223,476 femoral procedures were studied with a NC rate of 0.11% in each cohort. In univariate (odds ratio 1.01, 95% CI 0.82-1.24, P = .93) and multivariate analysis (odds ratio 0.99, 95% CI 0.79-1.23, P = .91), there was no significant association between the use of radial access and the occurrence of NCs. CONCLUSION These results suggest that radial access is not associated with an increased risk of clinically detected NCs, even during a period when there was a rapid evolution in the preferred access site for PCI in the United Kingdom. These are reassuring results, particularly for operators embarking on a change to radial access for PCI.},
author = {Ratib, Karim and Mamas, Mamas A and Routledge, Helen C and Ludman, Peter F and Fraser, Douglas and Nolan, James},
doi = {10.1016/j.ahj.2012.10.015},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Ratib et al. - 2013 - Influence of access site choice on incidence of neurologic complications after percutaneous coronary intervention.pdf:pdf},
issn = {1097-6744},
journal = {American heart journal},
month = {mar},
number = {3},
pages = {317--24},
pmid = {23453099},
publisher = {Elsevier},
title = {{Influence of access site choice on incidence of neurologic complications after percutaneous coronary intervention.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23453099},
volume = {165},
year = {2013}
}
@article{Jurga2011,
abstract = {BACKGROUND AND PURPOSE Microemboli observed during coronary angiography can cause silent ischemic cerebral lesions. The aim of this study was to investigate if the number of particulate cerebral microemboli during coronary angiography is influenced by access site used. METHODS Fifty-one patients with stable angina pectoris referred for coronary angiography were randomized to right radial or right femoral arterial access. The number of particulate microemboli passing the middle cerebral arteries was continuously registered with transcranial Doppler. RESULTS The median (minimum-maximum range) numbers of particulate emboli were significantly higher with radial 10 (1-120) than with femoral 6 (1-19) access. More particulate microemboli passed the right middle cerebral artery with the radial access. CONCLUSIONS This study indicates that the radial access used for coronary angiography generates more particulate cerebral microemboli than the femoral access and thus may influence the occurrence of silent cerebral injuries.},
author = {Jurga, Juliane and Nyman, Jesper and Tornvall, Per and Mannila, Maria Nastase and Svenarud, Peter and van der Linden, Jan and Sarkar, Nondita},
doi = {10.1161/STROKEAHA.110.608638},
issn = {1524-4628},
journal = {Stroke; a journal of cerebral circulation},
month = {may},
number = {5},
pages = {1475--7},
pmid = {21393589},
title = {{Cerebral microembolism during coronary angiography: a randomized comparison between femoral and radial arterial access.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21393589},
volume = {42},
year = {2011}
}
@article{Dangoisse2013,
author = {Dangoisse, Vincent and Guedes, Antoine and Gabriel, Laurence and Jamart, Jacques and Chenu, Patrick and Marchandise, Baudouin and Schroeder, Erwin},
doi = {10.4244/EIJV9I3A56},
issn = {1969-6213},
journal = {EuroIntervention},
month = {jul},
number = {3},
pages = {345--52},
pmid = {23872649},
title = {{Full conversion from transfemoral to transradial approach for percutaneous coronary interventions results in a similar success rate and a rapid reduction of in-hospital cardiac and vascular major events.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23872649},
volume = {9},
year = {2013}
}
@article{Mitchell2012,
author = {Mitchell, Matthew D and Hong, Jaekyoung A and Lee, Bruce Y and Umscheid, Craig A and Bartsch, Sarah M and Don, Creighton W},
doi = {10.1161/CIRCOUTCOMES.112.965269},
issn = {1941-7705},
journal = {Circulation. Cardiovascular quality and outcomes},
month = {jul},
number = {4},
pages = {454--62},
pmid = {22740010},
title = {{Systematic review and cost-benefit analysis of radial artery access for coronary angiography and intervention.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22740010 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3430729},
volume = {5},
year = {2012}
}
@article{Bhat2012,
abstract = {Transradial access for cardiac catheterization is now widely accepted among the invasive cardiology community as a safe and viable approach with a markedly reduced incidence of major access-related complications compared with the transfemoral approach. As this access technique is now being used more commonly for cardiac catheterization, it is of paramount importance to be aware of its complications and to understand their prevention and management. Some of the common complications of transradial access include asymptomatic radial artery occlusion, nonocclusive radial artery injury and radial artery spasm. Among these complications, radial artery spasm is still a significant challenge. Symptomatic radial arterial occlusion, pseudoaneurysm and radial artery perforation are rarely reported complications of the transradial approach. Early identification of these rare complications and their immediate management is of vital importance. Arteriovenous fistula, minor nerve damage and complex regional pain syndrome are very rare but have been reported. Recently, granulomas have been reported to be associated with the use of a particular brand of hydrophilic sheaths during the procedure. Generally, access-site complications can be minimized by avoiding multiple punctures, selection of smaller sheaths, gentle catheter manipulation, adequate anticoagulation, use of appropriate compression devices and avoiding prolonged high-pressure compression. In addition, careful observation for any ominous signs such as pain, numbness and hematoma formation during and in the immediate postprocedure period is essential in the prevention of catastrophic hand ischemia.},
author = {Bhat, Tariq and Teli, Sumaya and Bhat, Hilal and Akhtar, Muhammad and Meghani, Mustafain and Lafferty, James and Gala, Bhavesh},
doi = {10.1586/erc.12.16},
issn = {1744-8344},
journal = {Expert review of cardiovascular therapy},
month = {may},
number = {5},
pages = {627--34},
pmid = {22651838},
title = {{Access-site complications and their management during transradial cardiac catheterization.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22651838},
volume = {10},
year = {2012}
}
@article{Nathan2012,
abstract = {Since its advent over two decades ago, transradial access for cardiac catheterization and percutaneous intervention has evolved into a versatile and evidence-based approach for containing the risks of access-site bleeding and vascular complications without compromising the technical range or success associated with contemporary percutaneous coronary intervention (PCI). Early studies demonstrated reduced rates of vascular complications and access-site bleeding with radial-access catheterization but at the cost of increased access-site crossover and reduced procedural success. Contemporary data demonstrate that while the rates of major bleeding with femoral-access PCI in standard-risk cohorts have declined significantly over time, the transradial approach still retains significant advantages by way of reductions in vascular complications, length of stay, and enhanced patient comfort and patient preference over the femoral approach, while maintaining procedural success. Major adverse cardiovascular events and bleeding are lowest with the transradial approach when procedures are performed at high-volume radial centers, by experienced radial operators, or in the context of ST-segment elevation myocardial infarction. Choice of procedural anticoagulation appears to differentially impact access-site bleeding in transradial versus transfemoral PCI; however, non-access site bleeding remains a significant contributor to major bleeding in both groups. Despite abundant supporting data, adoption of transradial technique as the default strategy in cardiac catheterization in the United States has lagged behind many other countries. However, recent trends suggest that interest and adoption of the technique in the United States is growing at a brisker pace than previously observed.},
author = {Nathan, Sandeep and Rao, Sunil V},
doi = {10.1007/s11886-012-0287-5},
issn = {1534-3170},
journal = {Current cardiology reports},
month = {aug},
number = {4},
pages = {502--9},
pmid = {22733412},
title = {{Radial versus femoral access for percutaneous coronary intervention: implications for vascular complications and bleeding.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22733412},
volume = {14},
year = {2012}
}
@article{Burzotta2013,
abstract = {BACKGROUND Access-site vascular complications (ASVC) in patients undergoing trans-radial coronary procedures are rare but may have relevant clinical consequences. Data regarding the optimal management of radial-access-related ASVC are lacking. METHODS During a period of 6 years we prospectively collected ASVC. ASVC were defined as any complication requiring ultrasound examination or upper limb angiography. ASVC were categorized according to the timing of diagnosis: "very early" (in the cath lab), "early" (after cath lab discharge, but during the hospital stay) and "late" (after hospital discharge). The need of surgery (primary end-point) and the development of neurological hand deficit (secondary end-point) were assessed. RESULTS Fifty-seven radial-artery related ASVC were collected. ASVC diagnosis was obtained by upper limb angiography in 25 patients (44%) and by Doppler in 32 patients (56%). Surgery was required in 6 cases (11%), the remaining patients receiving successful conservative management (which included prolonged local compression). Three patients (who received surgery) exhibited a mild neurological hand deficit in the follow-up. Need for surgery differed significantly according to timing of diagnosis as it occurred in 1 of 26 patients (3.8%) with "very early" diagnosis, in 1 of 21 patients (4.8%) with "early" diagnosis, and in 4 of 10 patients (40%) with "late" diagnosis (p=0.026). CONCLUSIONS ASVC are diagnosed with different timing after trans-radial procedures. Conservative management including local compression allows successful management in the majority of ASVC. Prompt recognition is pivotal as late diagnosis is associated to the need for surgery.},
author = {Burzotta, Francesco and Mariani, Luca and Trani, Carlo and Coluccia, Valentina and Brancati, Marta Francesca and Porto, Italo},
doi = {10.1016/j.ijcard.2012.05.017},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Burzotta et al. - 2013 - Management and timing of access-site vascular complications occurring after trans-radial percutaneous coronary.pdf:pdf},
issn = {1874-1754},
journal = {International journal of cardiology},
keywords = {Management of access-site vascular complications,Percutaneous coronary interventions,Trans-radial approach},
month = {sep},
number = {5},
pages = {1973--8},
pmid = {22633677},
publisher = {Elsevier},
title = {{Management and timing of access-site vascular complications occurring after trans-radial percutaneous coronary procedures.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22633677},
volume = {167},
year = {2013}
}
@article{Bourassa2005,
abstract = {The evolution of cardiac catheterization has occurred over at least four centuries. One of the first major steps was the description of the circulation of the blood by William Harvey in 1628. The next milestone was the measurement of arterial pressure by Stephen Hales, one century later. However, the 19th century represented the golden age of cardiovascular physiology, highlighted by the achievements of Carl Ludwig, Etienne-Jules Marey and Claude Bernard, among others. Human cardiac catheterization developed during the 20th century. The first right heart catheterization in a human was performed by Werner Forssmann on himself in 1929. Diagnostic cardiac catheterization was introduced by Andr{\'{e}} Cournand and Dickinson Richards in the early 1940s, and selective coronary angiography was described by Mason Sones in the early 1960s. More recently, with the advent of catheter-based interventions, pioneered by Andreas Gruentzig in the late 1970s, there has been considerable progress in the refinement and expansion of these techniques. Currently, the Sones technique is used only infrequently, and coronary angiography and percutaneous coronary intervention rely mainly on percutaneous femoral and percutaneous radial artery approaches. On the occasion of the 50th anniversary of the Montreal Heart Institute, it seems appropriate to highlight the contribution of this institution in these two areas.},
author = {Bourassa, Martial G},
issn = {0828-282X},
journal = {The Canadian journal of cardiology},
month = {oct},
number = {12},
pages = {1011--4},
pmid = {16234881},
title = {{The history of cardiac catheterization.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16234881},
volume = {21},
year = {2005}
}
@article{Woo2008,
abstract = {Propensity score matching is often used in observational studies to create treatment and control groups with similar distributions of observed covariates. Typically, propensity scores are estimated using logistic regressions that assume linearity between the logistic link and the predictors. We evaluate the use of generalized additive models (GAMs) for estimating propensity scores. We compare logistic regressions and GAMs in terms of balancing covariates using simulation studies with artificial and genuine data. We find that, when the distributions of covariates in the treatment and control groups overlap sufficiently, using GAMs can improve overall covariate balance, especially for higher-order moments of distributions. When the distributions in the two groups overlap insufficiently, GAM more clearly reveals this fact than logistic regression does. We also demonstrate via simulation that matching with GAMs can result in larger reductions in bias when estimating treatment effects than matching with logistic regression.},
author = {Woo, Mi-Ja and Reiter, Jerome P and Karr, Alan F},
doi = {10.1002/sim.3278},
isbn = {02776715\r10970258},
issn = {0277-6715},
journal = {Statistics in Medicine},
month = {aug},
number = {19},
pages = {3805--3816},
pmid = {18366144},
title = {{Estimation of propensity scores using generalized additive models}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18366144},
volume = {27},
year = {2008}
}
@article{Paper2013,
abstract = {Background To maintain the balance between the demand of the body and supply (cardiac output), cardiac performance is tightly regulated via the parasympathetic and sympathetic nervous systems. In heart failure, cardiac output (supply) is decreased due to pathologic remodeling of the heart. To meet the demands of the body, the sympathetic system is activated and catecholamines stimulate $\beta$-adrenergic receptors ($\beta$-ARs) to increase contractile performance and cardiac output. Although this is beneficial in the acute phase, chronic $\beta$-ARs stimulation initiates a cascade of alterations at the cellular level, resulting in a diminished contractile performance of the heart.},
author = {Raposo, Luis and Madeira, S{\'{e}}rgio and {Campante Teles}, Rui and Gabriel, Henrique and Gon{\c{c}}alves, Pedro and Brito, Jo{\~{a}}o and Leal, Silvio and Almeida, Manuel and Mendes, Miguel},
doi = {10.3837/tiis.0000.00.000},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Raposo et al. - 2013 - Neurologic complications after transradial or transfemoral approach for diagnostic and interventional cardiac cat.pdf:pdf},
isbn = {1972952943},
issn = {1972952943},
journal = {Catheterization and Cardiovascular Interventions Neurologic},
pages = {0--34},
pmid = {25071666},
title = {{Neurologic complications after transradial or transfemoral approach for diagnostic and interventional cardiac catheterization: a propensity score analysis of 16,710 cases from a prospective registry}},
year = {2013}
}
@article{Huppler2002,
abstract = {SUMMARY Propensity score methods are used to estimate a treatment effect with observational data. This paper considers the formation of propensity score subclasses by investigating different methods for determining subclass boundaries and the number of subclasses used. We compare several methods: balancing a summary of the observed information matrix and equal-frequency subclasses. Subclasses that balance the inverse variance of the treatment effect reduce the mean squared error of the estimates and maximize the number of usable subclasses.},
author = {{Huppler Hullsiek}, Katherine and Louis, Thomas A},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Huppler Hullsiek, Louis - 2002 - Propensity score modeling strategies for the causal analysis of observational data.pdf:pdf},
journal = {Biostatistics},
keywords = {Bias reduction,Confounding,Observational data,Propensity score methods},
number = {4},
pages = {179--193},
title = {{Propensity score modeling strategies for the causal analysis of observational data}},
volume = {2},
year = {2002}
}
@article{Cochran1968,
author = {Cochran, WG},
journal = {Biometrics},
title = {{The effectiveness of adjustment by subclassification in removing bias in observational studies.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/5683871},
year = {1968}
}
@article{Lunceford2004,
author = {Lunceford, Jared and Davidian, Marie},
doi = {10.1002/sim.1903},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Lunceford et al. - 2004 - Strati{\"{y}}cation and weighting via the propensity score in estimation of causal treatment e ects a comparative s.pdf:pdf},
journal = {Statistics in Medicine},
keywords = {covariate balance,double robustness,inverse-probability-of-treatment-weighted-estimato},
number = {19},
pages = {2937--2960},
title = {{Strati{\"{y}}cation and weighting via the propensity score in estimation of causal treatment e ects: a comparative study}},
volume = {2960},
year = {2004}
}
@article{Lunceford2004a,
author = {Lunceford, Jared K and Lunceford, Jared K and Davidian, Marie and Davidian, Marie},
doi = {10.1002/sim.1903},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Lunceford et al. - 2004 - Strati{\"{y}}cation and weighting via the propensity score in estimation of causal treatment e ects a comparative s.pdf:pdf},
journal = {Statistics in Medicine},
keywords = {covariate balance,double robustness,inverse-probability-of-treatment-weighted-estimato},
number = {19},
pages = {2937--2960},
title = {{Strati{\"{y}}cation and weighting via the propensity score in estimation of causal treatment e ects: a comparative study}},
volume = {2960},
year = {2004}
}
@article{Hirano2001,
abstract = {We consider methods for estimating causal effects of treatments when treatment assignment is unconfounded with outcomes conditional on a possibly large set of covariates. Robins and Rotnitzky (1995) suggested combining regression adjustment with weighting based on the propensity score (Rosenbaum and Rubin, 1983). We adopt this approach, allowing for a flexible specification of both the propensity score and the regression function. We apply these methods to data on the effects of right heart catheterization (RHC) studied in Connors et al (1996), and we find that our estimator gives stable estimates over a wide range of values for the two parameters governing the selection of variables},
author = {Hirano, Keisuke and Imbens, GW},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Hirano, Imbens - 2001 - Estimation of causal effects using propensity score weighting An application to data on right heart catheterizat.pdf:pdf},
journal = {Health Services & Outcomes Research Methodology},
keywords = {casual inference,propensity score,right heart catheterization,treatment effects,variable selection},
number = {3},
pages = {259--278},
title = {{Estimation of causal effects using propensity score weighting: An application to data on right heart catheterization}},
volume = {2},
year = {2001}
}
@article{Firth1993,
author = {Firth, David},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Firth - 1993 - Bias Reduction of Maximum Likelihood.pdf:pdf},
journal = {Biometrika},
number = {1},
pages = {27--38},
title = {{Bias Reduction of Maximum Likelihood}},
url = {http://www.jstor.org/stable/2336755},
volume = {80},
year = {1993}
}
@article{Heinze2002,
abstract = {The phenomenon of separation or monotone likelihood is observed in the fitting process of a logistic model if the likelihood converges while at least one parameter estimate diverges to +/- infinity. Separation primarily occurs in small samples with several unbalanced and highly predictive risk factors. A procedure by Firth originally developed to reduce the bias of maximum likelihood estimates is shown to provide an ideal solution to separation. It produces finite parameter estimates by means of penalized maximum likelihood estimation. Corresponding Wald tests and confidence intervals are available but it is shown that penalized likelihood ratio tests and profile penalized likelihood confidence intervals are often preferable. The clear advantage of the procedure over previous options of analysis is impressively demonstrated by the statistical analysis of two cancer studies.},
author = {Heinze, Georg and Schemper, Michael},
doi = {10.1002/sim.1047},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Heinze, Schemper - 2002 - A solution to the problem of separation in logistic regression.pdf:pdf},
isbn = {1097-0258},
issn = {02776715},
journal = {Statistics in Medicine},
keywords = {Bias reduction,Case-control studies,Infinite estimates,Monotone likelihood,Penalized likelihood,Profile likelihood},
number = {16},
pages = {2409--2419},
pmid = {12210625},
title = {{A solution to the problem of separation in logistic regression}},
volume = {21},
year = {2002}
}
@article{Mazumdar2016,
abstract = {Categorizing prognostic variables is essential for their use in clinical decision-making. Often a single cutpoint that stratifies patients into high-risk and low-risk categories is sought. These categories may be used for making treatment recommendations, determining study eligibility, or to control for varying patient prognoses in the design of a clinical trial. Methods used to categorize variables include: biological determination (most desirable but often unavailable); arbitrary selection of a cutpoint at the median value; graphical examination of the data for a threshold effect; and exploration of all observed values for the one which best separates the risk groups according to a chi-squared test. The last method, called the minimum p-value approach, involves multiple testing which inflates the type I error rates. Several methods for adjusting the inflated p-values have been proposed but remain infrequently used. Exploratory methods for categorization and the minimum p-value approach with its various p-value corrections are reviewed, and code for their easy implementation is provided. The combined use of these methods is recommended, and demonstrated in the context of two cancer-related examples which highlight a variety of the issues involved in the categorization of prognostic variables.},
author = {Mazumdar, M and Glassman, J R},
issn = {0277-6715},
journal = {Statistics in medicine},
month = {jan},
number = {1},
pages = {113--32},
pmid = {10623917},
title = {{Categorizing a prognostic variable: review of methods, code for easy implementation and applications to decision-making about cancer treatments.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10623917},
volume = {19},
year = {2000}
}
@article{Hastie1986,
author = {Hastie, Trevor and Tibshirani, Robert},
issn = {2168-8745},
journal = {Statistical Science},
keywords = {Generalized linear models,nonlinearity,nonparametric regression,partial residuals,smoothing},
language = {EN},
month = {aug},
number = {3},
pages = {297--310},
publisher = {Institute of Mathematical Statistics},
title = {{Generalized Additive Models}},
url = {http://projecteuclid.org/euclid.ss/1177013604},
volume = {1},
year = {1986}
}
@article{Abadie2002,
abstract = {Matching estimators for average treatment effects are widely used in evaluation research despite the fact that their large sample properties have not been established in many cases. In this article, we develop a new framework to analyze the properties of matching estimators and establish a number of new results. First, we show that matching estimators include a conditional bias term which may not vanish at a rate faster than root-N when more than one continuous variable is used for matching. As a result, matching estimators may not be root-N-consistent. Second, we show that even after removing the conditional bias, matching estimators with a fixed number of matches do not reach the semiparametric efficiency bound for average treatment effects, although the efficiency loss may be small. Third, we propose a bias-correction that removes the conditional bias asymptotically, making matching estimators root-N-consistent. Fourth, we provide a new estimator for the conditional variance that does not require consistent nonparametric estimation of unknown functions. We apply the bias-corrected matching estimators to the study of the effects of a labor market program previously analyzed by Lalonde (1986). We also carry out a small simulation study based on Lalonde's example where a simple implementation of the biascorrected matching estimator performs well compared to both simple matching estimators and to regression estimators in terms of bias and root-mean-squared-error. Software for implementing the proposed estimators in STATA and Matlab is available from the authors on the web.},
author = {Abadie, Alberto and Imbens, Guido W.},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Abadie, Imbens - 2002 - Simple and Bias-Corrected Matching Estimators for Average Treatment Effects.pdf:pdf},
journal = {NBER Technical Working Papers},
publisher = {National Bureau of Economic Research, Inc},
title = {{Simple and Bias-Corrected Matching Estimators for Average Treatment Effects}},
url = {http://ideas.repec.org/p/nbr/nberte/0283.html},
year = {2002}
}
@article{Abadie2006,
author = {Abadie, Alberto and Imbens, Guido W.},
doi = {10.1111/j.1468-0262.2006.00655.x},
issn = {0012-9682},
journal = {Econometrica},
month = {jan},
number = {1},
pages = {235--267},
title = {{Large Sample Properties of Matching Estimators for Average Treatment Effects}},
url = {http://doi.wiley.com/10.1111/j.1468-0262.2006.00655.x},
volume = {74},
year = {2006}
}
@article{Sturmer2006,
abstract = {OBJECTIVE: Propensity score (PS) analyses attempt to control for confounding in nonexperimental studies by adjusting for the likelihood that a given patient is exposed. Such analyses have been proposed to address confounding by indication, but there is little empirical evidence that they achieve better control than conventional multivariate outcome modeling.

STUDY DESIGN AND METHODS: Using PubMed and Science Citation Index, we assessed the use of propensity scores over time and critically evaluated studies published through 2003.

RESULTS: Use of propensity scores increased from a total of 8 reports before 1998 to 71 in 2003. Most of the 177 published studies abstracted assessed medications (N=60) or surgical interventions (N=51), mainly in cardiology and cardiac surgery (N=90). Whether PS methods or conventional outcome models were used to control for confounding had little effect on results in those studies in which such comparison was possible. Only 9 of 69 studies (13%) had an effect estimate that differed by more than 20% from that obtained with a conventional outcome model in all PS analyses presented.

CONCLUSIONS: Publication of results based on propensity score methods has increased dramatically, but there is little evidence that these methods yield substantially different estimates compared with conventional multivariable methods.},
author = {St{\"{u}}rmer, Til and Joshi, Manisha and Glynn, Robert J and Avorn, Jerry and Rothman, Kenneth J and Schneeweiss, Sebastian},
doi = {10.1016/j.jclinepi.2005.07.004},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/St{\"{u}}rmer et al. - 2006 - A review of the application of propensity score methods yielded increasing use, advantages in specific settings.pdf:pdf},
issn = {0895-4356},
journal = {Journal of clinical epidemiology},
keywords = {Bias (Epidemiology),Cardiac Surgical Procedures,Cardiac Surgical Procedures: methods,Confounding Factors (Epidemiology),Data Interpretation, Statistical,Databases, Bibliographic,Epidemiologic Methods,Heart Diseases,Heart Diseases: drug therapy,Humans,Models, Statistical,Research Design,Treatment Outcome},
month = {may},
number = {5},
pages = {437--47},
pmid = {16632131},
title = {{A review of the application of propensity score methods yielded increasing use, advantages in specific settings, but not substantially different estimates compared with conventional multivariable methods.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1448214&tool=pmcentrez&rendertype=abstract},
volume = {59},
year = {2006}
}
@article{Normand2001,
abstract = {We determined whether adherence to recommendations for coronary angiography more than 12 h after symptom onset but prior to hospital discharge after acute myocardial infarction (AMI) resulted in better survival. Using propensity scores, we created a matched retrospective sample of 19,568 Medicare patients hospitalized with AMI during 1994-1995 in the United States. Twenty-nine percent, 36%, and 34% of patients were judged necessary, appropriate, or uncertain, respectively, for angiography while 60% of those judged necessary received the procedure during the hospitalization. The 3-year survival benefit was largest for patients rated necessary [mean survival difference (95% CI): 17.6% (15.1, 20.1)] and smallest for those rated uncertain [8.8% (6.8, 10.7)]. Angiography recommendations appear to select patients who are likely to benefit from the procedure and the consequent interventions. Because of the magnitude of the benefit and of the number of patients involved, steps should be taken to replicate these findings.},
author = {Normand, S T and Landrum, M B and Guadagnoli, E and Ayanian, J Z and Ryan, T J and Cleary, P D and McNeil, B J},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Normand et al. - 2001 - Validating recommendations for coronary angiography following acute myocardial infarction in the elderly a match.pdf:pdf},
issn = {0895-4356},
journal = {Journal of clinical epidemiology},
keywords = {Aged,Aged, 80 and over,Algorithms,Coronary Angiography,Coronary Angiography: standards,Coronary Angiography: utilization,Female,Guideline Adherence,Guideline Adherence: standards,Guideline Adherence: statistics & numerical data,Humans,Logistic Models,Male,Matched-Pair Analysis,Medicare,Myocardial Infarction,Myocardial Infarction: mortality,Myocardial Infarction: radiography,Patient Selection,Practice Guidelines as Topic,Practice Guidelines as Topic: standards,Quality Indicators, Health Care,Retrospective Studies,Survival Analysis,Time Factors,Treatment Outcome,United States,United States: epidemiology},
month = {apr},
number = {4},
pages = {387--98},
pmid = {11297888},
title = {{Validating recommendations for coronary angiography following acute myocardial infarction in the elderly: a matched analysis using propensity scores.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/11297888},
volume = {54},
year = {2001}
}
@book{AmaralTurkman2000,
address = {Lisbon},
author = {{Amaral Turkman}, M.Ant{\'{o}}nia and {Loiola Silva}, Giovani},
publisher = {Sociedade Portuguesa de Estat{\'{i}}stica},
title = {{Modelos Lineares Generalizados -da teoria {\`{a}} pr{\'{a}}tica-}},
year = {2000}
}
@article{Austin2011b,
abstract = {In a study comparing the effects of two treatments, the propensity score is the probability of assignment to one treatment conditional on a subject's measured baseline covariates. Propensity-score matching is increasingly being used to estimate the effects of exposures using observational data. In the most common implementation of propensity-score matching, pairs of treated and untreated subjects are formed whose propensity scores differ by at most a pre-specified amount (the caliper width). There has been a little research into the optimal caliper width. We conducted an extensive series of Monte Carlo simulations to determine the optimal caliper width for estimating differences in means (for continuous outcomes) and risk differences (for binary outcomes). When estimating differences in means or risk differences, we recommend that researchers match on the logit of the propensity score using calipers of width equal to 0.2 of the standard deviation of the logit of the propensity score. When at least some of the covariates were continuous, then either this value, or one close to it, minimized the mean square error of the resultant estimated treatment effect. It also eliminated at least 98% of the bias in the crude estimator, and it resulted in confidence intervals with approximately the correct coverage rates. Furthermore, the empirical type I error rate was approximately correct. When all of the covariates were binary, then the choice of caliper width had a much smaller impact on the performance of estimation of risk differences and differences in means.},
author = {Austin, P. C.},
doi = {10.1002/pst.433},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Austin - 2011 - Optimal caliper widths for propensity-score matching when estimating differences in means and differences in proportions.pdf:pdf},
issn = {1539-1612},
journal = {Pharmaceutical statistics},
keywords = {80 and over,Aged,Confidence Intervals,Female,Humans,Male,Monte Carlo Method,Propensity Score,Risk},
month = {jan},
number = {2},
pages = {150--61},
pmid = {20925139},
title = {{Optimal caliper widths for propensity-score matching when estimating differences in means and differences in proportions in observational studies.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3120982&tool=pmcentrez&rendertype=abstract},
volume = {10},
year = {2011}
}
@article{Hill2006,
abstract = {In causal studies without random assignment of treatment, causal effects can be estimated using matched treated and control samples, where matches are obtained using estimated propensity scores. Propensity score matching can reduce bias in treatment effect estimators in cases where the matched samples have overlapping covariate distributions. Despite its application in many applied problems, there is no universally employed approach to interval estimation when using propensity score matching. In this article, we present and evaluate approaches to interval estimation when using propensity score matching.},
author = {Hill, Jennifer and Reiter, Jerome P},
doi = {10.1002/sim.2277},
issn = {0277-6715},
journal = {Statistics in medicine},
keywords = {Adolescent,African Americans,Cognition,Computer Simulation,Data Interpretation, Statistical,Female,Humans,Infant,Infant, Low Birth Weight,Infant, Low Birth Weight: growth & development,Infant, Newborn,Infant, Premature,Infant, Premature: growth & development,Male,Socioeconomic Factors,Treatment Outcome},
month = {jul},
number = {13},
pages = {2230--56},
pmid = {16220488},
title = {{Interval estimation for treatment effects using propensity score matching.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16220488},
volume = {25},
year = {2006}
}
@article{Schafer2008,
abstract = {In a well-designed experiment, random assignment of participants to treatments makes causal inference straightforward. However, if participants are not randomized (as in observational study, quasi-experiment, or nonequivalent control-group designs), group comparisons may be biased by confounders that influence both the outcome and the alleged cause. Traditional analysis of covariance, which includes confounders as predictors in a regression model, often fails to eliminate this bias. In this article, the authors review Rubin's definition of an average causal effect (ACE) as the average difference between potential outcomes under different treatments. The authors distinguish an ACE and a regression coefficient. The authors review 9 strategies for estimating ACEs on the basis of regression, propensity scores, and doubly robust methods, providing formulas for standard errors not given elsewhere. To illustrate the methods, the authors simulate an observational study to assess the effects of dieting on emotional distress. Drawing repeated samples from a simulated population of adolescent girls, the authors assess each method in terms of bias, efficiency, and interval coverage. Throughout the article, the authors offer insights and practical guidance for researchers who attempt causal inference with observational data.},
author = {Schafer, Joseph L and Kang, Joseph},
doi = {10.1037/a0014268},
issn = {1082-989X},
journal = {Psychological methods},
keywords = {Adolescent,Bias (Epidemiology),Causality,Clinical Trials as Topic,Clinical Trials as Topic: statistics & numerical d,Diet,Emotions,Experimental,Experimental: statistics & numerical d,Female,Humans,Models,Outcome Assessment (Health Care),Outcome Assessment (Health Care): statistics & num,Personality Inventory,Personality Inventory: statistics & numerical data,Psychology,Psychometrics,Psychometrics: statistics & numerical data,Reducing,Reducing: psychology,Regression Analysis,Statistical},
month = {dec},
number = {4},
pages = {279--313},
pmid = {19071996},
title = {{Average causal effects from nonrandomized studies: a practical guide and simulated example.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19071996},
volume = {13},
year = {2008}
}
@article{Rubin1973,
author = {Rubin, Donald B},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Rubin - 1973 - Matching to remove bias in observational studies.pdf:pdf},
journal = {Biometrics},
title = {{Matching to Remove Bias in Observational Studies}},
year = {1973}
}
@article{Campbell,
abstract = {"In analyzing the extraneous variables which experimental designs for social settings seek to control, seven categories have been distinguished: history, maturation, testing, instrument decay, regression, selection, and mortality. In general, the simple or main effects of these variables jeopardize the internal validity of the experiment and are adequately controlled in standard experimental designs. The interactive effects of these variables and of experimental arrangements affect the external validity or generalizability of experimental results. Standard experimental designs vary in their susceptibility to these interactive effects." 37 references.},
author = {Campbell, Donald T.},
journal = {Psychological Bulletin},
title = {{Factors relevant to the validity of experiments in social settings.}},
year = {1957}
}
@article{Emura2008,
author = {Emura, Takeshi and Wang, Jingfang and Katsuyama, Hitomi},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Emura, Wang, Katsuyama - 2008 - Assessing the Assumption of Strongly Ignorable Treatment Assignment Under Assumed Causal Models.pdf:pdf},
keywords = {: Counterfactual model of causality,Independence test,Likelihood ratio test,Missing data,Model checking,Propensity score},
title = {{Assessing the Assumption of Strongly Ignorable Treatment Assignment Under Assumed Causal Models}},
url = {http://www.math.s.chiba-u.ac.jp/report/files/08008.pdf},
year = {2008}
}
@incollection{Heckman1999,
abstract = {Policy makers view public sector-sponsored employment and training programs and other active labor market policies as tools for integrating the unemployed and economically disadvantaged into the work force. Few public sector programs have received such intensive scrutiny, and been subjected to so many different evaluation strategies. This chapter examines the impacts of active labor market policies, such as job training, job search assistance, and job subsidies, and the methods used to evaluate their effectiveness. Previous evaluations of policies in OECD countries indicate that these programs usually have at best a modest impact on participants' labor market prospects. But at the same time, they also indicate that there is considerable heterogeneity in the impact of these programs. For some groups, a compelling case can be made that these policies generate high rates of return, while for other groups these policies have had no impact and may have been harmful. Our discussion of the methods used to evaluate these policies has more general interest. We believe that the same issues arise generally in the social sciences and are no easier to address elsewhere. As a result, a major focus of this chapter is on the methodological lessons learned from evaluating these programs. One of the most important of these lessons is that there is no inherent method of choice for conducting program evaluations. The choice between experimental and non-experimental methods or among alternative econometric estimators should be guided by the underlying economic models, the available data, and the questions being addressed. Too much emphasis has been placed on formulating alternative econometric methods for correcting for selection bias and too little given to the quality of the underlying data. Although it is expensive, obtaining better data is the only way to solve the evaluation problem in a convincing way. However, better data are not synonymous with social experiments. {\textcopyright} 1999 Elsevier Science B.V. All rights reserved.},
author = {Heckman, James J. and Lalonde, Robert J. and Smith, Jeffrey A.},
booktitle = {Handbook of Labor Economics},
doi = {10.1016/S1573-4463(99)03012-6},
isbn = {9780444501875},
issn = {15734463},
keywords = {C50,C93,J24,J31,J64},
pages = {1865--2097},
title = {{Chapter 31 The Economics and Econometrics of Active Labor Market Programs}},
url = {http://www.sciencedirect.com/science/article/pii/S1573446399030126},
volume = {3},
year = {1999}
}
@book{Rosenbaum2002,
abstract = {An Observational study is an empiric investigation of the effects caused by a treatment, policy , or intervention in which it is not possible to assign subjects at random to treatment or control, as would be done in a controlled experiment. Observational studies are common in most fields that study the effects of treatments on people. The second edition of ¿Observational Studies¿ is about 50 percent longer than the first edition, with many new examples and methods. There are new chapters on nonadditive models for treatment effects (Chapter 5) and planning observational studies (Chapter 11) and Chapter 9, on coherence, has been extensively rewritten. Paul R. Rosenbaum is Robert G. Putzel Professor, Department of Statistics, The Wharton School of the University of Pennsylvania. He is a fellow of the American Statistical Association.},
address = {New York},
author = {Rosenbaum, Paul R.},
isbn = {0387989676},
publisher = {Springer},
title = {{Observational Studies}},
url = {https://books.google.pt/books/about/Observational_Studies.html?id=K0OglGXtpGMC&pgis=1},
year = {2002}
}
@book{Guo2009,
abstract = {Propensity Score Analysis provides readers with a systematic review of the origins, history, and statistical foundations of PSA and illustrates how it can be used for solving evaluation problems. With a strong focus on practical applications, the authors explore various types of data and evaluation problems related to, strategies for employing, and the limitations of PSA. Unlike the existing textbooks on program evaluation, Propensity Score Analysis delves into statistical concepts, formulas, and models underlying the application.Key FeaturesPresents key information on model derivations Summarizes complex statistical arguments but omits their proofsLinks each method found in this book to specific Stata programs and provides empirical examples Guides readers using two conceptual frameworks: the Neyman-Rubin counterfactual framework and the Heckman econometric model of causality Contains examples representing real challenges commonly found in social behavioral research Utilizes data simulation and Monte Carlo studies to illustrate key points Presents descriptions of new statistical approaches necessary for understanding the four evaluation methods incorporated throughout the textIntended AudienceThis text is appropriate for graduate and doctoral students taking Evaluation, Quantitative Methods, Survey Research, and Research Design courses across business, social work, public policy, psychology, sociology, and health/medicine disciplines.},
address = {Thousand Oaks},
author = {Guo, Shenyang and Fraser, Mark W.},
isbn = {1483342735},
pages = {392},
publisher = {SAGE Publications},
title = {{Propensity Score Analysis: Statistical Methods and Applications}},
url = {https://books.google.com/books?id=kd8gAQAAQBAJ&pgis=1},
year = {2009}
}
@book{Shadish2002,
abstract = {This long awaited successor of the original Cook/Campbell Quasi-Experimentation: Design and Analysis Issues for Field Settings represents updates in the field over the last two decades. The book covers four major topics in field experimentation:Theoretical matters: Experimentation, causation, and validityQuasi-experimental design: Regression discontinuity designs, interrupted time series designs, quasi-experimental designs that use both pretests and control groups, and other designsRandomized experiments: Logic and design issues, and practical problems involving ethics, recruitment, assignment, treatment implementation, and attritionGeneralized causal inference: A grounded theory of generalized causal inference, along with methods for implementing that theory in single and multiple studies},
address = {Belmont},
annote = {internal validity},
author = {Shadish, William R. and Cook, Thomas D. and Campbell, Donald Thomas},
isbn = {0395615569},
publisher = {Wadsworth Cengage Learning},
title = {{Experimental and Quasi-experimental Designs for Generalized Causal Inference}},
url = {https://books.google.pt/books/about/Experimental_and_Quasi_experimental_Desi.html?id=o7jaAAAAMAAJ&pgis=1},
year = {2002}
}
@article{King2016,
author = {King, Gary and Nielsen, Richard},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/King, Nielsen - 2016 - Why propensity score should not be used for matching.pdf:pdf},
number = {617},
title = {{Why propensity score should not be used for matching}},
year = {2016}
}
@article{Engelgau2004,
abstract = {A diabetes epidemic emerged during the 20th century and continues unchecked into the 21st century. It has already taken an extraordinary toll on the U.S. population through its acute and chronic complications, disability, and premature death. Trend data suggest that the burden will continue to increase. Efforts to pre- vent or delay the complications of diabetes or, better yet, to prevent or delay the development of diabetes itself are urgently needed.},
author = {Engelgau, Michael M and Geiss, Linda S and Saaddine, Jinan B and Boyle, James P and Benjamin, Stephanie M and Gregg, Edward W and Tierney, Edward F and Rios-Burrows, Nilka and Mokdad, Ali H and Ford, Earl S and Imperatore, Giuseppina and Narayan, K M Venkat},
issn = {1539-3704},
journal = {Annals of internal medicine},
keywords = {Diabetes Complications,Diabetes Mellitus,Diabetes Mellitus: classification,Diabetes Mellitus: epidemiology,Disease Outbreaks,Forecasting,Humans,Prevalence,Risk Factors,United States},
month = {jun},
number = {11},
pages = {945--50},
pmid = {15172919},
title = {{The evolving diabetes burden in the United States.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15172919},
volume = {140},
year = {2004}
}
@article{FREEMAN2008,
author = {FREEMAN, K},
doi = {10.1016/j.optm.2007.12.009},
isbn = {http://dx.doi.org/10.1016/j.optm.2007.12.009},
issn = {1529-1839},
journal = {Optometry - Journal of the American Optometric Association},
month = {mar},
number = {3},
pages = {116--117},
title = {{Evidence-Based Eye Care, P.J. Kertes, T.M. Johnson Lippincott Williams & Wilkins, Philadelphia (2007), Hardcover, 338 pages, $99}},
url = {https://www.researchgate.net/publication/244826811_Evidence-Based_Eye_Care_PJ_Kertes_TM_Johnson_Lippincott_Williams_Wilkins_Philadelphia_2007_Hardcover_338_pages_99},
volume = {79},
year = {2008}
}
@article{Dutra-Medeiros2015,
author = {Dutra-Medeiros, M and Mesquita, E and Papoila, AL and Genro, V and Raposo, J},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Dutra-Medeiros et al. - 2015 - First Diabetic Retinopathy Prevalence Study in Portugal RETINODIAB Study-Evaluation of the Screening Prog.pdf:pdf},
journal = {British Journal of Ophthalmology},
keywords = {CHLC OFT,Clinical Trial,Epidemiology,Macula,Portugal,Retina,Retrospective Studies,Treatment Lasers},
language = {eng},
publisher = {BMJ Publishing Group},
title = {{First Diabetic Retinopathy Prevalence Study in Portugal: RETINODIAB Study-Evaluation of the Screening Programme for Lisbon and Tagus Valley Region}},
url = {http://repositorio.chlc.min-saude.pt/handle/10400.17/2117},
year = {2015}
}
@inproceedings{Sepulveda2015,
address = {Lisbon},
author = {Sep{\'{u}}lveda, N.},
booktitle = {Semin{\'{a}}rios em Bioestat{\'{i}}stica},
publisher = {London School of Hygiene and Tropical Medicine},
title = {{Measuring Malaria Elimination: Random Ideas for a Deterministic Situation}},
year = {2015}
}
@article{Glynn2006,
abstract = {Use of propensity scores to identify and control for confounding in observational studies that relate medications to outcomes has increased substantially in recent years. However, it remains unclear whether, and if so when, use of propensity scores provides estimates of drug effects that are less biased than those obtained from conventional multivariate models. In the great majority of published studies that have used both approaches, estimated effects from propensity score and regression methods have been similar. Simulation studies further suggest comparable performance of the two approaches in many settings. We discuss five reasons that favour use of propensity scores: the value of focus on indications for drug use; optimal matching strategies from alternative designs; improved control of confounding with scarce outcomes; ability to identify interactions between propensity of treatment and drug effects on outcomes; and correction for unobserved confounders via propensity score calibration. We describe alternative approaches to estimate and implement propensity scores and the limitations of the C-statistic for evaluation. Use of propensity scores will not correct biases from unmeasured confounders, but can aid in understanding determinants of drug use and lead to improved estimates of drug effects in some settings.},
author = {Glynn, Robert J and Schneeweiss, Sebastian and St{\"{u}}rmer, Til},
doi = {10.1111/j.1742-7843.2006.pto_293.x},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Glynn, Schneeweiss, St{\"{u}}rmer - 2006 - Indications for propensity scores and review of their use in pharmacoepidemiology.pdf:pdf},
issn = {1742-7835},
journal = {Basic & clinical pharmacology & toxicology},
keywords = {Bias (Epidemiology),Clinical Trials as Topic,Clinical Trials as Topic: methods,Confounding Factors (Epidemiology),Data Interpretation, Statistical,Drug-Related Side Effects and Adverse Reactions,Humans,Models, Statistical,Outcome Assessment (Health Care),Pharmacoepidemiology,Pharmacoepidemiology: methods,Research Design,Treatment Outcome},
month = {mar},
number = {3},
pages = {253--9},
pmid = {16611199},
title = {{Indications for propensity scores and review of their use in pharmacoepidemiology.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1790968&tool=pmcentrez&rendertype=abstract},
volume = {98},
year = {2006}
}
@article{Targett1970,
author = {Targett, G A},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Targett - 1970 - Antibody response to Plasmodium falciparum malaria. Comparisons of immunoglobulin concentrations, antibody titres and t.pdf:pdf},
issn = {0009-9104},
journal = {Clinical and experimental immunology},
keywords = {Acute Disease,Adult,Animals,Antibodies,Antibodies: analysis,Antibody Formation,Antigens,Child,Child, Preschool,Fluorescent Antibody Technique,Humans,Immunoglobulin G,Immunoglobulin G: analysis,Immunoglobulin M,Immunoglobulin M: analysis,Immunoglobulins,Immunoglobulins: analysis,Infant,Malaria,Malaria: epidemiology,Malaria: immunology,Plasmodium falciparum,Plasmodium falciparum: growth & development,Plasmodium falciparum: immunology,Rabbits,gamma-Globulins,gamma-Globulins: analysis},
month = {oct},
number = {4},
pages = {501--17},
pmid = {4097746},
title = {{Antibody response to Plasmodium falciparum malaria. Comparisons of immunoglobulin concentrations, antibody titres and the antigenicity of different asexual forms of the parasite.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1712858&tool=pmcentrez&rendertype=abstract},
volume = {7},
year = {1970}
}
@article{WHO,
author = {{World Health Organization}},
publisher = {World Health Organization},
title = {{World Malaria Report}},
url = {http://www.who.int/malaria/publications/world_malaria_report_2014/en/},
year = {2015}
}
@article{Liaw2002,
author = {Liaw, Andy and Wiener, Matthew},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Liaw, Wiener - 2002 - Classifcation and Regression by randomForest.pdf:pdf},
number = {December},
pages = {18--22},
title = {{Classifcation and Regression by randomForest}},
volume = {2},
year = {2002}
}
@article{Lee2011,
author = {Lee, Brian K and Lessler, Justin and Stuart, Elizabeth A},
doi = {10.1002/sim.3782.Improving},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Lee, Lessler, Stuart - 2011 - Improving propensity score weighting using machine learning.pdf:pdf},
keywords = {boosting,cart,ensemble methods,machine learning,propensity score,simulation,weighting},
number = {3},
pages = {337--346},
title = {{Improving propensity score weighting using machine learning}},
volume = {29},
year = {2011}
}
@article{Tein2013,
author = {Tein, Jenn-yun and West, Stephen and Mackinnon, David},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Tein, West, Mackinnon - 2013 - Dissertacao - Propensity Score Estimation with random forest.pdf:pdf},
number = {August},
title = {{Dissertacao - Propensity Score Estimation with random forest}},
year = {2013}
}
@article{Austin2007,
abstract = {The propensity score--the probability of exposure to a specific treatment conditional on observed variables--is increasingly being used in observational studies. Creating strata in which subjects are matched on the propensity score allows one to balance measured variables between treated and untreated subjects. There is an ongoing controversy in the literature as to which variables to include in the propensity score model. Some advocate including those variables that predict treatment assignment, while others suggest including all variables potentially related to the outcome, and still others advocate including only variables that are associated with both treatment and outcome. We provide a case study of the association between drug exposure and mortality to show that including a variable that is related to treatment, but not outcome, does not improve balance and reduces the number of matched pairs available for analysis. In order to investigate this issue more comprehensively, we conducted a series of Monte Carlo simulations of the performance of propensity score models that contained variables related to treatment allocation, or variables that were confounders for the treatment-outcome pair, or variables related to outcome or all variables related to either outcome or treatment or neither. We compared the use of these different propensity scores models in matching and stratification in terms of the extent to which they balanced variables. We demonstrated that all propensity scores models balanced measured confounders between treated and untreated subjects in a propensity-score matched sample. However, including only the true confounders or the variables predictive of the outcome in the propensity score model resulted in a substantially larger number of matched pairs than did using the treatment-allocation model. Stratifying on the quintiles of any propensity score model resulted in residual imbalance between treated and untreated subjects in the upper and lower quintiles. Greater balance between treated and untreated subjects was obtained after matching on the propensity score than after stratifying on the quintiles of the propensity score. When a confounding variable was omitted from any of the propensity score models, then matching or stratifying on the propensity score resulted in residual imbalance in prognostically important variables between treated and untreated subjects. We considered four propensity score models for estimating treatment effects: the model that included only true confounders; the model that included all variables associated with the outcome; the model that included all measured variables; and the model that included all variables associated with treatment selection. Reduction in bias when estimating a null treatment effect was equivalent for all four propensity score models when propensity score matching was used. Reduction in bias was marginally greater for the first two propensity score models than for the last two propensity score models when stratification on the quintiles of the propensity score model was employed. Furthermore, omitting a confounding variable from the propensity score model resulted in biased estimation of the treatment effect. Finally, the mean squared error for estimating a null treatment effect was lower when either of the first two propensity scores was used compared to when either of the last two propensity score models was used.},
author = {Austin, P. C. and Grootendorst, Paul and Anderson, Geoffrey M.},
doi = {10.1002/sim.2580},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Austin, Grootendorst, Anderson - 2007 - A comparison of the ability of different propensity score models to balance measured variables b.pdf:pdf},
isbn = {1097-0258},
issn = {02776715},
journal = {Statistics in Medicine},
keywords = {Balance,Monte Carlo simulations,Observational studies,Propensity score},
number = {4},
pages = {734--753},
pmid = {16708349},
title = {{A comparison of the ability of different propensity score models to balance measured variables between treated and untreated subjects: A Monte Carlo study}},
volume = {26},
year = {2007}
}
@article{Austin2006,
author = {Austin, P. C. and Mamdani, Muhammad M.},
doi = {10.1002/sim.2328},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Austin, Mamdani - 2006 - A comparison of propensity score methods a case-study estimating the effectiveness of post-AMI statin use.pdf:pdf},
issn = {0277-6715},
journal = {Statistics in Medicine},
keywords = {acute myocardial infarction,pharmacoepidemiology,propensity score,statins,statistical},
number = {12},
pages = {2084--2106},
title = {{A comparison of propensity score methods: a case-study estimating the effectiveness of post-AMI statin use}},
url = {http://doi.wiley.com/10.1002/sim.2328},
volume = {25},
year = {2006}
}
@article{Hill2009,
author = {Hill, Jennifer},
doi = {10.1002/sim},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Hill - 2008 - Discussion of research using propensity-score matching Comments on ‘A critical appraisal of propensity-score matching.pdf:pdf},
isbn = {2007090091480},
issn = {02776715},
journal = {Statistics in medicine},
number = {15},
pages = {1982--1998},
pmid = {19455509},
title = {{Discussion of research using propensity-score matching: Comments on ‘A critical appraisal of propensity-score matching in the medical literature between 1996 and 2003'}},
volume = {28},
year = {2008}
}
@article{Williamson2012,
abstract = {Estimation of the effect of a binary exposure on an outcome in the presence of confounding is often carried out via outcome regression modelling. An alternative approach is to use propensity score methodology. The propensity score is the conditional probability of receiving the exposure given the observed covariates and can be used, under the assumption of no unmeasured confounders, to estimate the causal effect of the exposure. In this article, we provide a non-technical and intuitive discussion of propensity score methodology, motivating the use of the propensity score approach by analogy with randomised studies, and describe the four main ways in which this methodology can be implemented. We carefully describe the population parameters being estimated - an issue that is frequently overlooked in the medical literature. We illustrate these four methods using data from a study investigating the association between maternal choice to provide breast milk and the infant's subsequent neurodevelopment. We outline useful extensions of propensity score methodology and discuss directions for future research. Propensity score methods remain controversial and there is no consensus as to when, if ever, they should be used in place of traditional outcome regression models. We therefore end with a discussion of the relative advantages and disadvantages of each.},
author = {Williamson, E. and Morley, R. and Lucas, A. and Carpenter, J.},
doi = {10.1177/0962280210394483},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Williamson et al. - 2011 - Propensity scores From naive enthusiasm to intuitive understanding.pdf:pdf},
isbn = {1477-0334 (Electronic)\n0962-2802 (Linking)},
issn = {0962-2802},
journal = {Statistical Methods in Medical Research},
number = {3},
pages = {273--293},
pmid = {21262780},
title = {{Propensity scores: From naive enthusiasm to intuitive understanding}},
volume = {21},
year = {2011}
}
@article{Austin2008,
author = {Austin, P. C.},
doi = {10.1002/sim.3150},
isbn = {0277-6715},
journal = {Statistics in Medicine},
number = {12},
title = {{A critical appraisal of propensity-score matching in the medical literature between 1996 and 2003}},
url = {http://search.ebscohost.com/login.aspx?direct=true&db=edselc&AN=edselc.2-52.0-44649173785&site=eds-live},
volume = {27},
year = {2008}
}
@article{Liaw2015,
author = {Liaw, Andy and Wiener, Matthew},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Liaw, Wiener - 2015 - Package ‘ randomForest '.pdf:pdf},
title = {{Package ‘ randomForest '}},
year = {2015}
}
@inproceedings{Marques2015,
address = {Lisboa},
author = {Marques, Tiago},
booktitle = {Semin{\'{a}}rios em Bioestat{\'{i}}stica},
publisher = {Faculdade de Ci{\^{e}}ncias da Universidade de Lisboa},
title = {{Conceitos estat{\'{i}}sticos na estima{\c{c}}{\~{a}}o da abund{\^{a}}ncia de ursos polares no Mar de Barentz}},
year = {2015}
}
@inproceedings{Cabral2015,
address = {Lisboa},
author = {Cabral, Salom{\'{e}}},
booktitle = {Semin{\'{a}}rios em Bioestat{\'{i}}stica},
publisher = {Faculdade de Ci{\^{e}}ncias da Universidade de Lisboa},
title = {{Influence of lipids on the development of mammary adenocarcinomas. A longitudinal study}},
year = {2015}
}
@article{Rosenbaum1984,
author = {Rosenbaum, Paul R and Rubin, Donald B},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Rosenbaum, Rubin - 1984 - Reducing bias in observational studies using subclassification on the propensity score.pdf:pdf},
isbn = {0162-1459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
number = {387},
pages = {516--524},
title = {{Reducing bias in observational studies using subclassification on the propensity score}},
volume = {79},
year = {1984}
}
@book{Gortmaker1994,
abstract = {Contents: Contents: 1. Introduction to the Logistic Regression Model Fitting the Logistic Regression Model Testing for the Significance of the Coefficients Confidence Interval Estimation Other Methods of Estimation 2. Multiple Logistic Regression Multiple Logistic Regression Model Fitting the Multiple Logistic Regression Model Testing for the Significance of the Model Confidence Interval Estimation Other Methods of Estimation 3. Interpretation of the Fitted Logistic Regression Model Dichotomous Independent Variable Polychotomous Independent Variable Continuous Independent Variable Multivariable Model Interaction and Confounding Estimation of Odds Ratios in the Presence of Interaction Comparison of Logistic Regression and Stratified Analysis for 2 x 2 Tables Interpretation of the Fitted Values 4. Model-Building Strategic and Methods for Logistic Regression ; Contents: Contents: Variable Selection Stepwise Logistic Regression Best Subsets Logistic Regression Numerical Problems 5. Assessing the Fit of the Model Summary Measures of Goodness-of-Fit Logistic Regression Diagnostics Assessment of Fit via External Validation Interpretation and Presentation of Results from a Fitted Logistic Regression Model 6. Application of Logistic Regression with Different Sampling Models Cohort Studies Case-Control Studies Fitting Logistic Regression Models to Data from Complex Sample Surveys 7. Logistic Regression for Matched Case-Control Studies Logistic Regression Analysis for the I-1 Matched Study Example of the Use of the Logistic Regression Model in a I-1 Matched Study Assessment of Fit in a Matched Study Example of the Use of the Logistic Regression Model in a I-M Matched Study Methods for Assessment of Fit in a I-M Matched Study ; Contents: Contents: Example of Assessment of Fit in a I-M Matched Study 8. Special Topics ; Contents: Multinominal Logistic Regression Model Ordinal Logistic Regression Models Logistic Regression Models for the Analysis of Correlated Data Exact Methods for Logistic Regression Models Sample Size Issues When Fitting Logistic Regression Models.; Contents: Contents: 1. Introduction to the Logistic Regression Model Fitting the Logistic Regression Model Testing for the Significance of the Coefficients Confidence Interval Estimation Other Methods of Estimation 2. Multiple Logistic Regression Multiple Logistic Regression Model Fitting the Multiple Logistic Regression Model Testing for the Significance of the Model Confidence Interval Estimation Other Methods of Estimation 3. Interpretation of the Fitted Logistic Regression Model Dichotomous Independent Variable Polychotomous Independent Variable Continuous Independent Variable Multivariable Model Interaction and Confounding Estimation of Odds Ratios in the Presence of Interaction Comparison of Logistic Regression and Stratified Analysis for 2 x 2 Tables Interpretation of the Fitted Values 4. Model-Building Strategic and Methods for Logistic Regression ; Contents: Contents: Variable Selection Stepwise Logistic Regression Best Subsets Logistic Regression Numerical Problems 5. Assessing the Fit of the Model Summary Measures of Goodness-of-Fit Logistic Regression Diagnostics Assessment of Fit via External Validation Interpretation and Presentation of Results from a Fitted Logistic Regression Model 6. Application of Logistic Regression with Different Sampling Models Cohort Studies Case-Control Studies Fitting Logistic Regression Models to Data from Complex Sample Surveys 7. Logistic Regression for Matched Case-Control Studies Logistic Regression Analysis for the I-1 Matched Study Example of the Use of the Logistic Regression Model in a I-1 Matched Study Assessment of Fit in a Matched Study Example of the Use of the Logistic Regression Model in a I-M Matched Study Methods for Assessment of Fit in a I-M Matched Study ; Contents: Contents: Example of Assessment of Fit in a I-M Matched Study 8. Special Topics ; Contents: Multinominal Logistic Regression Model Ordinal Logistic Regression Models Logistic Regression Models for the Analysis of Correlated Data Exact Methods for Logistic Regression Models Sample Size Issues When Fitting Logistic Regression Models.},
address = {New York},
annote = {Resumo Geral dos PS.
V{\'{a}}rias Aplica{\c{c}}{\~{o}}es (estrat, iptw, matching, cov. adjust.)},
author = {Gortmaker, Steven L. and Hosmer, David W. and Lemeshow, Stanley},
booktitle = {Contemporary Sociology},
doi = {10.2307/2074954},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Gortmaker, Hosmer, Lemeshow - 1994 - Applied Logistic Regression.pdf:pdf},
isbn = {0471356328},
issn = {00943061},
number = {1},
pages = {159},
pmid = {11970842},
publisher = {Wiley},
title = {{Applied Logistic Regression.}},
volume = {23},
year = {1994}
}
@book{Gordis2014,
abstract = {This book is an introduction to epidemiology and to the epidemiologic approach to problems of health and disease. The basic principles and methods of epidemiology are presented together with many examples of the applications of epidemiology to public health and clinical practice. The},
address = {Philadelphia},
author = {Gordis, Leon},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Gordis - 2014 - Epidemiology.pdf:pdf},
isbn = {9781455702824},
pages = {pp 392},
publisher = {Elsevier Health Sciences},
title = {{Epidemiology}},
year = {2014}
}
@article{Austin2011,
abstract = {The propensity score is the probability of treatment assignment conditional on observed baseline characteristics. The propensity score allows one to design and analyze an observational (nonrandomized) study so that it mimics some of the particular characteristics of a randomized controlled trial. In particular, the propensity score is a balancing score: conditional on the propensity score, the distribution of observed baseline covariates will be similar between treated and untreated subjects. I describe 4 different propensity score methods: matching on the propensity score, stratification on the propensity score, inverse probability of treatment weighting using the propensity score, and covariate adjustment using the propensity score. I describe balance diagnostics for examining whether the propensity score model has been adequately specified. Furthermore, I discuss differences between regression-based methods and propensity score-based methods for the analysis of observational data. I describe different causal average treatment effects and their relationship with propensity score analyses.},
author = {Austin, P. C.},
doi = {10.1080/00273171.2011.568786},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Austin - 2011 - An Introduction to Propensity Score Methods for Reducing the Effects of Confounding in Observational Studies.pdf:pdf},
isbn = {0027-3171},
issn = {0027-3171},
journal = {Multivariate behavioral research},
number = {3},
pages = {399--424},
pmid = {21818162},
title = {{An Introduction to Propensity Score Methods for Reducing the Effects of Confounding in Observational Studies.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3144483&tool=pmcentrez&rendertype=abstract},
volume = {46},
year = {2011}
}
@article{Vaughan2015,
author = {Vaughan, Adam S and Kelley, Colleen F and Luisi, Nicole and Rio, Carlos and Sullivan, Patrick S and Rosenberg, Eli S},
doi = {10.1186/s12874-015-0017-y},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Vaughan et al. - 2015 - An application of propensity score weighting to quantify the causal effect of rectal sexually transmitted infect.pdf:pdf},
issn = {1471-2288},
journal = {BMC Medical Research Methodology},
keywords = {HIV,Marginal structural models,Men who have sex with men,Propensity scores,STI,Survival analysis,asvaugh,correspondence,edu,emory,hiv,marginal structural models,men who have sex,propensity scores,sti,survival analysis,with men},
number = {1},
pages = {25},
pmid = {25888416},
title = {{An application of propensity score weighting to quantify the causal effect of rectal sexually transmitted infections on incident HIV among men who have sex with men}},
volume = {15},
year = {2015}
}
@article{Deviation2013,
author = {Deviation, Standard and Dev, Std and Value, Good},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Deviation, Dev, Value - 2013 - {\ldots}a Resource for Survey Researchers.pdf:pdf},
pages = {5--6},
title = {{{\ldots}a Resource for Survey Researchers}},
year = {2013}
}
@article{Imbens2004,
abstract = {Recently there has been a surge in econometric work focusing on estimating average treatment effects under various sets of assumptions. One strand of this literature has developed methods for estimating average treatment effects for a binary treatment under assumptions variously described as exogeneity, unconfoundedness, or selection on observables. The implication of these assumptions is that systematic (for example, average or distributional) differences in outcomes between treated and control units with the same values for the covariates are attributable to the treatment. Recent analysis has considered estimation and inference for average treatment effects under weaker assumptions than typical of the earlier literature by avoiding distributional and functional-form assump- tions. Various methods of semiparametric estimation have been proposed, including estimating the unknown regression functions, matching, meth- ods using the propensity score such as weighting and blocking, and combinations of these approaches. In this paper I review the state of this literature and discuss some of its unanswered questions, focusing in particular on the practical implementation of these methods, the plausi- bility of this exogeneity assumption in economic applications, the relative performance of the various semiparametric estimators when the key assumptions (unconfoundedness and overlap) are satis? ed, alternative estimands such as quantile treatment effects, and alternate methods such as Bayesian inference.},
annote = {Imbens described a method to estimate either the ATE or the ATT when using covariate adjustment using the propensity score,17 his approach is rarely implemented. Propensity score covariate adjustment, as typically imple- mented, does not permit estimation of either the ATE or the ATT.},
author = {Imbens, Guido W},
doi = {10.1162/003465304323023651},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Imbens - 2004 - Nonparametric Estimation of Average Treatment Effects Under Exogeneity a Review.pdf:pdf},
isbn = {0034-6535},
issn = {0034-6535},
journal = {The Review of Economics and Statistics},
number = {1},
pages = {4--29},
pmid = {12908330},
title = {{Nonparametric Estimation of Average Treatment Effects Under Exogeneity : a Review}},
volume = {86},
year = {2004}
}
@article{Chang2012,
author = {Chang, Chiao-chih and Butar, Ferry Butar},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Chang, Butar - 2012 - Weighting Methods in Survey Sampling.pdf:pdf},
journal = {Section on Survey Research Methods-JSM},
keywords = {adjustment,calibration,missing data,nonresponse},
pages = {4768--4782},
title = {{Weighting Methods in Survey Sampling}},
url = {https://www.amstat.org/sections/srms/Proceedings/y2012/Files/306345_76367.pdf},
year = {2012}
}
@misc{Champ2014,
author = {Champ, Colin E},
doi = {10.1089/act.2014.20604},
isbn = {1076-2809},
number = {6},
title = {{Nutritional Approaches for Cancer Prevention and Treatment.}},
url = {http://search.ebscohost.com/login.aspx?direct=true&db=rzh&AN=103862464&site=eds-live},
volume = {20},
year = {2014}
}
@article{LaVecchia1992,
abstract = {The association between fat intake and several common cancers, eg, those of the colorectum, breast, endometrium, ovary, and prostate, received its strongest support from correlation studies on populations. On an international scale, strong direct correlations were observed between fat intake and incidence or mortality from these neoplasms; several correlations were also observed on a national level and persisted after allowance for major identified covariates. Further support came from the observation of change in rates in migrant groups. This association, however, has been described as being weak in individuals as opposed to populations. Briefly, diets high in fat (and meat) have been associated with high risk of colorectal cancer in several case-control studies, with saturated fat being specifically implicated. However, the strength of the association is generally moderate, and a few disparities have to be considered. In relation to breast cancer, several case-control studies have reported associations with total fat, and there was some indication that the associations might be stronger for saturated fat. These relationships, however, were weak and inconsistent in various studies. Thus, a plausible conclusion from case-control studies is that there is indeed some association between fats and breast cancer risk, which is, however, limited and, hence, extremely difficult to prove in epidemiological terms. To further complicate the issue, the results of cohort studies do not support the association. Data from analytical studies are more limited for endometrial, ovarian, and prostate cancers, but, again, seem to indicate a possible relationship with diets high in fat.(ABSTRACT TRUNCATED AT 250 WORDS)},
author = {{La Vecchia}, C},
issn = {1052-6773},
journal = {Journal of the National Cancer Institute. Monographs},
keywords = {Breast Neoplasms,Breast Neoplasms: etiology,Colorectal Neoplasms,Colorectal Neoplasms: etiology,Dietary Fats,Dietary Fats: adverse effects,Endometrial Neoplasms,Endometrial Neoplasms: etiology,Female,Humans,Male,Neoplasms,Neoplasms: etiology,Ovarian Neoplasms,Ovarian Neoplasms: etiology,Prostatic Neoplasms,Prostatic Neoplasms: etiology},
month = {jan},
number = {12},
pages = {79--85},
pmid = {1616815},
title = {{Cancers associated with high-fat diets.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/1616815},
year = {1992}
}
@article{Fitzmaurice2015,
abstract = {IMPORTANCE: Cancer is among the leading causes of death worldwide. Current estimates of cancer burden in individual countries and regions are necessary to inform local cancer control strategies. OBJECTIVE: To estimate mortality, incidence, years lived with disability (YLDs), years of life lost (YLLs), and disability-adjusted life-years (DALYs) for 28 cancers in 188 countries by sex from 1990 to 2013. EVIDENCE REVIEW: The general methodology of the Global Burden of Disease (GBD) 2013 study was used. Cancer registries were the source for cancer incidence data as well as mortality incidence (MI) ratios. Sources for cause of death data include vital registration system data, verbal autopsy studies, and other sources. The MI ratios were used to transform incidence data to mortality estimates and cause of death estimates to incidence estimates. Cancer prevalence was estimated using MI ratios as surrogates for survival data; YLDs were calculated by multiplying prevalence estimates with disability weights, which were derived from population-based surveys; YLLs were computed by multiplying the number of estimated cancer deaths at each age with a reference life expectancy; and DALYs were calculated as the sum of YLDs and YLLs. FINDINGS: In 2013 there were 14.9 million incident cancer cases, 8.2 million deaths, and 196.3 million DALYs. Prostate cancer was the leading cause for cancer incidence (1.4 million) for men and breast cancer for women (1.8 million). Tracheal, bronchus, and lung (TBL) cancer was the leading cause for cancer death in men and women, with 1.6 million deaths. For men, TBL cancer was the leading cause of DALYs (24.9 million). For women, breast cancer was the leading cause of DALYs (13.1 million). Age-standardized incidence rates (ASIRs) per 100 000 and age-standardized death rates (ASDRs) per 100 000 for both sexes in 2013 were higher in developing vs developed countries for stomach cancer (ASIR, 17 vs 14; ASDR, 15 vs 11), liver cancer (ASIR, 15 vs 7; ASDR, 16 vs 7), esophageal cancer (ASIR, 9 vs 4; ASDR, 9 vs 4), cervical cancer (ASIR, 8 vs 5; ASDR, 4 vs 2), lip and oral cavity cancer (ASIR, 7 vs 6; ASDR, 2 vs 2), and nasopharyngeal cancer (ASIR, 1.5 vs 0.4; ASDR, 1.2 vs 0.3). Between 1990 and 2013, ASIRs for all cancers combined (except nonmelanoma skin cancer and Kaposi sarcoma) increased by more than 10% in 113 countries and decreased by more than 10% in 12 of 188 countries. CONCLUSIONS AND RELEVANCE: Cancer poses a major threat to public health worldwide, and incidence rates have increased in most countries since 1990. The trend is a particular threat to developing nations with health systems that are ill-equipped to deal with complex and expensive cancer treatments. The annual update on the Global Burden of Cancer will provide all stakeholders with timely estimates to guide policy efforts in cancer prevention, screening, treatment, and palliation.},
author = {Fitzmaurice, Christina and Dicker, Daniel and Pain, Amanda and Hamavid, Hannah and Moradi-Lakeh, Maziar and MacIntyre, Michael F and ... and Naghavi, Mohsen},
doi = {10.1001/jamaoncol.2015.0735},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Fitzmaurice et al. - 2015 - The Global Burden of Cancer 2013.pdf:pdf},
issn = {2374-2445},
journal = {JAMA oncology},
month = {jul},
number = {4},
pages = {505--27},
pmid = {26181261},
publisher = {American Medical Association},
title = {{The Global Burden of Cancer 2013.}},
url = {http://oncology.jamanetwork.com/article.aspx?articleid=2294966#ArticleInformation},
volume = {1},
year = {2015}
}
@article{Ridgeway2007,
abstract = {Boosting takes on various forms with different programs using different loss functions, different base models, and different optimization schemes. The gbm package takes the approach described in 2 and 3. Some of the terminology differs, mostly due to an effort to cast boosting terms into more standard statistical terminology (e.g. deviance). In addition, the gbm package implements boosting for models commonly used in statistics but not commonly associated with boosting. The Cox proportional hazard model, for example, is an incredibly useful model and the boosting framework applies quite readily with only slight modification 5. Also some algorithms implemented in the gbm package differ from the standard implementation. The AdaBoost algorithm 1 has a particular loss function and a particular optimization algorithm associated with it. The gbm implementation of AdaBoost adopts AdaBoosts exponential loss function (its bound on misclassification rate) but uses Friedmans gradient descent algorithm rather than the original one proposed. So the main purposes of this document is to spell out in detail what the gbm package implements.},
author = {Ridgeway, Greg},
doi = {10.1111/j.1467-9752.1996.tb00390.x},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Ridgeway - 2007 - Generalized Boosted Models A guide to the gbm package.pdf:pdf},
issn = {14679752},
journal = {Compute},
number = {4},
pages = {1--12},
title = {{Generalized Boosted Models : A guide to the gbm package}},
url = {http://cran.r-project.org/web/packages/gbm/vignettes/gbm.pdf},
volume = {1},
year = {2007}
}
@article{Generalized2015,
author = {Generalized, Title and Regression, Boosted},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Generalized, Regression - 2015 - Package ‘ gbm '.pdf:pdf},
title = {{Package ‘ gbm '}},
year = {2015}
}
@article{Rubin1983,
author = {Rubin, D.B. and Rosenbaum, Paul R.},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Rubin, Rosenbaum - 1983 - a. The Central Role of the Propensity Score in Observational Studies for Causal Effects.pdf:pdf},
journal = {Biometrika},
number = {1},
pages = {41--55},
title = {{a. The Central Role of the Propensity Score in Observational Studies for Causal Effects}},
volume = {70 SRC -},
year = {1983}
}
@article{Marjanovic2009,
annote = {Different estimates calculating PS (logit or CART)

GBM modelos complexos para estimar PS

Diferen{\c{c}}a entre ATE e ATT},
author = {Marjanovic, Sonja and Hanney, Stephen and Wooding, Steven},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Marjanovic, Hanney, Wooding - 2009 - Toolkit for Weighting and Analysis of Nonequivalent Groups A tutorial for the twang package.pdf:pdf},
isbn = {9780833050632},
journal = {RAND Corporation},
title = {{Toolkit for Weighting and Analysis of Nonequivalent Groups: A tutorial for the twang package}},
year = {2009}
}
@article{Ridgeway2015,
abstract = {This package offers functions for propensity score estimating and weighting, nonresponse weighting, and diagnosis of the weights. This},
author = {Ridgeway, Author Greg and Mccaffrey, Dan and Morral, Andrew and Ann, Beth and Burgette, Lane},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Ridgeway et al. - 2015 - Package ‘ twang '.pdf:pdf},
title = {{Package ‘ twang '}},
year = {2015}
}
@misc{Multivariate2015,
author = {Multivariate, Title},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Multivariate - 2015 - Package ‘ Matching '.pdf:pdf},
title = {{Package ‘ Matching '}},
year = {2015}
}
@article{Diamond2012a,
abstract = {Genetic matching is a new method for performing multivariate matching which uses an evolutionary search algorithm to determine the weight each covariate is given. The method utilizes an evolutionary algorithm developed by Mebane and Sekhon (1998; Sekhon and Mebane 1998) that maximizes the balance of observed potential confounders across matched treated and control units. The method is nonparametric and does not depend on knowing or estimating the propensity score, but the method is greatly improved when a known or estimated propensity score is incorporated. Genetic matching reliably reduces both the bias and the mean square error of the estimated causal effect even when the property of equal percent bias reduction (EPBR) does not hold. When this property does not hold, matching methods—such as Mahalanobis distance and propensity score matching—often perform poorly. Even if the EPBR property does hold and the propensity score is correctly specified, in finite samples, estimates based on genetic matching have lower mean square error than those based on the usual matching methods. We present a reanalysis of the LaLonde (1986) job training dataset which demonstrates the benefits of genetic matching and which helps to resolve a longstanding debate between Dehejia and Wahba (1997; 1999; 2002; Dehejia 2005) and Smith and Todd (2001, 2005a,b) over the ability of matching to overcome LaLonde's critique of nonexperimental estimators. Monte Carlos are also presented to demonstrate the properties of our method.},
author = {Diamond, Alexis and Sekhon, Js},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Diamond, Sekhon - 2012 - Genetic Matching for Estimating Causal Effects(2).pdf:pdf},
journal = {The Review of Economics and Statistics},
number = {July},
pages = {932--945},
title = {{Genetic Matching for Estimating Causal Effects}},
url = {http://sekhon.polisci.berkeley.edu/papers/GenMatch.pdf},
volume = {95},
year = {2012}
}
@article{CastrodelaGuardia2013,
abstract = {The primary habitat of polar bears is sea ice, but in Western Hudson Bay (WH), the seasonal ice cycle forces polar bears ashore each summer. Survival of bears on land in WH is correlated with breakup and the ice-free season length, and studies suggest that exceeding thresholds in these variables will lead to large declines in the WH population. To estimate when anthropogenic warming may have progressed sufficiently to threaten the persistence of polar bears in WH, we predict changes in the ice cycle and the sea ice concentration (SIC) in spring (the primary feeding period of polar bears) with a high-resolution sea ice-ocean model and warming forced with 21st century IPCC greenhouse gas (GHG) emission scenarios: B1 (low), A1B (medium), and A2 (high). We define critical years for polar bears based on proposed thresholds in breakup and ice-free season and we assess when ice-cycle conditions cross these thresholds. In the three scenarios, critical years occur more commonly after 2050. From 2001 to 2050, 2 critical years occur under B1 and A2, and 4 under A1B; from 2051 to 2100, 8 critical years occur under B1, 35 under A1B and 41 under A2. Spring SIC in WH is high (>90%) in all three scenarios between 2001 and 2050, but declines rapidly after 2050 in A1B and A2. From 2090 to 2100, the mean spring SIC is 84 (±7)% in B1, 56 (±26)% in A1B and 20 (±13)% in A2. Our predictions suggest that the habitat of polar bears in WH will deteriorate in the 21st century. Ice predictions in A1B and A2 suggest that the polar bear population may struggle to persist after ca. 2050. Predictions under B1 suggest that reducing GHG emissions could allow polar bears to persist in WH throughout the 21st century.},
author = {{Castro de la Guardia}, Laura and Derocher, Andrew E and Myers, Paul G and {Terwisscha van Scheltinga}, Arjen D and Lunn, Nick J},
doi = {10.1111/gcb.12272},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Castro de la Guardia et al. - 2013 - Future sea ice conditions in Western Hudson Bay and consequences for polar bears in the 21st centur.pdf:pdf},
isbn = {1365-2486},
issn = {13541013},
journal = {Global Change Biology},
keywords = {Breakup,Climate change,Global warming,Hudson Bay,Ice-free season,Polar bear,Sea ice concentration,Ursus maritimus},
number = {9},
pages = {2675--2687},
pmid = {23716301},
title = {{Future sea ice conditions in Western Hudson Bay and consequences for polar bears in the 21st century}},
volume = {19},
year = {2013}
}
@article{Thomas2006,
abstract = {Distance sampling is a widely-used group of closely related methods for estimating the density and/or abundance of biological populations. The main meth- ods are line transects and point transects (also called variable circular plots). These have been used suc- cessfully in a very diverse array of taxa, including trees, shrubs and herbs, insects, amphibians, reptiles, birds, fish, marine and land mammals. In both cases, the basic idea is the same. The observer(s) perform a standardized survey along a series of lines or points, searching for objects of interest (usually animals or clusters of animals). For each object detected, they record the distance from the line or point to the object. Not all the objects that the observers pass will be detected, but a fundamental assumption of the basic methods is that all objects that are actually on the line or point are detected. Intuitively, one would expect that objects become harder to detect with increasing distance from the line or point, resulting in fewer detections with increasing distance. The key to dis- tance sampling analyses is to fit a detection function to the observed distances, and use this fitted func- tion to estimate the proportion of objects missed by the survey. From here we can readily obtain point and interval estimates for the density and abundance of objects in the survey area. The basic methods (sometimes called conventional or standard distance sampling) are described in detail in [5], which is an updated version of [4]. Free software, Distance [19], provides for the design and analysis of distance sam- pling surveys, and is being updated to include much of the work mentioned in the section on Current Research below.},
author = {Thomas, Len and Buckland, Stepen T. and Burnham, Kenneth P. and Anderson, David R. and Laake, Jeffrey L. and Borchers, David L. and Strindberg, Samantha},
doi = {10.2307/2532812},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Thomas et al. - 2006 - Distance sampling.pdf:pdf},
isbn = {0471 899976},
issn = {0006341X},
journal = {Encyclopedia of Environmetrics},
keywords = {552,anderson,borchers,buckland,burnham,david l,david r,jeffrey l,kenneth p,laake,len thomas,pp 544,samantha strindberg,stephen t,tance sampling,volume 1},
pages = {544--552},
pmid = {248},
title = {{Distance sampling}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/9780470057339.vad033/full},
volume = {1},
year = {2006}
}
@article{Larue2007,
author = {Larue, Michelle a and Nielsen, Clayton K and Grund, Marrett D},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Larue, Nielsen, Grund - 2007 - Using Distance Sampling to Estimate Densities of White-Tailed Deer in South-Central Minnesota.pdf:pdf},
journal = {The Prairie Naturalist},
pages = {57--68},
title = {{Using Distance Sampling to Estimate Densities of White-Tailed Deer in South-Central Minnesota}},
volume = {39},
year = {2007}
}
@article{Hirst1994,
abstract = {The problem of estimating animal populations from the numbers in successive catches has been addressed by many authors. Most have assumed asymptotic normality of maximum likelihood estimates to construct confidence intervals. This assumption can be shown to fail in many common practical situations. We construct intervals based on the profile likelihood ratio and show, by stimulation, that their true confidence is much closer to the nominal value than that of previous methods.},
author = {Hirst, D},
doi = {10.2307/2533392},
isbn = {0006-341X},
issn = {0006-341X},
journal = {Biometrics},
number = {2},
pages = {501--505},
pmid = {8068849},
title = {{An improved removal method for estimating animal abundance.}},
volume = {50},
year = {1994}
}
@book{Buckland2004,
address = {Oxford},
author = {Buckland, S.T. and Anderson, D.R. and Burnham, K.P. and Laake, J.L. and Borchers, D.L. and Thomas, L.},
publisher = {Oxford University Press},
title = {{Advanced Distance Sampling}},
url = {https://global.oup.com/academic/product/advanced-distance-sampling-9780199225873?cc=pt&lang=en&},
year = {2004}
}
@article{Cassey1999,
author = {Cassey, Phillip and McArdle, Brian H.},
doi = {10.1002/(SICI)1099-095X(199905/06)10:3<261::AID-ENV351>3.0.CO;2-O},
issn = {1180-4009},
journal = {Environmetrics},
month = {may},
number = {3},
pages = {261--278},
title = {{An assessment of distance sampling techniques for estimating animal abundance}},
url = {http://doi.wiley.com/10.1002/%28SICI%291099-095X%28199905/06%2910%3A3%3C261%3A%3AAID-ENV351%3E3.0.CO%3B2-O},
volume = {10},
year = {1999}
}
@article{Aus2015,
annote = {describes weights for ATT and ATE estimation

4 assumptions on propensity scores methods

the evolution of IPTW

Standardized differencce in weghted samples and unweighted

v{\'{a}}rias especifica{\c{c}}{\~{o}}es de modelos de propensity scroes

v{\'{a}}rias representa{\c{c}}{\~{o}}es graficas de balanceamento de variaveis

.While several applied studies have reported the area under the receiver operating characteristic curve of the propensity score model (equivalent to the model c-statistic), recent research has indicated that this does not serve as a test of whether the propensity scoremodel has been correctly specified [10,49].},
author = {Austin, P. C. and Stuart, E. A.},
doi = {10.1002/sim.6607},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Austin, Stuart - 2015 - Moving towards best practice when using inverse probability of treatment weighting (IPTW) using the propensity s.pdf:pdf},
isbn = {0277-6715},
number = {28},
title = {{Moving towards best practice when using inverse probability of treatment weighting (IPTW) using the propensity score to estimate causal treatment effects in observational studies.}},
url = {http://search.ebscohost.com/login.aspx?direct=true&db=rzh&AN=110589958&site=eds-live},
volume = {34},
year = {2015}
}
@article{Sekhon2011,
abstract = {Matching is an R package which provides functions for multivariate and propensity score matching and for finding optimal covariate balance based on a genetic search algo- rithm. A variety of univariate and multivariate metrics to determine if balance actually has been obtained are provided. The underlying matching algorithm is written in C++, makes extensive use of system BLAS and scales efficiently with dataset size. The genetic algorithm which finds optimal balance is parallelized and can make use of multiple CPUs or a cluster of computers. A large number of options are provided which control exactly how the matching is conducted and how balance is evaluated.},
author = {Sekhon, Jasjeet},
doi = {10.1.1.335.7044},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Sekhon - 2011 - Multivariate and Propensity Score Matching Software with Automated Balance Optimization The Matching Package for R.pdf:pdf},
journal = {Journal of Statistical Software},
keywords = {R.,causal inference,genetic optimization,multivariate matching,propensity score matching},
number = {7},
pages = {52},
title = {{Multivariate and Propensity Score Matching Software with Automated Balance Optimization: The Matching Package for R}},
volume = {42},
year = {2011}
}
@article{Rosenbaum1985,
abstract = {Matched sampling is a method for selecting units from a large reservoir of potential controls to produce a control group of modest size that is similar to a treated group with respect to the distribution of observed covariates. We illustrate the use of multivariate matching methods in an observational study of the effects of prenatal exposure to barbiturates on subsequent psychological development. A key idea is the use of the propensity score as a distinct matching variable.},
annote = {Aqui no match usaram uma fun{\c{c}}{\~{a}}o dos propensity scores para n{\~{a}}o tarem tao comprimidos entre 0 e 1.

Aqui pode-se exigir matching exato a certas variaveis categoricas, e depois faz-se o matching pelos propensity scores.

Mistura de Malanobis com PS},
author = {Rosenbaum, Paul R and Rubin, Donald B},
doi = {10.1080/00031305.1985.10479383},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Rosenbaum, Rubin - 1985 - Constructing a Control Group Using Multivariate Matched Sampling Methods that Incorporate the Propensity Score.pdf:pdf},
isbn = {00031305},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {Bias reduction,Mahalanobis metric matching,Nearest available matching,Observational studies,Propensity scores},
number = {1},
pages = {33--38},
pmid = {5025595},
title = {{Constructing a Control Group Using Multivariate Matched Sampling Methods that Incorporate the Propensity Score}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00031305.1985.10479383},
volume = {39},
year = {1985}
}
@article{Pan2015,
author = {Pan, Wei and Bai, Haiyan},
doi = {10.1186/s12874-015-0049-3},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Pan, Bai - 2015 - Propensity score interval matching using bootstrap confidence intervals for accommodating estimation errors of propens.pdf:pdf},
isbn = {1287401500493},
issn = {1471-2288},
journal = {BMC Medical Research Methodology},
keywords = {caliper matching,causal inference,confidence intervals,matching,nearest neighbour,observational studies,propensity score matching,propensity score methods,the bootstrap},
number = {1},
pages = {53},
title = {{Propensity score interval matching: using bootstrap confidence intervals for accommodating estimation errors of propensity scores}},
url = {http://www.biomedcentral.com/1471-2288/15/53},
volume = {15},
year = {2015}
}
@article{Diamond2012,
abstract = {Genetic matching is a new method for performing multivariate matching which uses an evolutionary search algorithm to determine the weight each covariate is given. The method utilizes an evolutionary algorithm developed by Mebane and Sekhon (1998; Sekhon and Mebane 1998) that maximizes the balance of observed potential confounders across matched treated and control units. The method is nonparametric and does not depend on knowing or estimating the propensity score, but the method is greatly improved when a known or estimated propensity score is incorporated. Genetic matching reliably reduces both the bias and the mean square error of the estimated causal effect even when the property of equal percent bias reduction (EPBR) does not hold. When this property does not hold, matching methods—such as Mahalanobis distance and propensity score matching—often perform poorly. Even if the EPBR property does hold and the propensity score is correctly specified, in finite samples, estimates based on genetic matching have lower mean square error than those based on the usual matching methods. We present a reanalysis of the LaLonde (1986) job training dataset which demonstrates the benefits of genetic matching and which helps to resolve a longstanding debate between Dehejia and Wahba (1997; 1999; 2002; Dehejia 2005) and Smith and Todd (2001, 2005a,b) over the ability of matching to overcome LaLonde's critique of nonexperimental estimators. Monte Carlos are also presented to demonstrate the properties of our method.},
annote = {Explains the differences and similarities between MD, PS, and genetic matching (generalization of both)

comparing machine learning methods including genMatch},
author = {Diamond, Alexis and Sekhon, Js},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Diamond, Sekhon - 2012 - Genetic Matching for Estimating Causal Effects.pdf:pdf},
journal = {The Review of Economics and Statistics},
number = {July},
pages = {932--945},
title = {{Genetic Matching for Estimating Causal Effects}},
url = {http://sekhon.polisci.berkeley.edu/papers/GenMatch.pdf},
volume = {95},
year = {2012}
}
@article{Derocher2004,
abstract = {Polar bears (Ursus maritimus) live throughout the ice-covered waters of the circumpolar Arctic, particularly in near shore annual ice over the continental shelf where biological productivity is highest. However, to a large degree under scenarios predicted by climate change models, these preferred sea ice habitats will be substantially altered. Spatial and temporal sea ice changes will lead to shifts in trophic interactions involving polar bears through reduced availability and abundance of their main prey: seals. In the short term, climatic warming may improve bear and seal habitats in higher latitudes over continental shelves if currently thick multiyear ice is replaced by annual ice with more leads, making it more suitable for seals. A cascade of impacts beginning with reduced sea ice will be manifested in reduced adipose stores leading to lowered reproductive rates because females will have less fat to invest in cubs during the winter fast. Non-pregnant bears may have to fast on land or offshore on the remaining multiyear ice through progressively longer periods of open water while they await freeze-up and a return to hunting seals. As sea ice thins, and becomes more fractured and labile, it is likely to move more in response to winds and currents so that polar bears will need to walk or swim more and thus use greater amounts of energy to maintain contact with the remaining preferred habitats. The effects of climate change are likely to show large geographic, temporal and even individual differences and be highly variable, making it difficult to develop adequate monitoring and research programs. All ursids show behavioural plasticity but given the rapid pace of ecological change in the Arctic, the long generation time, and the highly specialised nature of polar bears, it is unlikely that polar bears will survive as a species if the sea ice disappears completely as has been predicted by some.},
author = {Derocher, Andrew E and Lunn, Nicholas J and Stirling, Ian},
doi = {10.1093/icb/44.2.163},
issn = {1540-7063},
journal = {Integrative and comparative biology},
month = {apr},
number = {2},
pages = {163--76},
pmid = {21680496},
title = {{Polar bears in a warming climate.}},
url = {http://icb.oxfordjournals.org/content/44/2/163.short},
volume = {44},
year = {2004}
}
@article{Deb2015,
annote = {Notas importantes sobre matching, covariate adjustment, IPTW... Refere mesmo a teoria.

----------MATCHING---------

refere matiching 1:1, 2:1, 3:1

NNM - nearest-neightbor matching ou NNM-CD(caliper distance)

matching without/with replacing

greedy/optimal matching

--------STRATIFICATION------

--------Covariate adjustment-------

INVERSE PROBABILITY OF TREATMENT WEIGHTING




vantagens e desvantagens},
author = {Deb, Saswata and Austin, P. C. and Tu, Jack V and Ko, Dennis T and Mazer, C David and Kiss, Alex and Fremes, Stephen E},
doi = {10.1016/j.cjca.2015.05.015},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Deb et al. - 2015 - Methods in Cardiovascular Research A Review of Propensity-Score Methods and Their Use in Cardiovascular Research.pdf:pdf},
isbn = {0828-282X},
journal = {Canadian Journal of Cardiology},
title = {{Methods in Cardiovascular Research: A Review of Propensity-Score Methods and Their Use in Cardiovascular Research}},
url = {http://search.ebscohost.com/login.aspx?direct=true&db=edselp&AN=S0828282X15003979&site=eds-live},
year = {2015}
}
@article{Kurth2005,
author = {Kurth, T.},
doi = {10.1093/aje/kwj047},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Kurth - 2005 - Results of Multivariable Logistic Regression, Propensity Matching, Propensity Adjustment, and Propensity-based Weighting.pdf:pdf},
issn = {0002-9262},
journal = {American Journal of Epidemiology},
keywords = {iptw,matching,regression},
mendeley-tags = {iptw,matching,regression},
number = {3},
pages = {262--270},
title = {{Results of Multivariable Logistic Regression, Propensity Matching, Propensity Adjustment, and Propensity-based Weighting under Conditions of Nonuniform Effect}},
url = {http://aje.oxfordjournals.org/cgi/doi/10.1093/aje/kwj047},
volume = {163},
year = {2005}
}
@article{Dagostino1998,
abstract = {SUMMARY In observational studies, investigators have no control over the treatment assignment. The treated and non-treated (that is, control) groups may have large differences on their observed covariates, and these differences can lead to biased estimates of treatment effects. Even traditional covariance analysis adjust-ments may be inadequate to eliminate this bias. The propensity score, defined as the conditional probability of being treated given the covariates, can be used to balance the covariates in the two groups, and therefore reduce this bias. In order to estimate the propensity score, one must model the distribution of the treatment indicator variable given the observed covariates. Once estimated the propensity score can be used to reduce bias through matching, stratification (subclassification), regression adjustment, or some combination of all three. In this tutorial we discuss the uses of propensity score methods for bias reduction, give references to the literature and illustrate the uses through applied examples.},
author = {{D 'agostino}, Ralph B},
doi = {10.1002/(SICI)1097-0258(19981015)17:19<2265::AID-SIM918>3.0.CO;2-B},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/D 'agostino - 1998 - Tutorial in Biostatistics Propensity Score Methods for Bias Reduction in the Comparison of a Treatment To a Non-Ran.pdf:pdf},
isbn = {0277-6715 (Print)\r0277-6715 (Linking)},
issn = {02776715},
journal = {Statistics in Medicine},
pages = {2265--2281},
pmid = {9802183},
title = {{Tutorial in Biostatistics Propensity Score Methods for Bias Reduction in the Comparison of a Treatment To a Non-Randomized Control Group}},
volume = {17},
year = {1998}
}
@article{Brookhart2006,
author = {Brookhart, M. A.},
doi = {10.1093/aje/kwj149},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Brookhart - 2006 - Variable Selection for Propensity Score Models.pdf:pdf},
issn = {0002-9262},
journal = {American Journal of Epidemiology},
number = {12},
pages = {1149--1156},
title = {{Variable Selection for Propensity Score Models}},
url = {http://aje.oxfordjournals.org/cgi/doi/10.1093/aje/kwj149},
volume = {163},
year = {2006}
}
@article{Cepeda2003,
author = {Cepeda, M. S.},
doi = {10.1093/aje/kwg115},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Cepeda - 2003 - Comparison of Logistic Regression versus Propensity Score When the Number of Events Is Low and There Are Multiple Confou.pdf:pdf},
issn = {0002-9262},
journal = {American Journal of Epidemiology},
number = {3},
pages = {280--287},
title = {{Comparison of Logistic Regression versus Propensity Score When the Number of Events Is Low and There Are Multiple Confounders}},
url = {http://aje.oupjournals.org/cgi/doi/10.1093/aje/kwg115},
volume = {158},
year = {2003}
}
@article{Mansson2007,
abstract = {The use of propensity scores to adjust for measured confounding factors has become increasingly popular in cohort studies. However, their use in case-control and case-cohort studies has received little attention. The authors present some theory on the estimation and use of propensity scores in case-control and case-cohort studies and present the results of simulation studies that examine whether large-sample expectations are realized in studies of typical size. The application of propensity scores is less straightforward in case-control and case-cohort studies than in cohort studies. The authors' simulations revealed two potentially important issues. First, when using several potential approaches, there is artifactual effect modification of the odds ratio by level of propensity score. The magnitude of this phenomenon decreases as the sample size increases. Second, several potential approaches produce estimated propensity scores that do not converge to the true value as sample size increases and, thus, can fail to adjust fully for measured confounding factors. However, the magnitude of residual confounding appeared modest in our simulations. Researchers considering using propensity scores in case-control or case-cohort studies should consider the potential for artifactual effect modification and their reduced ability to control for potential confounding factors.},
author = {Mansson, Roger and Joffe, Marshall M. and Sun, Wenguang and Hennessy, Sean},
doi = {10.1093/aje/kwm069},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Mansson et al. - 2007 - On the estimation and use of propensity scores in case-control and case-cohort studies.pdf:pdf},
isbn = {0002-9262 (Print)},
issn = {00029262},
journal = {American Journal of Epidemiology},
keywords = {Bias (epidemiology),Case-control studies,Cohort studies,Confounding factors (epidemiology),Epidemiologic methods,Models,Propensity score,statistical},
number = {3},
pages = {332--339},
pmid = {17504780},
title = {{On the estimation and use of propensity scores in case-control and case-cohort studies}},
volume = {166},
year = {2007}
}
@article{Sturmer2005,
author = {Sturmer, T.},
doi = {10.1093/aje/kwi192},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Sturmer - 2005 - Adjusting Effect Estimates for Unmeasured Confounding with Validation Data using Propensity Score Calibration.pdf:pdf},
issn = {0002-9262},
journal = {American Journal of Epidemiology},
number = {3},
pages = {279--289},
title = {{Adjusting Effect Estimates for Unmeasured Confounding with Validation Data using Propensity Score Calibration}},
url = {http://aje.oxfordjournals.org/cgi/doi/10.1093/aje/kwi192},
volume = {162},
year = {2005}
}
@article{Joffe1999,
abstract = {The propensity score is the conditional probability of exposure to a treatment given observed covariates. In a cohort study, matching or stratifying treated and control subjects on a single variable, the propensity score, tends to balance all of the observed covariates; however, unlike random assignment of treatments, the propensity score may not also balance unobserved covariates. The authors review the uses and limitations of propensity scores and provide a brief outline of associated statistical theory. They also present a new result of using propensity scores in case-cohort studies.},
annote = {limitations},
author = {Joffe, M M and Rosenbaum, P R},
doi = {10.1002/ajmg.a.34338},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Joffe, Rosenbaum - 1999 - Invited commentary propensity scores.pdf:pdf},
isbn = {0002-9262 (Print)},
issn = {0002-9262},
journal = {American journal of epidemiology},
number = {4},
pages = {327--333},
pmid = {10453808},
title = {{Invited commentary: propensity scores.}},
volume = {150},
year = {1999}
}
@article{Austin2009,
abstract = {Although sample size calculations have become an important element in the design of research projects, such methods for studies involving current status data are scarce. Here, we propose a method for calculating power and sample size for studies using current status data. This method is based on a Weibull survival model for a two-group comparison. The Weibull model allows the investigator to specify a group difference in terms of a hazards ratio or a failure time ratio. We consider exponential, Weibull and uniformly distributed censoring distributions. We base our power calculations on a parametric approach with the Wald test because it is easy for medical investigators to conceptualize and specify the required input variables. As expected, studies with current status data have substantially less power than studies with the usual right-censored failure time data. Our simulation results demonstrate the merits of these proposed power calculations.},
author = {Austin, P. C.},
doi = {10.1002/sim},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Austin - 2009 - The performance of different propensity score methods for estimating marginal odds ratios.pdf:pdf},
isbn = {2007090091480},
issn = {02776715},
journal = {Statistics in medicine},
keywords = {missing covariates,missing data,multiple imputation,proportional hazards model},
number = {15},
pages = {1982--1998},
pmid = {19455509},
title = {{The performance of different propensity score methods for estimating marginal odds ratios}},
volume = {28},
year = {2009}
}
@article{Braitman2002,
author = {Braitman, Leonard E.},
doi = {10.7326/0003-4819-137-8-200210150-00015},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Braitman - 2002 - Rare Outcomes, Common Treatments Analytic Strategies Using Propensity Scores.pdf:pdf},
issn = {0003-4819},
journal = {Annals of Internal Medicine},
number = {8},
pages = {693},
title = {{Rare Outcomes, Common Treatments: Analytic Strategies Using Propensity Scores}},
url = {http://annals.org/article.aspx?doi=10.7326/0003-4819-137-8-200210150-00015},
volume = {137},
year = {2002}
}
@article{Austin2021,
abstract = {Propensity score–based analysis is increasingly being used in observational studies to estimate the effects of treatments, interventions, and exposures. We introduce the concept of the propensity score and how it can be used in observational research. We describe 4 different ways of using the propensity score: matching on the propensity score, inverse probability of treatment weighting using the propensity score, stratification on the propensity score, and covariate adjustment on the propensity score (with a focus on the first 2). We provide recommendations for the use and reporting of propensity score methods for the conduct of observational studies in neurologic research.

ATE=
:   average treatment effect; 
ATT=
:   average treatment effect in the treated; 
IPTW=
:   inverse probability of treatment weighting; 
NNM=
:   nearest neighbor matching; 
NNT=
:   number needed to treat; 
RCT=
:   randomized controlled trial; 
tPA=
:   tissue-type plasminogen activator},
author = {Austin, Peter C. and {Xin Yu}, Amy Ying and Vyas, Manav V. and Kapral, Moira K.},
doi = {10.1212/WNL.0000000000012777},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Austin et al. - 2021 - Applying Propensity Score Methods in Clinical Research in Neurology.pdf:pdf},
issn = {0028-3878},
journal = {Neurology},
month = {nov},
number = {18},
pages = {856--863},
pmid = {34504033},
publisher = {Wolters Kluwer Health, Inc. on behalf of the American Academy of Neurology},
title = {{Applying Propensity Score Methods in Clinical Research in Neurology}},
url = {https://n.neurology.org/content/97/18/856 https://n.neurology.org/content/97/18/856.abstract},
volume = {97},
year = {2021}
}
@article{Austin,
abstract = {The propensity score is a balancing score: conditional on the propensity score, treated and untreated subjects have the same distribution of observed baseline characteristics. Four methods of using the propensity score have been described in the literature: stratification on the propensity score, propensity score matching, inverse probability of treatment weighting using the propensity score, and covariate adjustment using the propensity score. However, the relative ability of these methods to reduce systematic differences between treated and untreated subjects has not been examined. The authors used an empirical case study and Monte Carlo simulations to examine the relative ability of the 4 methods to balance baseline covariates between treated and untreated subjects. They used standardized differences in the propensity score matched sample and in the weighted sample. For stratification on the propensity score, within-quintile standardized differences were computed comparing the distribution of baseline covariates between treated and untreated subjects within the same quintile of the propensity score. These quintile-specific standardized differences were then averaged across the quintiles. For covariate adjustment, the authors used the weighted conditional standardized absolute difference to compare balance between treated and untreated subjects. In both the empirical case study and in the Monte Carlo simulations, they found that matching on the propensity score and weighting using the inverse probability of treatment eliminated a greater degree of the systematic differences between treated and untreated subjects compared with the other 2 methods. In the Monte Carlo simulations, propensity score matching tended to have either comparable or marginally superior performance compared with propensity-score weighting.},
author = {Austin, P C},
doi = {10.1177/0272989X09341755},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Austin - 2009 - The relative ability of different propensity score methods to balance measured covariates between treated and untreated.pdf:pdf},
issn = {1552-681X},
journal = {Medical decision making : an international journal of the Society for Medical Decision Making},
keywords = {80 and over,Adrenergic beta-Antagonists,Adrenergic beta-Antagonists: therapeutic use,Aged,Angiotensin-Converting Enzyme Inhibitors,Angiotensin-Converting Enzyme Inhibitors: therapeu,Female,Heart Failure,Heart Failure: drug therapy,Humans,Male,Monte Carlo Method,Probability},
month = {jan},
number = {6},
pages = {661--77},
pmid = {19684288},
title = {{The relative ability of different propensity score methods to balance measured covariates between treated and untreated subjects in observational studies.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19684288},
volume = {29},
year = {2009}
}
@article{Muller2021,
abstract = {Background and Objectives Deep neural networks recently become a popular tool in medical research to predict disease progression and unveil its underlying temporal phenotypes. While being well suited to study longitudinal clinical data and learn disease progression models, its application in clinical setting is challenged by the importance of model explainability. In this work, we integrate model learning using recurrent neural networks with deep model explanation using SHAP to study disease progression in Amyotrophic Lateral Sclerosis (ALS). Methods We propose and evaluate different deep neural networks to predict disease progression in a large cohort of Portuguese ALS patients. We learn models of disease progression targeting the prediction and explanation of respiratory decline to respiratory failure, as measured by clinical administration of non-invasive ventilation (NIV). Afterward, we explain the learnt models using SHAP and inspect the outcome with clinical researchers, targeting the identification of highly influencing features in model prediction, and putative features to be discarded in an augmented model, due to their small influence. Results When used to predict breathing capability of ALS patients in different time windows, our recurrent neural networks with LSTMs (Long Short-Term Memory) showed mean squared errors below 0.01 and 0.02 in train and test, respectively. This enables their effective use to predict the need for non-invasive ventilation, and potentially other clinically relevant endpoints, while providing clinical insights regarding disease progression to respiratory insufficiency. It was exciting to find that this study supports previous results showing that neck weakness is related to disease outcome and respiratory decline in ALS. Conclusions Our study to learn and explain a predictive model for ALS shows the potentialities of using deep learning from longitudinal clinical data together with deep model explanation to achieve accurate prognostic prediction and model interpretability, while drawing insights into disease progression and promoting personalized medicine.},
author = {M{\"{u}}ller, Marcel and Gromicho, Marta and de Carvalho, Mamede and Madeira, Sara C.},
doi = {10.1016/j.cmpbup.2021.100018},
file = {:Users/jferreira-admin/Downloads/1-s2.0-S2666990021000173-main.pdf:pdf},
issn = {26669900},
journal = {Computer Methods and Programs in Biomedicine Update},
pages = {100018},
publisher = {Elsevier B.V.},
title = {{Explainable models of disease progression in ALS: Learning from longitudinal clinical data with recurrent neural networks and deep model explanation}},
url = {https://doi.org/10.1016/j.cmpbup.2021.100018},
volume = {1},
year = {2021}
}
@article{VanPuijenbroek2002,
abstract = {Purpose: A continuous systematic review of all combinations of drugs and suspected adverse reactions (ADRs) reported to a spontaneous reporting system, is necessary to optimize signal detection. To focus attention of human reviewers, quantitative procedures can be used to sift data in different ways. In various centres, different measures are used to quantify the extent to which an ADR is reported disproportionally to a certain drug compared to the generality of the database. The objective of this study is to examine the level of concordance of the various estimates to the measure used by the WHO Collaborating Centre for International ADR monitoring, the information component (IC), when applied to the dataset of the Netherlands Pharmacovigilance Foundation Lareb. Methods: The Reporting Odds Ratio - 1.96 standard errors (SE), proportional reporting ratio - 1.96 SE, Yule's Q - 1.96 SE, the Poisson probability and Chi-square test of all 17 330 combinations were compared with the IC minus 2 standard deviations. Additionally, the concordance of the various tests, in respect to the number of reports per combination, was examined. Results: In general, sensitivity was high in respect to the reference measure when a combination of point- and precision estimate was used. The concordance increased dramatically when the number of reports per combination increased. Conclusion: This study shows that the different measures used are broadly comparable when four or more cases per combination have been collected. Copyright {\textcopyright} 2002 John Wiley & Sons, Ltd.},
author = {{Van Puijenbroek}, Eug{\'{e}}ne P. and Bate, Andrew and Leufkens, Hubert G.M. and Lindquist, Marie and Orre, Roland and Egberts, Antoine C.G.},
doi = {10.1002/PDS.668},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Van Puijenbroek et al. - 2002 - A comparison of measures of disproportionality for signal detection in spontaneous reporting systems for.pdf:pdf},
issn = {1053-8569},
journal = {Pharmacoepidemiology and drug safety},
keywords = {Adverse Drug Reaction Reporting Systems / standard,Adverse Drug Reaction Reporting Systems / statisti,Algorithms,Andrew Bate,Antoine C G Egberts,Comparative Study,Eug{\`{e}}ne P van Puijenbroek,Humans,MEDLINE,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Odds Ratio,PubMed Abstract,Sensitivity and Specificity,doi:10.1002/pds.668,pmid:11998548},
number = {1},
pages = {3--10},
pmid = {11998548},
publisher = {Pharmacoepidemiol Drug Saf},
title = {{A comparison of measures of disproportionality for signal detection in spontaneous reporting systems for adverse drug reactions}},
url = {https://pubmed.ncbi.nlm.nih.gov/11998548/},
volume = {11},
year = {2002}
}
@article{Khaleel2022,
abstract = {One of the largest spontaneous adverse events reporting databases in the world is the Food and Drug Administration (FDA) Adverse Event Reporting System (FAERS). Unfortunately, researchers face many obstacles in analyzing data from the FAERS database. One of the major obstacles is the unstructured entry of drug names into the FAERS, as reporters might use generic names or trade names with different naming structures from all over the world and, in some cases, with typographical errors. Moreover, report duplication is a known problem in spontaneous adverse event-reporting systems, including the FAERS database. Hence, thorough text processing for database entries, especially drug name entries, coupled with a practical case-deduplication logic, is a prerequisite to analyze the database, which is a time-and resource-consuming procedure. In this study, we provide a clean, deduplicated, and ready-to-import dataset into any relational database management software of the FAERS database up to September 2021. Drug names are standardized to the RxNorm vocabulary and normalized to the single active ingredient level. Moreover, a pre-calculated disproportionate analysis is provided, which includes the reporting odds ratio (ROR), proportional reporting ratio (PRR), Chi-squared analysis with Yates correction (x2), and information component (IC) for each drug-adverse event pair in the database.},
author = {Khaleel, Mohammad Ali and Khan, Amer Hayat and Ghadzi, Siti Maisharah Sheikh and Adnan, Azreen Syazril and Abdallah, Qasem M.},
doi = {10.3390/HEALTHCARE10030420},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Khaleel et al. - 2022 - A Standardized Dataset of a Spontaneous Adverse Event Reporting System(2).pdf:pdf},
issn = {22279032},
journal = {Healthcare},
keywords = {Adverse drug reactions,Drug adverse event,FAERS,Information component,LAERS,PRR,ROR,Spontaneous adverse event reporting},
month = {mar},
number = {3},
pmid = {35326898},
publisher = {Multidisciplinary Digital Publishing Institute  (MDPI)},
title = {{A Standardized Dataset of a Spontaneous Adverse Event Reporting System}},
url = {/pmc/articles/PMC8954498/ /pmc/articles/PMC8954498/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8954498/},
volume = {10},
year = {2022}
}
@article{Polkey2017,
abstract = {Rationale: Biomarkers for survival in amyotrophic lateral sclerosis (ALS) would facilitate the development of novel drugs. Although respiratory muscle weakness is a known predictor of poor prognosis, a comprehensive comparison of different tests is lacking. Objectives: To compare the predictive power of invasive and noninvasive respiratory muscle strength assessments for survival or ventilator-free survival, up to 3 years. Methods: From a previously published report respiratory muscle strength measurements were available for 78 patients with ALS. Time to deathand/orventilationwereascertained.Receiveroperatingcharacteristic analysis was used to determine the cutoff point of each parameter. Measurements and Main Results: Each respiratory muscle strength assessment individually achieved statistical significance for prediction of survival or ventilator-free survival. In multivariate analysis sniff trans-diaphragmatic and esophageal pressure, twitch trans-diaphragmatic pressure (Tw Pdi), age, and maximal static expiratory mouth pressure were significant predictors of ventilationfree survival and Tw Pdi and maximal static expiratory mouth pressure for absolute survival. Although all measures had good specificity, there were differing sensitivities. All cutoff points for the VC were greater than 80% of normal, except for prediction of 3-month outcomes. Sequential data showed a linear decline for direct measures of respiratory muscle strength, whereas VC showed little to no decline until 12 months before death/ventilation. Conclusions: The most powerful biomarker for mortality stratification was Tw Pdi, but the predictive power of sniff nasal inspiratory pressure was also excellent. A VC within normal range suggested a good prognosis at 3 months but was of little other value.},
author = {Polkey, Michael I. and Lyall, Rebecca A. and Yang, Ke and Johnson, Erin and {Nigel Leigh}, P. and Moxham, John},
doi = {10.1164/rccm.201604-0848OC},
file = {:Users/jferreira-admin/PhD/articles/rccm.201604-0848OC.pdf:pdf},
issn = {15354970},
journal = {American Journal of Respiratory and Critical Care Medicine},
keywords = {Amyotrophic lateral sclerosis,Diaphragm,Maximal inspiratory pressure,Sniff nasal inspiratory pressure,Survival},
number = {1},
pages = {86--95},
pmid = {27494149},
title = {{Respiratory muscle strength as a predictive biomarker for survival in amyotrophic lateral sclerosis}},
volume = {195},
year = {2017}
}
@article{Muller2021a,
abstract = {Background and Objectives Deep neural networks recently become a popular tool in medical research to predict disease progression and unveil its underlying temporal phenotypes. While being well suited to study longitudinal clinical data and learn disease progression models, its application in clinical setting is challenged by the importance of model explainability. In this work, we integrate model learning using recurrent neural networks with deep model explanation using SHAP to study disease progression in Amyotrophic Lateral Sclerosis (ALS). Methods We propose and evaluate different deep neural networks to predict disease progression in a large cohort of Portuguese ALS patients. We learn models of disease progression targeting the prediction and explanation of respiratory decline to respiratory failure, as measured by clinical administration of non-invasive ventilation (NIV). Afterward, we explain the learnt models using SHAP and inspect the outcome with clinical researchers, targeting the identification of highly influencing features in model prediction, and putative features to be discarded in an augmented model, due to their small influence. Results When used to predict breathing capability of ALS patients in different time windows, our recurrent neural networks with LSTMs (Long Short-Term Memory) showed mean squared errors below 0.01 and 0.02 in train and test, respectively. This enables their effective use to predict the need for non-invasive ventilation, and potentially other clinically relevant endpoints, while providing clinical insights regarding disease progression to respiratory insufficiency. It was exciting to find that this study supports previous results showing that neck weakness is related to disease outcome and respiratory decline in ALS. Conclusions Our study to learn and explain a predictive model for ALS shows the potentialities of using deep learning from longitudinal clinical data together with deep model explanation to achieve accurate prognostic prediction and model interpretability, while drawing insights into disease progression and promoting personalized medicine.},
author = {M{\"{u}}ller, Marcel and Gromicho, Marta and de Carvalho, Mamede and Madeira, Sara C.},
doi = {10.1016/j.cmpbup.2021.100018},
file = {:Users/jferreira-admin/Downloads/1-s2.0-S2666990021000173-main.pdf:pdf},
issn = {26669900},
journal = {Computer Methods and Programs in Biomedicine Update},
keywords = {Explainable models,Longitudinal data,Disease progr},
pages = {100018},
publisher = {Elsevier B.V.},
title = {{Explainable models of disease progression in ALS: Learning from longitudinal clinical data with recurrent neural networks and deep model explanation}},
url = {https://doi.org/10.1016/j.cmpbup.2021.100018},
volume = {1},
year = {2021}
}
@article{Creemers2015,
abstract = {The progressive course of amyotrophic lateral sclerosis (ALS) results in an ever-changing spectrum of the care needs of patients with ALS. Knowledge of prognostic factors for the functional course of ALS may enhance clinical prediction and improve the timing of appropriate interventions. Our objective was to systematically review the evidence regarding prognostic factors for the rate of functional decline of patients with ALS, assessed with versions of the ALS Functional Rating Scale (ALSFRS). Two reviewers independently assessed the methodological quality of the thirteen included studies using the Quality in Prognosis Studies (QUIPS) tool. The overall quality of evidence for each prognostic factor was assessed using the Grading of Recommendations Assessment, Development and Evaluation (GRADE) approach, considering risk of bias, imprecision, inconsistency, indirectness, and publication bias. The quality of evidence for the prognostic value of age at onset, site of onset, time from symptom onset to diagnosis, and ALSFRS-Revised baseline score was low, mainly due to the limited data and inconsistency of results in the small number of studies included. The prognostic value of initial rate of disease progression, age at diagnosis, forced vital capacity, frontotemporal dementia, body mass index, and comorbidity remains unclear. We conclude that the current evidence on prognostic factors for functional decline in ALS is insufficient to allow the development of a prediction tool that can support clinical decisions. Given the limited data, future prognostic studies may need to focus on factors that have a predictive value for a decline in ALSFRS(-R) domain scores, preferably based on internationally collected and shared data.},
author = {Creemers, Huub and Grupstra, Hepke and Nollet, Frans and van den Berg, Leonard H. and Beelen, Anita},
doi = {10.1007/s00415-014-7564-8},
file = {:Users/jferreira-admin/PhD/articles/Creemers_PrognosticFactorsFunctionalCourseALS_SystematicReview2015.pdf:pdf},
issn = {14321459},
journal = {Journal of Neurology},
keywords = {Amyotrophic lateral sclerosis,Disease progression,Patient counseling,Prognosis},
number = {6},
pages = {1407--1423},
pmid = {25385051},
publisher = {Springer Berlin Heidelberg},
title = {{Prognostic factors for the course of functional status of patients with ALS: a systematic review}},
url = {http://dx.doi.org/10.1007/s00415-014-7564-8},
volume = {262},
year = {2015}
}
@article{Collier2021b,
abstract = {Background: The generalized propensity score (GPS) addresses selection bias due to observed confounding variables and provides a means to demonstrate causality of continuous treatment doses with propensity score analyses. Estimating the GPS with parametric models obliges researchers to meet improbable conditions such as correct model specification, normal distribution of variables, and large sample sizes. Objectives: The purpose of this Monte Carlo simulation study is to examine the performance of neural networks as compared to full factorial regression models to estimate GPS in the presence of Gaussian and skewed treatment doses and small to moderate sample sizes. Research design: A detailed conceptual introduction of neural networks is provided, as well as an illustration of selection of hyperparameters to estimate GPS. An example from public health and nutrition literature uses residential distance as a treatment variable to illustrate how neural networks can be used in a propensity score analysis to estimate a dose–response function of grocery spending behaviors. Results: We found substantially higher correlations and lower mean squared error values after comparing true GPS with the scores estimated by neural networks. The implication is that more selection bias was removed using GPS estimated with neural networks than using GPS estimated with classical regression. Conclusions: This study proposes a new methodological procedure, neural networks, to estimate GPS. Neural networks are not sensitive to the assumptions of linear regression and other parametric models and have been shown to be a contender against parametric approaches to estimate propensity scores for continuous treatments.},
author = {Collier, Zachary K. and Leite, Walter L. and Karpyn, Allison},
doi = {10.1177/0193841X21992199},
file = {:Users/jferreira-admin/Downloads/nihms-1824103.pdf:pdf},
issn = {15523926},
journal = {Evaluation Review},
keywords = {data mining,propensity scores,selection bias},
number = {1-2},
pages = {3--33},
title = {{Neural Networks to Estimate Generalized Propensity Scores for Continuous Treatment Doses}},
volume = {45},
year = {2021}
}
@article{Mandrekar2010,
abstract = {The performance of a diagnostic test in the case of a binary predictor can be evaluated using the measures of sensitivity and specificity. However, in many instances, we encounter predictors that are measured on a continuous or ordinal scale. In such cases, it is desirable to assess performance of a diagnostic test over the range of possible cutpoints for the predictor variable. This is achieved by a receiver operating characteristic (ROC) curve that includes all the possible decision thresholds from a diagnostic test result. In this brief report, we discuss the salient features of the ROC curve, as well as discuss and interpret the area under the ROC curve, and its utility in comparing two different tests or predictor variables of interest. {\textcopyright} 2010 by the International Association for the Study of Lung Cancer.},
author = {Mandrekar, Jayawant N.},
doi = {10.1097/JTO.0B013E3181EC173D},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Mandrekar - 2010 - Receiver Operating Characteristic Curve in Diagnostic Test Assessment.pdf:pdf},
issn = {1556-0864},
journal = {Journal of Thoracic Oncology},
keywords = {AUC,ROC,Sensitivity,Specificity},
month = {sep},
number = {9},
pages = {1315--1316},
pmid = {20736804},
publisher = {Elsevier},
title = {{Receiver Operating Characteristic Curve in Diagnostic Test Assessment}},
volume = {5},
year = {2010}
}
@article{Lundberg2017,
abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
archivePrefix = {arXiv},
arxivId = {1705.07874},
author = {Lundberg, Scott M. and Lee, Su In},
eprint = {1705.07874},
file = {:Users/jferreira-admin/Downloads/SHAP_camera_ready2.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {December},
pages = {4766--4775},
title = {{A unified approach to interpreting model predictions}},
volume = {2017-Decem},
year = {2017}
}
@article{Fisher,
abstract = {Most approaches to machine learning from electronic health data can only predict a single endpoint. the ability to simultaneously simulate dozens of patient characteristics is a crucial step towards personalized medicine for Alzheimer's Disease. Here, we use an unsupervised machine learning model called a conditional Restricted Boltzmann Machine (cRBM) to simulate detailed patient trajectories. We use data comprising 18-month trajectories of 44 clinical variables from 1909 patients with Mild cognitive impairment or Alzheimer's Disease to train a model for personalized forecasting of disease progression. We simulate synthetic patient data including the evolution of each sub-component of cognitive exams, laboratory tests, and their associations with baseline clinical characteristics. Synthetic patient data generated by the CRBM accurately reflect the means, standard deviations, and correlations of each variable over time to the extent that synthetic data cannot be distinguished from actual data by a logistic regression. Moreover, our unsupervised model predicts changes in total ADAS-Cog scores with the same accuracy as specifically trained supervised models, additionally capturing the correlation structure in the components of ADAS-Cog, and identifies sub-components associated with word recall as predictive of progression. Two patients with the same disease may present with different symptoms, progress at different rates, and respond differently to the same therapy. Understanding how to predict and manage differences between patients is the primary goal of precision medicine 1. Computational models of disease progression developed using machine learning approaches provide an attractive tool to combat such patient heterogeneity. One day these computational models may be used to guide clinical decisions; however, current applications are limited both by the availability of data and by the ability of algorithms to extract insights from those data. Most applications of machine learning to electronic health data have used techniques from supervised learning to predict specific endpoints 2-7. An alternative to developing separate supervised models to predict each characteristic is to build a single model that simultaneously predicts the evolution of many characteristics. Statistical models based on artificial neural networks provide one avenue for developing tools that can simulate patient progression in detail 8-10. Clinical data present a number of challenges that are not easily overcome with current approaches to machine learning 11. For example, most clinical datasets contain multiple types of data (i.e., they are "multimodal"), have a relatively small number of samples, and many missing observations. Dealing with these issues typically requires extensive preprocessing 3 or simply discarding variables that are too difficult to model. For example, one recent study focused on only four variables that were frequently measured across all 200,000 patients in an electronic health dataset from an intensive care unit 9. Developing methods that can overcome these limitations is a key step towards broader applications of machine learning in precision medicine. Precision medicine is especially important for complex disorders in which patients exhibit different patterns of disease progression and therapeutic responses. Alzheimer's Disease (AD) and Mild Cognitive Impairment (MCI) are complex neurodegenerative diseases with multiple cognitive and behavioral symptoms 12. The severity of these symptoms is usually assessed through exams such as the Alzheimer's Disease Assessment Scale (ADAS) 13 or Mini Mental State Exam (MMSE) 14. The heterogeneity of AD and related dementias makes these diseases difficult to},
author = {Fisher, Charles K and Smith, Aaron M and Walsh, Jonathan R},
doi = {10.1038/s41598-019-49656-2},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Fisher, Smith, Walsh - Unknown - Machine learning for comprehensive forecasting of Alzheimer's Disease progression & coalition Against M.pdf:pdf},
title = {{Machine learning for comprehensive forecasting of Alzheimer's Disease progression & coalition Against Major Diseases*}},
url = {https://doi.org/10.1038/s41598-019-49656-2}
}
@article{Grollemund2019,
abstract = {Background: Amyotrophic Lateral Sclerosis (ALS) is a relentlessly progressive neurodegenerative condition with limited therapeutic options at present. Survival from symptom onset ranges from 3 to 5 years depending on genetic, demographic, and phenotypic factors. Despite tireless research efforts, the core etiology of the disease remains elusive and drug development efforts are confounded by the lack of accurate monitoring markers. Disease heterogeneity, late-stage recruitment into pharmaceutical trials, and inclusion of phenotypically admixed patient cohorts are some of the key barriers to successful clinical trials. Machine Learning (ML) models and large international data sets offer unprecedented opportunities to appraise candidate diagnostic, monitoring, and prognostic markers. Accurate patient stratification into well-defined prognostic categories is another aspiration of emerging classification and staging systems. Methods: The objective of this paper is the comprehensive, systematic, and critical review of ML initiatives in ALS to date and their potential in research, clinical, and pharmacological applications. The focus of this review is to provide a dual, clinical-mathematical perspective on recent advances and future directions of the field. Another objective of the paper is the frank discussion of the pitfalls and drawbacks of specific models, highlighting the shortcomings of existing studies and to provide methodological recommendations for future study designs. Results: Despite considerable sample size limitations, ML techniques have already been successfully applied to ALS data sets and a number of promising diagnosis models have been proposed. Prognostic models have been tested using core clinical variables, biological, and neuroimaging data. These models also offer patient stratification opportunities for future clinical trials. Despite the enormous potential of ML in ALS research, statistical assumptions are often violated, the choice of specific statistical models is seldom justified, and the constraints of ML models are rarely enunciated. Conclusions: From a mathematical perspective, the main barrier to the development of validated diagnostic, prognostic, and monitoring indicators stem from limited sample sizes. The combination of multiple clinical, biofluid, and imaging biomarkers is likely to increase the accuracy of mathematical modeling and contribute to optimized clinical trial designs.},
annote = {Depending on geographical locations, the terms “ALS”
and “Motor Neuron Disease” (MND) are sometimes used interchangeably, but MND is the broader label, encompassing a spectrum of conditions, as illustrated by Figure 1

Most ML models, including the six approaches presented above, do not require stringent assumptions on data characteristics.

Calculating the Sample to Feature Ratio (SFR), i.e., the number of samples available per feature, is a simple way to assess whether the sample size is satisfactory for a given model. An
“SFR” of around 10–15 is often considered the bare minimum.

The four most common types ofbias include: study participation bias, study attrition bias , prognostic factor measurement bias, and outcomemeasurement bias. In ALS, study participation bias, - a.k.a. “clinical trial bias,” is by far the most significant. I

From a mathematical perspective, the main barrier to the development of validated diagnostic, prognostic, and monitoring indicators stem from limited sample sizes. The combination of multiple clinical, biofluid, and imaging biomarkers is likely to increase the accuracy of mathematical modeling and contribute to optimized clinical trial designs.},
author = {Grollemund, Vincent and Pradat, Pierre Fran{\c{c}}ois and Querin, Giorgia and Delbot, Fran{\c{c}}ois and {Le Chat}, Ga{\'{e}}tan and Pradat-Peyre, Jean Fran{\c{c}}ois and Bede, Peter},
doi = {10.3389/fnins.2019.00135},
file = {:Users/jferreira-admin/PhD/articles/Machine Learning in Amyotrophic Lateral Sclerosis Achievements, Pitfalls, and Future Directions.pdf:pdf},
issn = {1662453X},
journal = {Frontiers in Neuroscience},
keywords = {Amyotrophic lateral sclerosis,Clustering,Diagnosis,Machine learning,Motor neuron disease,Prognosis,Risk stratification},
number = {February},
pages = {1--28},
title = {{Machine learning in amyotrophic lateral sclerosis: Achievements, pitfalls, and future directions}},
volume = {13},
year = {2019}
}
@article{GolrizKhatami2020,
abstract = {PURPOSE OF REVIEW: With the advancement of computational approaches and abundance of biomedical data, a broad range of neurodegenerative disease models have been developed. In this review, we argue that computational models can be both relevant and useful in neurodegenerative disease research and although the current established models have limitations in clinical practice, artificial intelligence has the potential to overcome deficiencies encountered by these models, which in turn can improve our understanding of disease. RECENT FINDINGS: In recent years, diverse computational approaches have been used to shed light on different aspects of neurodegenerative disease models. For example, linear and nonlinear mixed models, self-modeling regression, differential equation models, and event-based models have been applied to provide a better understanding of disease progression patterns and biomarker trajectories. Additionally, the Cox-regression technique, Bayesian network models, and deep-learning-based approaches have been used to predict the probability of future incidence of disease, whereas nonnegative matrix factorization, nonhierarchical cluster analysis, hierarchical agglomerative clustering, and deep-learning-based approaches have been employed to stratify patients based on their disease subtypes. Furthermore, the interpretation of neurodegenerative disease data is possible through knowledge-based models which use prior knowledge to complement data-driven analyses. These knowledge-based models can include pathway-centric approaches to establish pathways perturbed in a given condition, as well as disease-specific knowledge maps, which elucidate the mechanisms involved in a given disease. Collectively, these established models have revealed high granular details and insights into neurodegenerative disease models. SUMMARY: In conjunction with increasingly advanced computational approaches, a wide spectrum of neurodegenerative disease models, which can be broadly categorized into data-driven and knowledge-driven, have been developed. We review the state of the art data and knowledge-driven models and discuss the necessary steps which are vital to bring them into clinical application.},
author = {{Golriz Khatami}, Sepehr and Mubeen, Sarah and Hofmann-Apitius, Martin},
doi = {10.1097/WCO.0000000000000795},
file = {:Users/jferreira-admin/Downloads/Data_science_in_neurodegenerative_disease__its.15.pdf:pdf},
isbn = {0000000000000},
issn = {14736551},
journal = {Current opinion in neurology},
keywords = {artificial intelligence,data-driven models,knowledge-driven models,neurodegenerative disease,virtual},
number = {2},
pages = {249--254},
pmid = {32073441},
title = {{Data science in neurodegenerative disease: its capabilities, limitations, and perspectives}},
volume = {33},
year = {2020}
}
@article{Pereira2020,
abstract = {Despite being able to make accurate predictions, most existing prognostic models lack a proper indication about the uncertainty of each prediction, that is, the risk of prediction error for individual patients. This hampers their translation to primary care settings through decision support systems. To address this problem, we studied different methods for transforming classifiers into probabilistic/confidence-based predictors (here called uncertainty methods), where predictions are complemented with probability estimates/confidence regions reflecting their uncertainty (uncertainty estimates). We tested several uncertainty methods: two well-known calibration methods (Platt Scaling and Isotonic Regression), Conformal Predictors, and Venn-ABERS predictors. We evaluated whether these methods produce valid predictions, where uncertainty estimates reflect the ground truth probabilities. Furthermore, we assessed the proportion of valid predictions made at high-certainty thresholds (predictions with uncertainty measures above a given threshold) since this impacts their usefulness in clinical decisions. Finally, we proposed an ensemble-based approach where predictions from multiple pairs of (classifier, uncertainty method) are combined to predict whether a given MCI patient will convert to AD. This ensemble should putatively provide predictions for a larger number of patients while releasing users from deciding which pair of (classifier, uncertainty method) is more appropriate for data under study. The analysis was performed with a Portuguese cohort (CCC) of around 400 patients and validated in the publicly available ADNI cohort. Despite our focus on MCI to AD prognosis, the proposed approach can be applied to other diseases and prognostic problems.},
author = {Pereira, Telma and Cardoso, Sandra and Guerreiro, Manuela and Mendon{\c{c}}a, Alexandre and Madeira, Sara C.},
doi = {10.1016/j.jbi.2019.103350},
file = {:Users/jferreira-admin/PhD/articles/1-s2.0-S1532046419302692-main.pdf:pdf},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
keywords = {Alzheimer's disease,Conformal prediction,Mild cognitive impairment,Prognostic prediction,Uncertainty at patient-level,Venn-ABERS},
number = {April 2019},
pages = {103350},
pmid = {31816401},
publisher = {Elsevier},
title = {{Targeting the uncertainty of predictions at patient-level using an ensemble of classifiers coupled with calibration methods, Venn-ABERS, and Conformal Predictors: A case study in AD}},
url = {https://doi.org/10.1016/j.jbi.2019.103350},
volume = {101},
year = {2020}
}
@article{Mitsumoto2003a,
abstract = {Objective: To compare characteristics of ALS patients with and without percutaneous endoscopic gastrostomy (PEG). Methods: Using the ALS Patient Care Database, data from patients with and without PEG with ALS Functional Rating Scale-bulbar subscale (ALSFRSb) scores ≤5 were analyzed; follow-up data were also collected. Results: PEG use was markedly increased with declining ALSFRSb scores. Demographics did not differ, but ALSFRS composite scores and bulbar and arm subscale scores were lower (P<0.0001). PEG patients used significantly more assistive devices, multidisciplinary care, home care nurses and aides, had more frequent physician and emergency department visits and hospital admissions (P<0.0001), and had lower health status based on the mini-SIP scale (P=0.0047). PEG use varied greatly between ALS centers. In the follow-up study, positive impact of PEG was noted in 79% of PEG patients but in only 37.5% of patients who received PEG later, based on a small number of patients. PEG use showed no survival benefit. Conclusion: Patients did not receive PEG until bulbar function was severely reduced and overall ALS had markedly progressed. PEG may have been performed too late to demonstrate survival benefits. Aggressive proactive nutritional management appears essential in patients with ALS. To determine whether PEG provides benefits, it must be performed at earlier stages of the disease and prospectively studied.},
author = {Mitsumoto, Hiroshi and Davidson, M. and Moore, D. and Gad, N. and Brandis, M. and Ringel, S. and Rosenfeld, J. and Shefner, J. M. and Strong, M. J. and Sufit, R. and Anderson, F. A. and Echols, Carol and Heiman-Patterson, Terry and Paylor, Theresa and Huffnagles, Vera and Murphy, Julie and Lou, Jau Shin and Tullar, David and McClusky, Leo and Damiano, Pat and Miller, Robert and Tkachenko, Igor and Neville, Hans and Blackwell, Kristin Howell and Oh, Shin and Olney, Richard and Mass, Jason and Pascuzzi, Robert and Michaels, Angi and Pioro, Erik and Andrews-Hinders, Doreen and Rosenfeld, Jeffrey and King, Ruth and Scelsa, Stephen and Shefner, Jeremy and Lepsky, Tina and Strong, Michael and Rowe, Ann and Sufit, Robert L. and Casey, Patricia},
doi = {10.1080/14660820310011728},
file = {:Users/jferreira-admin/Downloads/14660820310011728.pdf:pdf},
issn = {14660822},
journal = {Amyotrophic Lateral Sclerosis and Other Motor Neuron Disorders},
keywords = {Amyotrophic lateral sclerosis,Enteral feeding,Motor neuron disease,Nutritional care,Percutaneous endoscopic gastrostomy},
number = {3},
pages = {177--185},
pmid = {13129795},
title = {{Percutaneous endoscopic gastrostomy (PEG) in patients with ALS and bulbar dysfunction}},
volume = {4},
year = {2003}
}
@article{Russ2015,
abstract = {Background: Percutaneous endoscopic gastrostomy (PEG) tube placement is recommended in amyotrophic lateral sclerosis (ALS) patients with dysphagia to provide reliable access for medications and nutrition. It is preferably done while forced vital capacity (FVC) is greater than 50% of predicted to reduce risk of postprocedural respiratory complications. Percutaneous radiologic gastrostomy has been shown to have higher success rates and lower complication rates compared with PEG. The authors sought to investigate the safety of PEG placement in ALS patients with varying respiratory compromise. Methods: The records of 21 patients with ALS who underwent PEG tube placement from October 2010 to October 2013 were retrospectively reviewed to determine rates of successful placement, complication, mortality and survival. Results: PEG was placed successfully in 95.2% of patients. There was 1 major complication, procedure-related death, and failed placement in a patient with FVC 15% predicted who developed procedure-related aspiration and hypoxic respiratory failure and death 8 days after failed PEG attempt. Minor complications included 3 patients with minor pain at insertion site, 1 patient with minor bleeding requiring cauterization and 2 patients requiring PEG tube replacement within 6 months of procedure for accidental dislodgement. After PEG placement, median survival was 327 days (95% confidence interval: 180-687). Conclusions: In conclusion, PEG tube placement in patients with ALS seems to be a safe procedure in patients without significant respiratory compromise. In patients with FVC <50%, based on literature review and the results of this study, the authors recommend percutaneous radiologic gastrostomy over PEG placement.},
author = {Russ, Kirk B. and Phillips, Mark C. and {Mel Wilcox}, C. and Peter, Shajan},
doi = {10.1097/MAJ.0000000000000517},
file = {:Users/jferreira-admin/Downloads/MAJ.0000000000000517.pdf:pdf},
issn = {15382990},
journal = {American Journal of the Medical Sciences},
keywords = {Amyotrophic lateral sclerosis,Endoscopy,Gastrostomy,Nutritional support},
number = {2},
pages = {95--97},
pmid = {26135224},
publisher = {Elsevier Masson SAS},
title = {{Percutaneous Endoscopic Gastrostomy in Amyotrophic Lateral Sclerosis}},
url = {http://dx.doi.org/10.1097/MAJ.0000000000000517},
volume = {350},
year = {2015}
}
@article{Young2014,
abstract = {Summary • Data-driven models provide a uniquely fine-grained multi-modal picture of disease progression. • This offers major potential benefits to neurodegenerative disease research and clinical practice, by improving patient staging and monitoring, disease prognosis and differential diagnosis. • To date these models have provided valuable insights into neurodegenerative disease progression patterns, particularly in Alzheimer's disease. • Data-driven models are an emerging area of technology with numerous exciting opportunities for future developments. • These techniques have wide potential further application to any disease or developmental process.},
author = {Young, Alexandra L and Oxtoby, Neil P and Schott, Jonathan M and Alexander, Daniel C},
file = {:Users/jferreira-admin/Downloads/06-Young-review1.pdf:pdf},
journal = {Advances in Clinical Neuroscience and Rehabilitation},
number = {5},
pages = {6--9},
title = {{Data-driven models of neurodegenerative disease}},
volume = {14},
year = {2014}
}
@article{Jordan2015,
abstract = {Background: The Agency for Toxic Substances and Disease Registry established surveillance projects to determine the incidence, prevalence, and demographic characteristics of persons with Amyotrophic Lateral Sclerosis (ALS) in defined geographic areas. There is a need to characterize and account for the survival and prognostic factors among a population-based cohort of ALS cases in the United States. Methods: A cohort of incident cases diagnosed from 2009-2011 in New Jersey was followed until death or December 31, 2013, whichever happened first. Survival was assessed using Kaplan-Meier curves and Cox proportional hazards regression was used to identify prognostic factors. Results: Sixty-four percent of incident cases died between 2009 and 2013, 93.7% specifically from ALS. Among the 456 cases studied in the survival analysis, the median survival from diagnosis was 21 months; 46% of cases survived longer than two years from diagnosis. Older age predicted shorter survival. While there is some indication of differences because of sex, race, and ethnicity, these differences were not statistically significant when accounting for age. Conclusions: New Jersey mortality data were queried to determine the vital status of a cohort of incident ALS cases and used to investigate relationships between demographic factors and survival. Results are consistent with other population-based studies. Older age was a strong predictor of shorter survival time. Additional follow-up time is needed to characterize longer-term survival.},
annote = {Older age was a strong predictor of shorter survival time.},
author = {Jordan, Heather and Fagliano, Jerald and Rechtman, Lindsay and Lefkowitz, Daniel and Kaye, Wendy},
doi = {10.1159/000380855},
file = {:Users/jferreira-admin/Downloads/380855.pdf:pdf},
issn = {14230208},
journal = {Neuroepidemiology},
keywords = {Amyotrophic lateral sclerosis,Death certificates,Hazard ratio,Mortality,Survival curves},
number = {2},
pages = {114--120},
pmid = {25792423},
title = {{Effects of demographic factors on survival time after a diagnosis of amyotrophic lateral sclerosis}},
volume = {44},
year = {2015}
}
@article{Lefebvre2021,
abstract = {Background: In survival analysis, data can be modeled using either a multiplicative hazards regression model (such as the Cox model) or an additive hazards regression model (such as Lin's or Aalen's model). While several diagnostic tools are available to check the assumptions underpinning each type of model, there is no defined procedure to fit these models optimally. Moreover, the two types of models are rarely combined in survival analysis. Here, we propose a strategy for optimal fitting of multiplicative and additive hazards regression models in survival analysis. Methods: This section details our proposed strategy for optimal fitting of multiplicative and additive hazards regression models, with a focus on the assumptions underpinning each type of model, the diagnostic tools used to check these assumptions, and the steps followed to fit the data. The proposed strategy draws on classical diagnostic tools (Schoenfeld and martingale residuals) and less common tools (pseudo-observations, martingale residual processes, and Arjas plots). Results: The proposed strategy is applied to a dataset of patients with myocardial infarction (TRACE data frame). The effects of 5 covariates (age, sex, diabetes, ventricular fibrillation, and clinical heart failure) on the hazard of death are analyzed using multiplicative and additive hazards regression models. The proposed strategy is shown to fit the data optimally. Conclusions: Survival analysis is improved by using multiplicative and additive hazards regression models together, but specific steps must be followed to fit the data optimally. By providing different measures of the same effect, our proposed strategy allows for better interpretation of the data.},
author = {Lefebvre, Fran{\c{c}}ois and Giorgi, Roch},
doi = {10.1186/s12874-021-01273-2},
file = {:Users/jferreira-admin/Downloads/s12874-021-01273-2.pdf:pdf},
issn = {14712288},
journal = {BMC Medical Research Methodology},
keywords = {Additive hazards regression model,Multiplicative hazards regression model,Survival analysis},
number = {1},
pages = {1--17},
pmid = {33957858},
publisher = {BMC Medical Research Methodology},
title = {{A strategy for optimal fitting of multiplicative and additive hazards regression models}},
volume = {21},
year = {2021}
}
@article{Madadizadeh2017,
abstract = {Introduction: Colorectal cancer (CRC) is a commonly fatal cancer that ranks as third worldwide and third and the fifth in Iranian women and men, respectively. There are several methods for analyzing time to event data. Additive hazards regression models take priority over the popular Cox proportional hazards model if the absolute hazard (risk) change instead of hazard ratio is of primary concern, or a proportionality assumption is not made. Methods: This study used data gathered from medical records of 561 colorectal cancer patients who were admitted to Namazi Hospital, Shiraz, Iran, during 2005 to 2010 and followed until December 2015. The nonparametric Aalen's additive hazards model, semiparametric Lin and Ying's additive hazards model and Cox proportional hazards model were applied for data analysis. The proportionality assumption for the Cox model was evaluated with a test based on the Schoenfeld residuals and for test goodness of fit in additive models, Cox-Snell residual plots were used. Analyses were performed with SAS 9.2 and R3.2 software. Results: The median follow-up time was 49 months. The five-year survival rate and the mean survival time after cancer diagnosis were 59.6% and 68.1±1.4 months, respectively. Multivariate analyses using Lin and Ying's additive model and the Cox proportional model indicated that the age of diagnosis, site of tumor, stage, and proportion of positive lymph nodes, lymphovascular invasion and type of treatment were factors affecting survival of the CRC patients. Conclusion: Additive models are suitable alternatives to the Cox proportionality model if there is interest in evaluation of absolute hazard change, or no proportionality assumption is made.},
author = {Madadizadeh, Farzan and Ghanbarnejad, Amin and Ghavami, Vahid and Bandamiri, Mohammad Zare and Mohammadianpanah, Mohammad},
doi = {10.22034/APJCP.2017.18.4.1077},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Madadizadeh et al. - 2017 - Applying additive hazards models for analyzing survival in patients with colorectal cancer in Fars Province,.pdf:pdf},
issn = {2476762X},
journal = {Asian Pacific Journal of Cancer Prevention},
keywords = {Additive models,Colorectal neoplasms,Proportional hazards models,Survival analysis},
month = {apr},
number = {4},
pages = {1077--1083},
pmid = {28547944},
publisher = {Asian Pacific Organization for Cancer Prevention},
title = {{Applying additive hazards models for analyzing survival in patients with colorectal cancer in Fars Province, Southern Iran}},
volume = {18},
year = {2017}
}
@article{Kjældgaard2021,
abstract = {Introduction: Amyotrophic lateral sclerosis (ALS) is a progressive motor neuron disease with great heterogeneity. Biological prognostic markers are needed for the patients to plan future supportive treatment, palliative treatment, and end-of-life decisions. In addition, prognostic markers are greatly needed for the randomization in clinical trials. Objective: This study aimed to test the ALS Functional Rating Scale-Revised (ALSFRS-R) progression rate ($\Delta$FS) as a prognostic marker of survival in a Danish ALS cohort. Methods: The ALSFRS-R score at test date in association with duration of symptoms, from the onset of symptoms until test date, (defined as $\Delta$FS') was calculated for 90 Danish patients diagnosed with either probable or definite sporadic ALS. Median survival time was then estimated from the onset of symptoms until primary endpoint (either death or tracheostomy). $\Delta$FS' was subjected to survival analysis using Cox proportional hazards modelling, log-rank test, and Kaplan-Meier survival analysis. Results and conclusions: Both $\Delta$FS' and age was found to be strong predictors of survival of the Danish ALS cohort. Both variables are easily obtained at the time of diagnosis and could be used by clinicians and ALS patients to plan future supportive and palliative treatment. Furthermore, $\Delta$FS', is a simple, prognostic marker that predicts survival in the early phase of disease as well as at later stages of the disease.},
author = {Kj{\ae}ldgaard, Anne Lene and Pilely, Katrine and Olsen, Karsten Skovgaard and Jessen, Anders Hedegaard and Lauritsen, Anne {\O}berg and Pedersen, Stephen W{\o}rlich and Svenstrup, Kirsten and Karlsborg, Merete and Thagesen, Helle and Blaabjerg, Morten and The{\'{o}}d{\'{o}}rsd{\'{o}}ttir, {\'{A}}sta and Elmo, Elisabeth Gundtoft and M{\o}ller, Anette Torvin and Bonefeld, Lone and Berg, Mia and Garred, Peter and M{\o}ller, Kirsten},
doi = {10.1186/S12883-021-02187-8/TABLES/4},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Kj{\ae}ldgaard et al. - 2021 - Prediction of survival in amyotrophic lateral sclerosis a nationwide, Danish cohort study.pdf:pdf},
issn = {14712377},
journal = {BMC Neurology},
keywords = {ALSFRS-R slope,Amyotrophic lateral sclerosis,Median survival time,Prognostic biomarker},
month = {dec},
number = {1},
pages = {1--8},
pmid = {33865343},
publisher = {BioMed Central Ltd},
title = {{Prediction of survival in amyotrophic lateral sclerosis: a nationwide, Danish cohort study}},
url = {https://bmcneurol.biomedcentral.com/articles/10.1186/s12883-021-02187-8},
volume = {21},
year = {2021}
}
@article{Mehta2019,
abstract = {Amyotrophic lateral sclerosis (ALS), commonly known as Lou Gehrig's disease, is a rapidly progressive fatal neurologic disease. Currently, there is no cure for ALS and the available treatments only extend life by an average of a few months. The majority of ALS patients die within 2-5 years of diagnosis, though survival time varies depending on disease progression (1,2). For approximately 10% of patients, ALS is familial, meaning it and has a genetic component; the remaining 90% have sporadic ALS, where etiology is unknown, but might be linked to environmental factors such as chemical exposures (e.g., heavy metals, pesticides) and occupational history (3).},
author = {Mehta, Paul and Horton, D. Kevin and Kasarskis, Edward J. and Tessaro, Ed and Eisenberg, M. Shira and Laird, Susan and Iskander, John},
doi = {10.15585/MMWR.MM6650A3},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Mehta et al. - 2019 - CDC Grand Rounds National Amyotrophic Lateral Sclerosis (ALS) Registry Impact, Challenges, and Future Directions.pdf:pdf},
issn = {0149-21951545-861X},
journal = {MMWR. Morbidity and Mortality Weekly Report},
keywords = {ALS,Amyotrophic Lateral Sclerosis,Lou Gehrig's Disease,Morbidity & Mortality Weekly Report,National ALS Registry},
month = {dec},
number = {50},
pages = {1379--1382},
pmid = {29267263},
publisher = {Centers for Disease Control MMWR Office},
title = {{CDC Grand Rounds: National Amyotrophic Lateral Sclerosis (ALS) Registry Impact, Challenges, and Future Directions}},
url = {https://www.cdc.gov/mmwr/volumes/66/wr/mm6650a3.htm},
volume = {66},
year = {2019}
}
@article{Knibb2016,
abstract = {Background: Amyotrophic lateral sclerosis (ALS) is a progressive and usually fatal neurodegenerative disease. Survival from diagnosis varies considerably. Several prognostic factors are known, including site of onset (bulbar or limb), age at symptom onset, delay from onset to diagnosis and the use of riluzole and non-invasive ventilation (NIV). Clinicians and patients would benefit from a practical way of using these factors to provide an individualised prognosis. Methods: 575 consecutive patients with incident ALS from a population-based registry in South-East England register for ALS (SEALS) were studied. Their survival was modelled as a two-step process: the time from diagnosis to respiratory muscle involvement, followed by the time from respiratory involvement to death. The effects of predictor variables were assessed separately for each time interval. Findings: Younger age at symptom onset, longer delay from onset to diagnosis and riluzole use were associated with slower progression to respiratory involvement, and NIV use was associated with lower mortality after respiratory involvement, each with a clinically significant effect size. Riluzole may have a greater effect in younger patients and those with longer delay to diagnosis. A patient's survival time has a roughly 50% chance of falling between half and twice the predicted median. Interpretation: A simple and clinically applicable graphical method of predicting an individual patient's survival from diagnosis is presented. The model should be validated in an independent cohort, and extended to include other important prognostic factors.},
author = {Knibb, Jonathan A. and Keren, Noa and Kulka, Anna and Leigh, P. Nigel and Martin, Sarah and Shaw, Christopher E. and Tsuda, Miho and Al-Chalabi, Ammar},
doi = {10.1136/JNNP-2015-312908},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Knibb et al. - 2016 - A clinical tool for predicting survival in ALS.pdf:pdf},
issn = {1468330X},
journal = {Journal of Neurology, Neurosurgery and Psychiatry},
month = {dec},
number = {12},
pages = {1361--1367},
pmid = {27378085},
publisher = {BMJ Publishing Group},
title = {{A clinical tool for predicting survival in ALS}},
volume = {87},
year = {2016}
}
@article{Gordon2009,
abstract = {<h3>Objectives</h3><p>To examine how respiratory interventions affect survival as an outcome measure and to define survival rate for trials in amyotrophic lateral sclerosis.</p><h3>Design and Setting</h3><p>We reviewed the data of 3 phase 3 clinical trials and examined differences in times to death, tracheostomy, and permanent assisted ventilation. We assessed the outcomes with $\chi$<sup>2</sup>and Fisher exact tests for categorical variables and unpaired, 2-tailed<i>t</i>tests for continuous variables. We used Kaplan-Meier methods to estimate the differences in survival times between interventions. A power analysis generated sample size estimates for different end points.</p><h3>Patients</h3><p>In all, 2077 patients in 2 phase 3 trials of xaliproden and 400 patients in a phase 3 trial of pentoxifylline.</p><h3>Main Outcome Measures</h3><p>Death or combined death, tracheostomy, or permanent assisted ventilation.</p><h3>Results</h3><p>Of 745 deaths, 611 (82.0%) were owing to respiratory failure and 134 (18.0%) to other causes. The use of respiratory interventions across centers ranged from 0% to 6.6% (<i>P</i> = .001) of patients for tracheostomy and 11.1% to 23.1% (<i>P</i> = .05) of patients for noninvasive ventilation. Twelve of 55 patients (21.8%) undergoing tracheostomy had a vital capacity of 50% or more. Mean (SD) survival time was 457.9 (3.1) days using a combined end point and 467.2 (2.9) days with death alone as the outcome (<i>P</i> = .02). An estimated sample size to detect a 10% difference at 18 months between groups was 490 patients per arm for the combined end point and 410 patients for death alone.</p><h3>Conclusions</h3><p>Tracheostomy and permanent assisted ventilation are not equivalent to death in amyotrophic lateral sclerosis. The use of respiratory interventions differs between centers, leading to variability in combined outcome assessments. The time to the end point can differ significantly depending on its definition, and combining outcomes does not reduce the estimated sample size of a trial. The death rate alone is the least variable and most easily identifiable measure of survival rate in amyotrophic lateral sclerosis.</p>},
author = {Gordon, Paul H. and Corcia, Philippe and Lacomblez, Lucette and Pochigaeva, Ksenia and Abitbol, Jean Louis and Cudkowicz, Merit and Leigh, P. Nigel and Meininger, Vincent},
doi = {10.1001/ARCHNEUROL.2009.1},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Gordon et al. - 2009 - Defining Survival as an Outcome Measure in Amyotrophic Lateral Sclerosis.pdf:pdf},
issn = {0003-9942},
journal = {Archives of Neurology},
keywords = {amyotrophic lateral sclerosis,environmental air flow,forced vital capacity,noninvasive ventilation,outcome measures,pentoxifylline,respiration,respiratory insufficiency,surrogate endpoints,tracheostomy},
month = {jun},
number = {6},
pages = {758--761},
pmid = {19506136},
publisher = {American Medical Association},
title = {{Defining Survival as an Outcome Measure in Amyotrophic Lateral Sclerosis}},
url = {https://jamanetwork.com/journals/jamaneurology/fullarticle/797270},
volume = {66},
year = {2009}
}
@article{Barnes2014,
abstract = {Background Our objective in this study was to develop a point-based tool to predict conversion from amnestic mild cognitive impairment (MCI) to probable Alzheimer's disease (AD). Methods Subjects were participants in the first part of the Alzheimer's Disease Neuroimaging Initiative. Cox proportional hazards models were used to identify factors associated with development of AD, and a point score was created from predictors in the final model. Results The final point score could range from 0 to 9 (mean 4.8) and included: the Functional Assessment Questionnaire (2-3 points); magnetic resonance imaging (MRI) middle temporal cortical thinning (1 point); MRI hippocampal subcortical volume (1 point); Alzheimer's Disease Cognitive Scale - cognitive subscale (2-3 points); and the Clock Test (1 point). Prognostic accuracy was good (Harrell's c = 0.78; 95% CI 0.75, 0.81); 3-year conversion rates were 6% (0-3 points), 53% (4-6 points), and 91% (7-9 points). Conclusions A point-based risk score combining functional dependence, cerebral MRI measures, and neuropsychological test scores provided good accuracy for prediction of conversion from amnestic MCI to AD.},
author = {Barnes, Deborah E. and Cenzer, Irena S. and Yaffe, Kristine and Ritchie, Christine S. and Lee, Sei J.},
doi = {10.1016/J.JALZ.2013.12.014},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Barnes et al. - 2014 - A point-based tool to predict conversion from MCI to probable Alzheimer's disease.pdf:pdf},
issn = {15525279},
journal = {Alzheimer's & dementia : the journal of the Alzheimer's Association},
keywords = {Aged,Alzheimer's disease,Disease progression,Human,Magnetic resonance imaging,Mild cognitive impairment,Neuropsychological tests,Prognostic modeling,Risk factors},
month = {nov},
number = {6},
pages = {646},
pmid = {24495339},
publisher = {NIH Public Access},
title = {{A point-based tool to predict conversion from MCI to probable Alzheimer's disease}},
url = {/pmc/articles/PMC4119093/ /pmc/articles/PMC4119093/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4119093/},
volume = {10},
year = {2014}
}
@article{Lee,
abstract = {for Alzheimer's Disease Neuroimaging Initiative* Alzheimer's disease (AD) is a progressive neurodegenerative condition marked by a decline in cognitive functions with no validated disease modifying treatment. It is critical for timely treatment to detect AD in its earlier stage before clinical manifestation. Mild cognitive impairment (MCI) is an intermediate stage between cognitively normal older adults and AD. To predict conversion from MCI to probable AD, we applied a deep learning approach, multimodal recurrent neural network. We developed an integrative framework that combines not only cross-sectional neuroimaging biomarkers at baseline but also longitudinal cerebrospinal fluid (CSF) and cognitive performance biomarkers obtained from the Alzheimer's Disease Neuroimaging Initiative cohort (ADNI). The proposed framework integrated longitudinal multi-domain data. Our results showed that 1) our prediction model for MCI conversion to AD yielded up to 75% accuracy (area under the curve (AUC) = 0.83) when using only single modality of data separately; and 2) our prediction model achieved the best performance with 81% accuracy (AUC = 0.86) when incorporating longitudinal multi-domain data. A multi-modal deep learning approach has potential to identify persons at risk of developing AD who might benefit most from a clinical trial or as a stratification approach within clinical trials. Alzheimer's disease (AD) is an irreversible, progressive neurodegenerative disorder characterized by abnormal accumulation of amyloid plaques and neurofibrillary tangles in the brain, causing problems with memory, thinking , and behavior. AD is the most common form of dementia with no validated disease modifying treatment. An estimated 5.7 million Americans are living with AD in 2018. By 2050, this number is projected to rise to nearly 14 million 1. Current available treatments decelerate only the progression of AD and no treatment developed so far can cure a patient who is already in AD. Thus, it is of fundamental importance for timely treatment and progression delay to develop strategies for detection of AD at early stages before clinical manifestation. As a result, the concept of mild cognitive impairment (MCI) was introduced. MCI, a prodromal form of AD, is defined to describe people who have mild symptoms of brain malfunction but can still perform everyday tasks. Patients in the phase of MCI have an increased risk of progressing to dementia 1-4. Some patients in their MCI stages are converted to AD within a limit of the time window after baseline, while some are not. It has been reported that MCI patients progress to AD at a rate of 10% to 15% per year and 80% of these MCI patients will have converted to AD after approximately six years of follow-up 5,6. It is an ongoing topic among AD-related researches to identify biomarkers that classify patients with MCI who later progress to AD (MCI converter) from those with MCI who do not progress to AD (MCI non-converter). Various machine learning methods have been applied to identify biomarkers for MCI conversion prediction and improve their performances. Support vector machine (SVM) is one of methods frequently used for solving classification problem. A lot of studies applied SVM for MCI conversion prediction 7-12. A multi-task learning along with SVM was used to identify AD-relevant features, showing 73.9% accuracy, 68.6% sensitivity, and 73.6% specific-ity 7. For the use of additional subjects, a domain transfer learning method to use auxiliary samples such as AD and},
author = {Lee, Garam and Nho, Kwangsik and Kang, Byungkon and Sohn, Kyung-Ah and Kim, Dokyoon},
doi = {10.1038/s41598-018-37769-z},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Lee et al. - Unknown - predicting Alzheimer's disease progression using multi-modal deep learning approach.pdf:pdf},
isbn = {4159801837769},
title = {{predicting Alzheimer's disease progression using multi-modal deep learning approach}},
url = {https://doi.org/10.1038/s41598-018-37769-z}
}
@article{Li2013,
abstract = {The purpose of this paper is to investigate the relative utility of using neuroimaging, genetic, cerebrospinal fluid (CSF), and cognitive measures to predict progression from mild cognitive impairment (MCI) to Alzheimer's disease (AD) dementia over a follow-up period. The studied subjects were 139 persons with MCI enrolled in the Alzheimer's Disease Neuroimaging Initiative. Predictors of progression to AD included brain volume, ventricular volume, hippocampal volume, APOE $\epsilon$4 two alleles, A$\beta$42, p-tau181, p-tau181/A$\beta$42, memory, language, and executive function. We employ a combination of Cox regression analyses and time-dependent receiver operating characteristic (ROC) methods to assess the prognostic utility and performance stability of candidate biomarkers. In a demographic-adjusted multivariable Cox model, seven measures- brain volume, hippocampal volume, ventricular volume, APOE $\epsilon$4 two alleles, A$\beta$42, Memory composite, Executive function composite - predicted progression to AD. Time-dependent ROC revealed that this multivariable model had an area under the curve of 0.832, 0.788, 0.794, and 0.757 at 12, 18, 24, and 36 months respectively. Supplemental Cox models with time of origin set differentially at 12, 18, 24 and 36 months showed that six measures were significant predictors at 12 months whereas only memory and executive function predicted progression to AD at 18 and 24 months. The authors concluded that baseline volumetric MRI and cognitive measures selectively predict progression from MCI to AD, with cognitive measures remaining predictive even late in the follow-up period. These findings may inform case selection for AD clinical trials.},
author = {Li, Shanshan and Okonkwo, Ozioma and Albert, Marilyn and Wang, Mei-Cheng},
doi = {10.7726/AJAD.2013.1002},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2013 - Variation in Variables that Predict Progression from MCI to AD Dementia over Duration of Follow-up.pdf:pdf},
issn = {2162-9986},
journal = {American journal of Alzheimer's disease (Columbia, Mo.)},
keywords = {Alzheimer's disease,Cox models,Mild cognitive impairment,ROC analysis,memory},
number = {1},
pages = {12},
pmid = {24524014},
publisher = {NIH Public Access},
title = {{Variation in Variables that Predict Progression from MCI to AD Dementia over Duration of Follow-up}},
url = {/pmc/articles/PMC3919474/ /pmc/articles/PMC3919474/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3919474/},
volume = {2},
year = {2013}
}
@article{VonBernhardi2017,
abstract = {Alzheimer's disease treatment is still an open problem. The diversity of symptoms, the alterations in common pathophysiology, the existence of asymptomatic cases, the different types of sporadic and familial Alzheimer's and their relevance with other types of dementia and comorbidities, have already created a myth-fear against the leading disease of the twenty first century. Many failed latest clinical trials and novel medications have revealed the early diagnosis as the most critical treatment solution, even though scientists tested the amyloid hypothesis and few related drugs. Unfortunately, latest studies have indicated that the disease begins at the very young ages thus making it difficult to determine the right time of proper treatment. By taking into consideration all these multivariate aspects and unreliable factors against an appropriate treatment, we focused our research on a non-classic statistical evaluation of the most known and accepted Alzheimer's biomarkers. Therefore, in this paper, the code and few experimental results of a computational Bayesian tool have been reported. Moreover, major attention was dedicated to the correlation and assessment of several Alzheimer's biomarkers to export a probabilistic medical prognostic process. This new statistical software is executable in the Bayesian software Winbugs, based on the latest Alzheimer's classification and the formulation of the known relative probabilities of the various biomarkers, correlated with Alzheimer's progression, through a set of discrete distributions. A user-friendly web page has been implemented for the supporting of medical doctors and researchers, to upload Alzheimer's tests and receive statistics on the occurrence of Alzheimer's disease development or presence, due to abnormal testing in one or more biomarkers.},
author = {{Von Bernhardi}, Rommy and Tsolaki, Magda and {Korea Filippo Caraci}, South and Alexiou, Athanasios and Mantzavinos, Vasileios D and Greig, Nigel H and Kamal, Mohammad A},
doi = {10.3389/fnagi.2017.00077},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Von Bernhardi et al. - 2017 - A Bayesian Model for the Prediction and Early Diagnosis of Alzheimer's Disease.pdf:pdf},
journal = {Frontiers in Aging Neuroscience | www.frontiersin.org},
keywords = {Alzheimer's disease,Bayesian statistics,Gibbs Sampling,Markov Chain Monte Carlo,Metropolis-Hastings Algorithm,Winbugs,early diagnosis,medical decision systems},
pages = {77},
title = {{A Bayesian Model for the Prediction and Early Diagnosis of Alzheimer's Disease}},
url = {www.frontiersin.org},
volume = {9},
year = {2017}
}
@article{Fonteijn2012,
abstract = {Understanding the progression of neurological diseases is vital for accurate and early diagnosis and treatment planning. We introduce a new characterization of disease progression, which describes the disease as a series of events, each comprising a significant change in patient state. We provide novel algorithms to learn the event ordering from heterogeneous measurements over a whole patient cohort and demonstrate using combined imaging and clinical data from familial Alzheimer's and Huntington's disease cohorts. Results provide new detail in the progression pattern of these diseases, while confirming known features, and give unique insight into the variability of progression over the cohort. The key advantage of the new model and algorithms over previous progression models is that they do not require a priori division of the patients into clinical stages. The model and its formulation extend naturally to a wide range of other diseases and developmental processes and accommodate cross-sectional and longitudinal input data.},
author = {Fonteijn, Hubert M and Modat, Marc and Clarkson, Matthew J and Barnes, Josephine and Lehmann, Manja and Hobbs, Nicola Z and Scahill, Rachael I and Tabrizi, Sarah J and Ourselin, Sebastien and Fox, Nick C and Alexander, Daniel C},
doi = {10.1016/j.neuroimage.2012.01.062},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Fonteijn et al. - 2012 - An event-based model for disease progression and its application in familial Alzheimer's disease and Huntington.pdf:pdf},
journal = {NeuroImage},
keywords = {Alzheimer's disease,Disease progression,Huntington's disease,MRI},
pages = {1880--1889},
title = {{An event-based model for disease progression and its application in familial Alzheimer's disease and Huntington's disease}},
volume = {60},
year = {2012}
}
@article{Young,
abstract = {We demonstrate the use of a probabilistic generative model to explore the biomarker changes occurring as Alzheimer's disease develops and progresses. We enhanced the recently introduced event-based model for use with a multi-modal sporadic disease data set. This allows us to determine the sequence in which Alzheimer's disease biomarkers become abnormal without reliance on a priori clinical diagnostic information or explicit biomarker cut points. The model also characterizes the uncertainty in the ordering and provides a natural patient staging system. Two hundred and eighty-five subjects (92 cognitively normal, 129 mild cognitive impairment, 64 Alzheimer's disease) were selected from the Alzheimer's Disease Neuroimaging Initiative with measurements of 14 Alzheimer's disease-related biomarkers including cerebrospinal fluid proteins, regional magnetic resonance imaging brain volume and rates of atrophy measures, and cognitive test scores. We used the event-based model to determine the sequence of biomarker abnormality and its uncertainty in various population subgroups. We used patient stages assigned by the event-based model to discriminate cognitively normal subjects from those with Alzheimer's disease, and predict conversion from mild cognitive impairment to Alzheimer's disease and cognitively normal to mild cognitive impairment. The model predicts that cerebrospinal fluid levels become abnormal first, followed by rates of atrophy, then cognitive test scores, and finally regional brain volumes. In amyloid-positive (cerebrospinal fluid amyloid-b 1-42 5 192 pg/ml) or APOE-positive (one or more APOE4 alleles) subjects, the model predicts with high confidence that the cerebrospinal fluid biomarkers become abnormal in a distinct sequence: amyloid-b 1-42 , phosphorylated tau, total tau. However, in the broader population total tau and phos-phorylated tau are found to be earlier cerebrospinal fluid markers than amyloid-b 1-42 , albeit with more uncertainty. The model's staging system strongly separates cognitively normal and Alzheimer's disease subjects (maximum classification accuracy of 99%), and predicts conversion from mild cognitive impairment to Alzheimer's disease (maximum balanced accuracy of 77% over 3 years), and from cognitively normal to mild cognitive impairment (maximum balanced accuracy of 76% over 5 years). By fitting Cox proportional hazards models, we find that baseline model stage is a significant risk factor for conversion from both mild cognitive impairment to Alzheimer's disease (P = 2.06 {\^{A}} 10 {\`{A}} 7) and cognitively normal to mild cognitive impairment (P = 0.033). The data-driven model we describe supports hypothetical models of biomarker ordering in amyloid-positive and APOE-positive subjects, but suggests that biomarker ordering in the wider population may diverge from this sequence. The model provides useful disease staging information across the full spectrum of disease progression, from cognitively normal to mild cognitive impairment to Alzheimer's disease. This approach has broad application across neurodegenerative disease, providing insights into disease biology, as well as staging and prognostication. Initiative; CN-converters = cognitively normal subjects who convert to mild cognitive impairment at follow-up; CN-stable = cogni-tively normal subjects with a stable cognitively normal diagnosis at follow-up; EBM = event-based model; FDG = fluorodeoxyglucose; MCI-converters = mild cognitive impairment subjects who convert to Alzheimer's disease at follow-up; MCI-stable = mild cognitive impairment subjects with a stable mild cognitive impairment diagnosis at follow-up},
author = {Young, Alexandra L and Oxtoby, Neil P and Daga, Pankaj and Cash, David M and Fox, Nick C and Ourselin, Sebastien and Schott, Jonathan M and Alexander, Daniel C and Young, Alexandra},
doi = {10.1093/brain/awu176},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Young et al. - Unknown - A data-driven model of biomarker changes in sporadic Alzheimer's disease.pdf:pdf},
journal = {A JOURNAL OF NEUROLOGY},
keywords = {ADNI = Alzheimer's Disease Neuroimaging,Alzheimer's disease,biomarker ordering Abbreviations: ADAS-Cog = Alzhe,biomarkers,disease progression,event-based model},
title = {{A data-driven model of biomarker changes in sporadic Alzheimer's disease}},
url = {www.loni.ucla.edu/ADNI/}
}
@article{Oxtoby,
author = {Oxtoby, Neil P and Young, Alexandra L and Cash, David M and Benzinger, Tammie L S and Fagan, Anne M and Morris, John C and Bateman, Randall J and Fox, Nick C and Schott, Jonathan M and Alexander, Daniel C},
doi = {10.1093/brain/awy089},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Oxtoby et al. - Unknown - Data-driven models of dominantly-inherited Alzheimer's disease progression.pdf:pdf},
title = {{Data-driven models of dominantly-inherited Alzheimer's disease progression}}
}
@article{Villemagne2013,
abstract = {Background Similar to most chronic diseases, Alzheimer's disease (AD) develops slowly from a preclinical phase into a fully expressed clinical syndrome. We aimed to use longitudinal data to calculate the rates of amyloid $\beta$ (A$\beta$) deposition, cerebral atrophy, and cognitive decline.},
author = {Villemagne, Victor L and Burnham, Samantha and Bourgeat, Pierrick and Brown, Belinda and Ellis, Kathryn A and Salvado, Olivier and Szoeke, Cassandra and Macaulay, Lance and Martins, Ralph and Maruff, Paul and Ames, David and Rowe, Christopher C and Masters, Colin L},
doi = {10.1016/S1474-4422(13)70044-9},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Villemagne et al. - 2013 - Amyloid $\beta$ deposition, neurodegeneration, and cognitive decline in sporadic Alzheimer's disease a prospective.pdf:pdf},
title = {{Amyloid $\beta$ deposition, neurodegeneration, and cognitive decline in sporadic Alzheimer's disease: a prospective cohort study}},
url = {www.thelancet.com/neurology},
year = {2013}
}
@article{Kirkwood2013,
abstract = {BACKGROUND: On-site source data verification is a common and expensive activity, with little evidence that it is worthwhile. Central statistical monitoring (CSM) is a cheaper alternative, where data checks are performed by the coordinating centre, avoiding the need to visit all sites. Several publications have suggested methods for CSM; however, few have described their use in real trials. METHODS: R-programs were created to check data at either the subject level (7 tests within 3 programs) or site level (9 tests within 8 programs) using previously described methods or new ones we developed. These aimed to find possible data errors such as outliers, incorrect dates, or anomalous data patterns; digit preference, values too close or too far from the means, unusual correlation structures, extreme variances which may indicate fraud or procedural errors and under-reporting of adverse events. The methods were applied to three trials, one of which had closed and has been published, one in follow-up, and a third to which fabricated data were added. We examined how well the methods work, discussing their strengths and limitations. RESULTS: The R-programs produced simple tables or easy-to-read figures. Few data errors were found in the first two trials, and those added to the third were easily detected. The programs were able to identify patients with outliers based on single or multiple variables. They also detected (1) fabricated patients, generated to have values too close to the multivariate mean, or with too low variances in repeated measurements, and (2) sites which had unusual correlation structures or too few adverse events. Some methods were unreliable if applied to centres with few patients or if data were fabricated in a way which did not fit the assumptions used to create the programs. Outputs from the R-programs are interpreted using examples. LIMITATIONS: Detecting data errors is relatively straightforward; however, there are several limitations in the detection of fraud: some programs cannot be applied to small trials or to centres with few patients (<10) and data falsified in a manner which does not fit the program's assumptions may not be detected. In addition, many tests require a visual assessment of the output (showing flagged participants or sites), before data queries are made or on-site visits performed. CONCLUSIONS: CSM is a worthwhile alternative to on-site data checking and may be used to limit the number of site visits by targeting only sites which are picked up by the programs. We summarise the methods, show how they are implemented and that they can be easy to interpret. The methods can identify incorrect or unusual data for a trial subject, or centres where the data considered together are too different to other centres and therefore should be reviewed, possibly through an on-site visit.},
author = {Kirkwood, Amy A. and Cox, Trevor and Hackshaw, Allan},
doi = {10.1177/1740774513494504},
file = {:Users/jferreira-admin/Library/Application Support/Mendeley Desktop/Downloaded/Kirkwood, Cox, Hackshaw - 2013 - Application of methods for central statistical monitoring in clinical trials(2).pdf:pdf},
isbn = {1740-7753 (Electronic)\n1740-7745 (Linking)},
issn = {17407745},
journal = {Clinical Trials},
number = {5},
pages = {783--806},
pmid = {24130202},
title = {{Application of methods for central statistical monitoring in clinical trials}},
volume = {10},
year = {2013}
}
